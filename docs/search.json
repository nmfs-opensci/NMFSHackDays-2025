[
  {
    "objectID": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#basic-navigation",
    "href": "topics-skills/02-rstudio.html#basic-navigation",
    "title": "RStudio",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "href": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click “New Project” to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to “No”",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#installing-packages",
    "href": "topics-skills/02-rstudio.html#installing-packages",
    "title": "RStudio",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn’t have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‘piping’ in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won’t see it much in R from 10 years ago.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "href": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the “cog” icon in the Files tab of the bottom right panel. Then click Export.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#creating-files",
    "href": "topics-skills/02-rstudio.html#creating-files",
    "title": "RStudio",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let’s practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on “New Blank File” and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking “Run” 9. Try selecting lines of code and running that by clicking “Run”\n\n\ncsv file\n\nFrom the file panel, click on “New Blank File” and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let’s create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Presentation” on left of the popup and click “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#more-tips",
    "href": "topics-skills/02-rstudio.html#more-tips",
    "title": "RStudio",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-2025/2025-virtualizarr/index.html",
    "href": "topics-2025/2025-virtualizarr/index.html",
    "title": "VirtualiZarr",
    "section": "",
    "text": "https://virtualizarr.readthedocs.io/en/stable/index.html\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1677256"
  },
  {
    "objectID": "topics-2025/2025-icechunk/index.html",
    "href": "topics-2025/2025-icechunk/index.html",
    "title": "Icechunk",
    "section": "",
    "text": "https://earthmover.io/blog/icechunk/ https://icechunk.io/en/latest/ https://icechunk.io/en/latest/icechunk-python/quickstart/\n1 hr talk https://youtu.be/i-73e3_irpY?feature=shared Pangeo showcase: https://youtu.be/l_y8YOn8hm8?feature=shared"
  },
  {
    "objectID": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "href": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Other images on the hub",
    "text": "Other images on the hub\nUse the dropdown to select a non-default image on the hub. There are a variety. You can learn about them on the NMFS Open Science container images repo.",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "href": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Using other images not on the hub",
    "text": "Using other images not on the hub\nThe JupyterHub can run other images that are compatible with a JupyterHub, e.g. Binder images. When you start the hub, use the image dropdown to select “Other”:\n\nYou can add a url to a Docker image to this. For example, if you wanted to use the Pangeo notebook docker images image, you would paste one of these into the “Custom image” box.\nFrom DockerHub: pangeo/pangeo-notebook From Quay.io quay.io/pangeo/pangeo-notebook\nOther common data science images:\n\nJupyter Docker Stacks\nNASA Openscapes python\nRocker Binder image\nPangeo\ngeocompx\nGPU accelerated data science docker images",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-a-github-repo",
    "href": "topics-skills/04-other-images.html#using-a-github-repo",
    "title": "Using other images on the JupyterHub",
    "section": "Using a GitHub repo",
    "text": "Using a GitHub repo\nYou can also create an environment with a MyBinder.org compatible GitHub repo. By selecting the “Build your own image” option.\n\n\nSimple example for Python\nThere are two ways to do this. Either via a conda environment or a pip install.\nconda example\n\nPut an environment.yml file in your GitHub repo at the base level with your Python packages that you need.\nCopy the url to your repo and paste that into the “Repository” box (above).\n\nenvironment.yml\nname: example-environment\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.11\n  - numpy\n  - psutil\n  - toolz\n  - matplotlib\n  - dill\n  - pandas\n  - partd\n  - bokeh\n  - dask\npip install example\nYou will need requirements.txt for packages and runtime.txt for the Python version.\nrequirements.txt\nnumpy\nmatplotlib==3.*\nseaborn==0.13.*\npandas\nruntime.txt\npython-3.10\n\n\nSimple example for R\nr example\nYou will need install.R for packages and runtime.txt for the R version.\ninstall.R\ninstall.packages(\"tidyverse\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"httr\")\ninstall.packages(\"shinydashboard\")\ninstall.packages(\"leaflet\")\nruntime.txt\nr-4.3.2-2024-01-10",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html",
    "href": "topics-skills/03-earthdata.html",
    "title": "Earthdata Login",
    "section": "",
    "text": "NASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nTo programmatically access NASA data from within your Python or R scripts, you will need to enter your Earthdata username and password.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nRun the following commands on the JupyterHub:\n\n\n\n\n\n\nImportant\n\n\n\nIn the below command, replace EARTHDATA_LOGIN with your personal username and EARTHDATA_PASSWORD with your password\n\n\necho 'machine urs.earthdata.nasa.gov login \"EARTHDATA_LOGIN\" password \"EARTHDATA_PASSWORD\"' &gt; ~/.netrc\nchmod 0600 ~/.netrc",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html",
    "href": "topics-skills/03-AWS_S3_bucket.html",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "",
    "text": "This set of instructions will walk through how to setup an AWS S3 bucket for a specific project and how to configure that bucket to allow all members of the project team to have access.\nThis notebook is from the CryoCloud documentation. THE CODE WILL NOT WORK SINCE YOU NEED TO AUTHENTICATE TO THE S3 BUCKET.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create an AWS account and S3 bucket",
    "text": "Create an AWS account and S3 bucket\nThe first step is to create an AWS account that will be billed to your particular project. This can be done using these instructions.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create AWS S3 bucket",
    "text": "Create AWS S3 bucket\nWithin your new AWS account, create an new S3 bucket:\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nChoose Create bucket\nName the bucket and select us-west-2 for the region\nLeave all other default options\nClick Create Bucket",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create a user",
    "text": "Create a user\nWithin the same AWS account, create a new IAM user:\n\nOn the AWS Console Home page, select the IAM service\nIn the navigation pane, select Users and then select Add users\nName the user and click Next\nAttach policies directly\nDo not select any policies\nClick Next\nCreate user\n\nOnce the user has been created, find the user’s ARN and copy it.\nNow, create access keys for this user:\n\nSelect Users and click the user that you created\nOpen the Security Credentials tab\nCreate access key\nSelect Command Line Interface (CLI)\nCheck the box to agree to the recommendation and click Next\nLeave the tag blank and click Create access key\nIMPORTANT: Copy the access key and the secret access key. This will be used later.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create the bucket policy",
    "text": "Create the bucket policy\nConfigure a policy for this S3 bucket that will allow the newly created user to access it.\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nSelect the new S3 bucket that you created\nOpen the Permissions tab\nAdd the following bucket policy, replacing USER_ARN with the ARN that you copied above and BUCKET_ARN with the bucket ARN, found on the Edit bucket policy page on the AWS console:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBucket\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"BUCKET_ARN\"\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:*Object\",\n            \"Resource\": \"BUCKET_ARN/*\"\n        }\n    ]\n}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Reading from the S3 bucket",
    "text": "Reading from the S3 bucket\n\nExample: ls bucket using s3fs\n\nimport s3fs\ns3 = s3fs.S3FileSystem(anon=False, profile='icesat2')\n\n\n\nExample: open HDF5 file using xarray\n\nimport s3fs\nimport xarray as xr\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nprint(ssh_ds)\n\n&lt;xarray.Dataset&gt;\nDimensions:      (Longitude: 2160, nv: 2, Latitude: 960, Time: 1)\nCoordinates:\n  * Longitude    (Longitude) float32 0.08333 0.25 0.4167 ... 359.6 359.8 359.9\n  * Latitude     (Latitude) float32 -79.92 -79.75 -79.58 ... 79.58 79.75 79.92\n  * Time         (Time) datetime64[ns] 1992-10-10T12:00:00\nDimensions without coordinates: nv\nData variables:\n    Lon_bounds   (Longitude, nv) float32 ...\n    Lat_bounds   (Latitude, nv) float32 ...\n    Time_bounds  (Time, nv) datetime64[ns] ...\n    SLA          (Time, Latitude, Longitude) float32 ...\n    SLA_ERR      (Time, Latitude, Longitude) float32 ...\nAttributes: (12/21)\n    Conventions:            CF-1.6\n    ncei_template_version:  NCEI_NetCDF_Grid_Template_v2.0\n    Institution:            Jet Propulsion Laboratory\n    geospatial_lat_min:     -79.916664\n    geospatial_lat_max:     79.916664\n    geospatial_lon_min:     0.083333336\n    ...                     ...\n    version_number:         2205\n    Data_Pnts_Each_Sat:     {\"16\": 661578, \"1001\": 636257}\n    source_version:         commit dc95db885c920084614a41849ce5a7d417198ef3\n    SLA_Global_MEAN:        -0.0015108844021796562\n    SLA_Global_STD:         0.09098986023297456\n    latency:                final\n\n\n\nimport s3fs\n\nimport xarray as xr\n\nimport hvplot.xarray\nimport holoviews as hv\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_da = ssh_ds.SLA\n\nssh_da.hvplot.image(x='Longitude', y='Latitude', cmap='Spectral_r', geo=True, tiles='ESRI', global_extent=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nExample: read a geotiff using rasterio\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsession = rasterio.env.Env(profile_name='icesat2')\n\nurl = 's3://gris-outlet-glacier-seasonality-icesat2/out.tif'\n\nwith session:\n    with rasterio.open(url) as ds:\n        print(ds.profile)\n        band1 = ds.read(1)\n        \nband1[band1==-9999] = np.nan\nplt.imshow(band1)\nplt.colorbar()\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -9999.0, 'width': 556, 'height': 2316, 'count': 1, 'crs': CRS.from_epsg(3413), 'transform': Affine(50.0, 0.0, -204376.0,\n       0.0, -50.0, -2065986.0), 'blockysize': 3, 'tiled': False, 'interleave': 'band'}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Writing to the S3 bucket",
    "text": "Writing to the S3 bucket\n\ns3 = s3fs.core.S3FileSystem(profile='icesat2')\n\nwith s3.open('gris-outlet-glacier-seasonality-icesat2/new-file', 'wb') as f:\n    f.write(2*2**20 * b'a')\n    f.write(2*2**20 * b'a') # data is flushed and file closed\n\ns3.du('gris-outlet-glacier-seasonality-icesat2/new-file')\n\n4194304",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#what-is-git-and-github",
    "href": "topics-skills/02-git.html#what-is-git-and-github",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‘container’ for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#some-basic-git-jargon",
    "href": "topics-skills/02-git.html#some-basic-git-jargon",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and “commit” that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#overview",
    "href": "topics-skills/02-git.html#overview",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a “trunk-based workflow”.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe’ll do this",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#the-key-skills",
    "href": "topics-skills/02-git.html#the-key-skills",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "The Key Skills",
    "text": "The Key Skills\nThese basic skills are all you need to learn to get started:\n\nSkill 1: Create a blank repo on GitHub (the remote or origin)\nSkill 2: Clone your GitHub repo to your local computer (in our case the JupyterHub)\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub (the remote or origin)\nSkill 1b: Create a new repo from some else’s GitHub repository\n\nIn the next tutorials, you will practice these in RStudio or JuptyerHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#prerequisites",
    "href": "topics-skills/02-git-rstudio.html#prerequisites",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "href": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "href": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Git tab in RStudio",
    "text": "Git tab in RStudio\nWhen the instructions say to use or open or click the Git tab, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#the-key-skills",
    "href": "topics-skills/02-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "href": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPaste in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nActivity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me in RStudio\n\n\nActivity 2\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nGo to RStudio and pull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#footnotes",
    "href": "topics-skills/02-git-rstudio.html#footnotes",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html",
    "href": "topics-skills/02-git-jupyter-old.html",
    "title": "Git in JupyterLab",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html#step-3",
    "href": "topics-skills/02-git-jupyter-old.html#step-3",
    "title": "Git in JupyterLab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "href": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "title": "GitHub Authentication",
    "section": "Tell Git who you are",
    "text": "Tell Git who you are\nFirst open a terminal and run these lines. Replace &lt;your email&gt; with your email and remove the angle brackets.\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#authentication",
    "href": "topics-skills/02-git-authentication.html#authentication",
    "title": "GitHub Authentication",
    "section": "Authentication",
    "text": "Authentication\nYou need to authenticate to GitHub so you can push your local changes up to GitHub. There are a few ways to do this. For the JupyterHub, we will mainly use gh-scroped-creds which is a secure app that temporarily stores your GitHub credentials on a JupyterHub. But we will also show you a way to store your credentials in a file that works on any computer, including a virtual computer like the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "href": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "title": "GitHub Authentication",
    "section": "Preferred: gh-scoped-creds",
    "text": "Preferred: gh-scoped-creds\nIf you get the error that it cannot find gh-scoped-creds, then type\npip install gh-scoped-creds\nin a termnal.\n\nOpen a terminal\nType gh-scoped-creds\nFollow the instructions\nFIRST TIME: Make sure to follow the second pop-up instructions and tell it what repos it is allowed to interact with. You have to go through a number of pop up windows.\n\nJump down to the “Test” section to test.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "href": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "title": "GitHub Authentication",
    "section": "Also works: Set up authentication with a Personal Token",
    "text": "Also works: Set up authentication with a Personal Token\nThis will store your credentials in a file on the hub. This is not as secure since the file is unencrypted but sometimes gh-scoped-creds will not be an option.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nOpen a terminal in JupyterLab or RStudio\nPaste these 3 lines of code into the terminal\n\ngit config --global credential.helper store",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#test",
    "href": "topics-skills/02-git-authentication.html#test",
    "title": "GitHub Authentication",
    "section": "Test",
    "text": "Test\n\nGo to https://github.com/new\nCreate a PRIVATE repo called “test”\nMake sure to check the “Add a README file” box!\nOpen a terminal and type these lines. Make sure to replace &lt;username&gt;\n\ngit clone https://github.com/&lt;username&gt;/test\n\nIf you properly authenticated, git will ask for your username and password. At the password, paste in the TOKEN not your actual password.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-data-cube-opendap.html#overview",
    "href": "topics-2025/2025-opendap/1-data-cube-opendap.html#overview",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nTHREDDS is a common type of data server that usually includes multiple ways to access the data. One of those ways is via the OPeNDAP protocol which allows you to subset the data, instead of downloading the whole data file. Here you will learn to use xarray’s open_mfdataset to create data cubes on THREDDS servers using the OPeNDAP protocol."
  },
  {
    "objectID": "topics-2025/2025-opendap/1-data-cube-opendap.html#ncep-ncar-reanalysis-1",
    "href": "topics-2025/2025-opendap/1-data-cube-opendap.html#ncep-ncar-reanalysis-1",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "NCEP-NCAR Reanalysis 1",
    "text": "NCEP-NCAR Reanalysis 1\nFor our first example, we will use 4xDaily Air temperature at sigma level 995 data (air.sig995) from the NCEP-NCAR Reanalysis 1. First look at the THREDDS catalog to orient yourself to the file naming convention. We can click on one of the files an see our [access options]. We are looking for the OPeNDAP information as we need to get the url for that. Clicking the OPeNDAP link reveals the url format of the files:\nhttps://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995.1948.nc\nNow we can create the file urls.\n\nbase_url = 'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995'\n\nfiles = [f'{base_url}.{year}.nc' for year in range(1948, 2025)]\nlen(files)\n\n77\n\n\nWe will load the file metadata with open_mfdataset and create our virtual data cube. It will take 30 seconds or so but it doesn’t use much memory as we are only reading and loading the file metadata.\n\n%%time\nimport xarray as xr\nds = xr.open_mfdataset(files);\n\nCPU times: user 2.73 s, sys: 368 ms, total: 3.09 s\nWall time: 25.3 s\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 5GB\nDimensions:  (time: 112500, lat: 73, lon: 144)\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 900kB 1948-01-01 ... 2024-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nData variables:\n    air      (time, lat, lon) float32 5GB dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;\nAttributes:\n    Conventions:                     COARDS\n    title:                           4x daily NMC reanalysis (1948)\n    description:                     Data is from NMC initialized reanalysis\\...\n    platform:                        Model\n    history:                         created 99/05/11 by Hoop (netCDF2.3)\\nCo...\n    dataset_title:                   NCEP-NCAR Reanalysis 1\n    References:                      http://www.psl.noaa.gov/data/gridded/dat...\n    _NCProperties:                   version=2,netcdf=4.6.3,hdf5=1.10.5\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:time: 112500lat: 73lon: 144Coordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 2024-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '2024-12-31T06:00:00.000000000',\n       '2024-12-31T12:00:00.000000000', '2024-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Data variables: (1)air(time, lat, lon)float32dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.41 GiB\n58.71 MiB\n\n\nShape\n(112500, 73, 144)\n(1464, 73, 144)\n\n\nDask graph\n77 chunks in 155 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             144 73 112500\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '2024-12-29 12:00:00', '2024-12-29 18:00:00',\n               '2024-12-30 00:00:00', '2024-12-30 06:00:00',\n               '2024-12-30 12:00:00', '2024-12-30 18:00:00',\n               '2024-12-31 00:00:00', '2024-12-31 06:00:00',\n               '2024-12-31 12:00:00', '2024-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=112500, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (9)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelhistory :created 99/05/11 by Hoop (netCDF2.3)\nConverted to chunked, deflated non-packed NetCDF4 2014/09dataset_title :NCEP-NCAR Reanalysis 1References :http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis.html_NCProperties :version=2,netcdf=4.6.3,hdf5=1.10.5DODS_EXTRA.Unlimited_Dimension :time\n\n\nThe data set is not that huge.\n\nprint(f\"{ds.nbytes / 1e9} Gb\")\n\n4.731300868 Gb\n\n\n\nds['air'].isel(time=1).plot()"
  },
  {
    "objectID": "topics-2025/2025-opendap/1-data-cube-opendap.html#creating-daily-means",
    "href": "topics-2025/2025-opendap/1-data-cube-opendap.html#creating-daily-means",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Creating daily means",
    "text": "Creating daily means\nFor one year, we can create a daily mean since 1 year is not that much data and we can fit that into memory.\n\nds_mean = ds[\"air\"].sel(time=\"1949\").mean(dim=['lat', 'lon'])\nds_mean.plot();\n\n\n\n\n\n\n\n\n\n# resample to daily\nds_mean.resample(time='D').mean().plot();\n\n\n\n\n\n\n\n\nBut if we try to do all years at once, we will run out of memory and it is possible that the THREDDS server will complain that we are making too big of a data request. So let’s tackle this problem by chunking up the data.\n\nChunking the data\nI choose chunking to get to about 100 Mb sized chunks.\n\nimport dask\nds_chunk = ds[\"air\"].sel(lat=slice(25,-25), lon=slice(-25,25)).chunk({'time': 1464, 'lat': -1, 'lon': -1})\nds_chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 112500, lat: 21, lon: 11)&gt; Size: 104MB\ndask.array&lt;rechunk-merge, shape=(112500, 21, 11), dtype=float32, chunksize=(1464, 21, 11), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lon      (lon) float32 44B 0.0 2.5 5.0 7.5 10.0 ... 15.0 17.5 20.0 22.5 25.0\n  * time     (time) datetime64[ns] 900kB 1948-01-01 ... 2024-12-31T18:00:00\n  * lat      (lat) float32 84B 25.0 22.5 20.0 17.5 ... -17.5 -20.0 -22.5 -25.0\nAttributes: (12/13)\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    ...            ...\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    valid_range:   [185.16 331.16]\n    dataset:       NCEP Reanalysis\n    level_desc:    0.995 sigma\n    _ChunkSizes:   [  1  73 144]xarray.DataArray'air'time: 112500lat: 21lon: 11dask.array&lt;chunksize=(1464, 21, 11), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n99.13 MiB\n1.29 MiB\n\n\nShape\n(112500, 21, 11)\n(1464, 21, 11)\n\n\nDask graph\n77 chunks in 157 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             11 21 112500\n\n\n\n\nCoordinates: (3)lon(lon)float320.0 2.5 5.0 7.5 ... 20.0 22.5 25.0units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([ 0. ,  2.5,  5. ,  7.5, 10. , 12.5, 15. , 17.5, 20. , 22.5, 25. ],\n      dtype=float32)time(time)datetime64[ns]1948-01-01 ... 2024-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '2024-12-31T06:00:00.000000000',\n       '2024-12-31T12:00:00.000000000', '2024-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3225.0 22.5 20.0 ... -22.5 -25.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 25. ,  22.5,  20. ,  17.5,  15. ,  12.5,  10. ,   7.5,   5. ,   2.5,\n         0. ,  -2.5,  -5. ,  -7.5, -10. , -12.5, -15. , -17.5, -20. , -22.5,\n       -25. ], dtype=float32)Indexes: (3)lonPandasIndexPandasIndex(Index([0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0, 22.5, 25.0], dtype='float32', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '2024-12-29 12:00:00', '2024-12-29 18:00:00',\n               '2024-12-30 00:00:00', '2024-12-30 06:00:00',\n               '2024-12-30 12:00:00', '2024-12-30 18:00:00',\n               '2024-12-31 00:00:00', '2024-12-31 06:00:00',\n               '2024-12-31 12:00:00', '2024-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=112500, freq=None))latPandasIndexPandasIndex(Index([ 25.0,  22.5,  20.0,  17.5,  15.0,  12.5,  10.0,   7.5,   5.0,   2.5,\n         0.0,  -2.5,  -5.0,  -7.5, -10.0, -12.5, -15.0, -17.5, -20.0, -22.5,\n       -25.0],\n      dtype='float32', name='lat'))Attributes: (13)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\nmean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon'])\nmean_all_years\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 16072)&gt; Size: 64kB\ndask.array&lt;mean_agg-aggregate, shape=(16072,), dtype=float32, chunksize=(1464,), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 129kB 1948-01-01 ... 1958-12-31T18:00:00xarray.DataArray'air'time: 16072dask.array&lt;chunksize=(1464,), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n62.78 kiB\n5.72 kiB\n\n\nShape\n(16072,)\n(1464,)\n\n\nDask graph\n11 chunks in 160 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                   16072 1\n\n\n\n\nCoordinates: (1)time(time)datetime64[ns]1948-01-01 ... 1958-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1958-12-31T06:00:00.000000000',\n       '1958-12-31T12:00:00.000000000', '1958-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1958-12-29 12:00:00', '1958-12-29 18:00:00',\n               '1958-12-30 00:00:00', '1958-12-30 06:00:00',\n               '1958-12-30 12:00:00', '1958-12-30 18:00:00',\n               '1958-12-31 00:00:00', '1958-12-31 06:00:00',\n               '1958-12-31 12:00:00', '1958-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=16072, freq=None))Attributes: (0)\n\n\n\n# This is takes about 4 minutes; 1.56\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    mean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon']).compute()\n\n[########################################] | 100% Completed | 51.77 s\n\n\n\nmean_all_years.compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 16072)&gt; Size: 64kB\narray([276.0041 , 276.12732, 276.54147, ..., 275.8719 , 276.2409 ,\n       276.08835], dtype=float32)\nCoordinates:\n  * time     (time) datetime64[ns] 129kB 1948-01-01 ... 1958-12-31T18:00:00xarray.DataArray'air'time: 16072276.0 276.1 276.5 276.5 276.3 276.4 ... 276.0 275.7 275.9 276.2 276.1array([276.0041 , 276.12732, 276.54147, ..., 275.8719 , 276.2409 ,\n       276.08835], dtype=float32)Coordinates: (1)time(time)datetime64[ns]1948-01-01 ... 1958-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1958-12-31T06:00:00.000000000',\n       '1958-12-31T12:00:00.000000000', '1958-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1958-12-29 12:00:00', '1958-12-29 18:00:00',\n               '1958-12-30 00:00:00', '1958-12-30 06:00:00',\n               '1958-12-30 12:00:00', '1958-12-30 18:00:00',\n               '1958-12-31 00:00:00', '1958-12-31 06:00:00',\n               '1958-12-31 12:00:00', '1958-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=16072, freq=None))Attributes: (0)\n\n\n\n# This is takes about 4 minutes; 1.56\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    mean_all_years.compute()\n\n[########################################] | 100% Completed | 203.47 s\n\n\n\nmean_all_years.plot();\n\n\n\n\n\n\n\n\n\nds_mean = ds[\"air\"].sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon']).compute()\n\n\nds_mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 16072)&gt; Size: 64kB\ndask.array&lt;mean_agg-aggregate, shape=(16072,), dtype=float32, chunksize=(1464,), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 129kB 1948-01-01 ... 1958-12-31T18:00:00xarray.DataArray'air'time: 16072dask.array&lt;chunksize=(1464,), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n62.78 kiB\n5.72 kiB\n\n\nShape\n(16072,)\n(1464,)\n\n\nDask graph\n11 chunks in 158 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                   16072 1\n\n\n\n\nCoordinates: (1)time(time)datetime64[ns]1948-01-01 ... 1958-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1958-12-31T06:00:00.000000000',\n       '1958-12-31T12:00:00.000000000', '1958-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1958-12-29 12:00:00', '1958-12-29 18:00:00',\n               '1958-12-30 00:00:00', '1958-12-30 06:00:00',\n               '1958-12-30 12:00:00', '1958-12-30 18:00:00',\n               '1958-12-31 00:00:00', '1958-12-31 06:00:00',\n               '1958-12-31 12:00:00', '1958-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=16072, freq=None))Attributes: (0)\n\n\n\nds_mean.plot();\n\n\n\n\n\n\n\n\n\n# takes 4 minutes; 1.5 Gb\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    mean_ds = ds.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon']).persist()\n\n[########################################] | 100% Completed | 231.80 s\n\n\n\nds[\"air\"].encoding[\"chunksizes\"]\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 ds[\"air\"].encoding[\"chunksizes\"]\n\nKeyError: 'chunksizes'\n\n\n\n\nprint(ds[\"air\"].chunks)\n\n((1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464), (73,), (144,))\n\n\n\nmean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon'])\nmean_all_years.data.visualize()\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[9], line 2\n      1 mean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon'])\n----&gt; 2 mean_all_years.data.visualize()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/dask/base.py:298, in DaskMethodsMixin.visualize(self, filename, format, optimize_graph, **kwargs)\n    254 def visualize(self, filename=\"mydask\", format=None, optimize_graph=False, **kwargs):\n    255     \"\"\"Render the computation of this object's task graph using graphviz.\n    256 \n    257     Requires ``graphviz`` to be installed.\n   (...)\n    296     https://docs.dask.org/en/latest/optimize.html\n    297     \"\"\"\n--&gt; 298     return visualize(\n    299         self,\n    300         filename=filename,\n    301         format=format,\n    302         optimize_graph=optimize_graph,\n    303         **kwargs,\n    304     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/dask/base.py:757, in visualize(filename, traverse, optimize_graph, maxval, engine, *args, **kwargs)\n    753 args, _ = unpack_collections(*args, traverse=traverse)\n    755 dsk = dict(collections_to_dsk(args, optimize_graph=optimize_graph))\n--&gt; 757 return visualize_dsk(\n    758     dsk=dsk,\n    759     filename=filename,\n    760     traverse=traverse,\n    761     optimize_graph=optimize_graph,\n    762     maxval=maxval,\n    763     engine=engine,\n    764     **kwargs,\n    765 )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/dask/base.py:900, in visualize_dsk(dsk, filename, traverse, optimize_graph, maxval, o, engine, limit, **kwargs)\n    898     return cytoscape_graph(dsk, filename=filename, **kwargs)\n    899 elif engine is None:\n--&gt; 900     raise RuntimeError(\n    901         \"No visualization engine detected, please install graphviz or ipycytoscape\"\n    902     )\n    903 else:\n    904     raise ValueError(f\"Visualization engine {engine} not recognized\")\n\nRuntimeError: No visualization engine detected, please install graphviz or ipycytoscape\n\n\n\n\nds_chunk.mean(dim=['lat', 'lon'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 112500)&gt; Size: 450kB\ndask.array&lt;mean_agg-aggregate, shape=(112500,), dtype=float32, chunksize=(1464,), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * time     (time) datetime64[ns] 900kB 1948-01-01 ... 2024-12-31T18:00:00xarray.DataArray'air'time: 112500dask.array&lt;chunksize=(1464,), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n439.45 kiB\n5.72 kiB\n\n\nShape\n(112500,)\n(1464,)\n\n\nDask graph\n77 chunks in 159 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           112500 1\n\n\n\n\nCoordinates: (1)time(time)datetime64[ns]1948-01-01 ... 2024-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '2024-12-31T06:00:00.000000000',\n       '2024-12-31T12:00:00.000000000', '2024-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')Indexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '2024-12-29 12:00:00', '2024-12-29 18:00:00',\n               '2024-12-30 00:00:00', '2024-12-30 06:00:00',\n               '2024-12-30 12:00:00', '2024-12-30 18:00:00',\n               '2024-12-31 00:00:00', '2024-12-31 06:00:00',\n               '2024-12-31 12:00:00', '2024-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=112500, freq=None))Attributes: (0)\n\n\n\ndask.visualize(ds.mean(dim=['lat', 'lon']))\n\n\n\n\n\n\n\n\n\npip install graphviz\n\nCollecting graphviz\n  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\nDownloading graphviz-0.20.3-py3-none-any.whl (47 kB)\nInstalling collected packages: graphviz\nSuccessfully installed graphviz-0.20.3\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom dask.distributed import Client\nclient = Client()  # Starts a local Dask cluster\nds_chunk = ds.chunk({'time': 1000, 'lat': 25, 'lon': -1})\nmean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon']).compute()\n\n2025-03-14 17:05:43,952 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 292.64 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:07,393 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 296.00 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:07,528 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 415.69 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:07,629 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 354.56 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:08,643 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43435 (pid=5301) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:08,671 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43435' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 2, 0, 0)} (stimulus_id='handle-worker-cleanup-1741971968.6711407')\n2025-03-14 17:06:08,671 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43435\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:49944 remote=tcp://127.0.0.1:43435&gt;: Stream is closed\n2025-03-14 17:06:08,679 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:10,413 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 343.86 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:15,451 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 413.09 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:15,550 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 353.94 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:15,693 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:45173 (pid=5297) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:15,729 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:45173\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n    bytes_read = self.read_from_fd(buf)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n    return self.socket.recv_into(buf, len(buf))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:49926 remote=tcp://127.0.0.1:45173&gt;: ConnectionResetError: [Errno 104] Connection reset by peer\n2025-03-14 17:06:15,740 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:45173' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 3, 0, 0)} (stimulus_id='handle-worker-cleanup-1741971975.7402785')\n2025-03-14 17:06:15,745 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:16,303 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 292.10 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:17,444 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44767 (pid=5293) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:17,466 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:44767' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {'original-open_dataset-air-384c8a0efee54035d38154b9923ef2ec', ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 1, 0, 0), 'original-open_dataset-air-dc149680e9a295f1c93f64761d49e7f5'} (stimulus_id='handle-worker-cleanup-1741971977.466795')\n2025-03-14 17:06:17,480 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:17,960 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 296.80 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:30,905 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 426.58 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:30,949 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 355.48 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:31,063 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:43555 (pid=5304) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:31,087 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:43555' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('mean_agg-aggregate-ae7a197e4518c41cb22b6023e55ff376', 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 7), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 7, 0, 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 3), ('rechunk-split-efe00704571913867602840a1828e0a4', 5)} (stimulus_id='handle-worker-cleanup-1741971991.0873015')\n2025-03-14 17:06:31,099 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:42,943 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36751 (pid=6604) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:42,963 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:36751' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 6, 0, 0), 'original-open_dataset-air-254984a64c9a1ce87a01bec635aa4615', ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 4, 0, 0), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 10, 0, 0)} (stimulus_id='handle-worker-cleanup-1741972002.9636204')\n2025-03-14 17:06:42,974 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:44,023 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 293.38 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:44,151 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 409.07 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:44,252 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 351.93 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:44,466 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 293.21 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:44,717 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 410.80 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:44,963 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41981 (pid=7505) exceeded 95% memory budget. Restarting...\n2025-03-14 17:06:44,986 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41981' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 0, 0, 0)} (stimulus_id='handle-worker-cleanup-1741972004.9867756')\n2025-03-14 17:06:44,994 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:06:44,986 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41981\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n    bytes_read = self.read_from_fd(buf)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n    return self.socket.recv_into(buf, len(buf))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2878, in get_data_from_worker\n    comm = await rpc.connect(worker)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1485, in connect\n    return await self._connect(addr=addr, timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1429, in _connect\n    comm = await connect(\n           ^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/core.py\", line 377, in connect\n    handshake = await comm.read()\n                ^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed)  local=tcp://127.0.0.1:46742 remote=tcp://127.0.0.1:41981&gt;: ConnectionResetError: [Errno 104] Connection reset by peer\n2025-03-14 17:06:57,385 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 352.14 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:57,659 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 412.52 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:57,933 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 359.99 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:58,359 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 395.29 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:06:58,656 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 361.29 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:08,443 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 412.46 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:08,693 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:44411 (pid=6890) exceeded 95% memory budget. Restarting...\n2025-03-14 17:07:08,720 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:44411' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 11, 0, 0), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 3, 0, 0)} (stimulus_id='handle-worker-cleanup-1741972028.7205348')\n2025-03-14 17:07:08,730 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:07:09,537 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 418.96 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:09,683 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 355.60 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:12,285 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 303.57 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:24,073 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 292.68 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:28,242 - distributed.worker.memory - WARNING - Worker is at 96% memory usage. Pausing worker.  Process memory: 460.02 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:28,543 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:41235 (pid=6970) exceeded 95% memory budget. Restarting...\n2025-03-14 17:07:28,568 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:41235' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('rechunk-split-efe00704571913867602840a1828e0a4', 43), ('rechunk-split-efe00704571913867602840a1828e0a4', 4), ('rechunk-split-efe00704571913867602840a1828e0a4', 33), ('rechunk-split-efe00704571913867602840a1828e0a4', 39), ('rechunk-split-efe00704571913867602840a1828e0a4', 6), ('rechunk-split-efe00704571913867602840a1828e0a4', 3), ('rechunk-split-efe00704571913867602840a1828e0a4', 38), ('rechunk-split-efe00704571913867602840a1828e0a4', 34), ('rechunk-split-efe00704571913867602840a1828e0a4', 41), ('rechunk-split-efe00704571913867602840a1828e0a4', 37), ('rechunk-split-efe00704571913867602840a1828e0a4', 5)} (stimulus_id='handle-worker-cleanup-1741972048.568713')\n2025-03-14 17:07:28,569 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41235\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n    bytes_read = self.read_from_fd(buf)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n    return self.socket.recv_into(buf, len(buf))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:43160 remote=tcp://127.0.0.1:41235&gt;: ConnectionResetError: [Errno 104] Connection reset by peer\n2025-03-14 17:07:28,568 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:41235\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 861, in _read_to_buffer\n    bytes_read = self.read_from_fd(buf)\n                 ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/tornado/iostream.py\", line 1116, in read_from_fd\n    return self.socket.recv_into(buf, len(buf))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 140, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:60256 remote=tcp://127.0.0.1:41235&gt;: ConnectionResetError: [Errno 104] Connection reset by peer\n2025-03-14 17:07:28,579 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:07:29,621 - distributed.worker.memory - WARNING - Worker is at 88% memory usage. Pausing worker.  Process memory: 420.08 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:29,931 - distributed.worker.memory - WARNING - Worker is at 61% memory usage. Resuming worker. Process memory: 292.48 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:29,931 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 292.48 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:37,640 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 382.99 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:37,739 - distributed.worker.memory - WARNING - Worker is at 62% memory usage. Resuming worker. Process memory: 296.82 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:39,797 - distributed.worker.memory - WARNING - Worker is at 89% memory usage. Pausing worker.  Process memory: 422.73 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:40,043 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:42023 (pid=7979) exceeded 95% memory budget. Restarting...\n2025-03-14 17:07:40,065 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:42023' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 8, 0, 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 20), ('rechunk-split-efe00704571913867602840a1828e0a4', 49), ('mean_agg-aggregate-ae7a197e4518c41cb22b6023e55ff376', 3), ('rechunk-split-efe00704571913867602840a1828e0a4', 55), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 7, 0, 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 51), ('mean_agg-aggregate-ae7a197e4518c41cb22b6023e55ff376', 2), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 11, 0, 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 22), ('rechunk-split-efe00704571913867602840a1828e0a4', 18), ('rechunk-split-efe00704571913867602840a1828e0a4', 53), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 9, 0, 0)} (stimulus_id='handle-worker-cleanup-1741972060.0656252')\n2025-03-14 17:07:40,065 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42023\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 227, in read\n    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:49728 remote=tcp://127.0.0.1:42023&gt;: Stream is closed\n2025-03-14 17:07:40,065 - distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:42023\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 227, in read\n    frames_nosplit = await read_bytes_rw(stream, frames_nosplit_nbytes)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 366, in read_bytes_rw\n    actual = await stream.read_into(chunk)  # type: ignore[arg-type]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2075, in gather_dep\n    response = await get_data_from_worker(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 2881, in get_data_from_worker\n    response = await send_recv(\n               ^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) Ephemeral Worker-&gt;Worker for gather local=tcp://127.0.0.1:37510 remote=tcp://127.0.0.1:42023&gt;: Stream is closed\n2025-03-14 17:07:40,089 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:07:41,807 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 390.62 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:42,077 - distributed.worker.memory - WARNING - Worker is at 70% memory usage. Resuming worker. Process memory: 332.86 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:47,399 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 401.98 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:47,680 - distributed.worker.memory - WARNING - Worker is at 69% memory usage. Resuming worker. Process memory: 331.58 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:47,870 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 291.98 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:52,797 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 385.86 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:52,822 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 422.16 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:52,847 - distributed.worker.memory - WARNING - Worker is at 74% memory usage. Resuming worker. Process memory: 353.02 MiB -- Worker memory limit: 474.75 MiB\n2025-03-14 17:07:56,692 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:36407 (pid=9002) exceeded 95% memory budget. Restarting...\n2025-03-14 17:07:56,716 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:36407' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('rechunk-split-efe00704571913867602840a1828e0a4', 40), ('rechunk-split-efe00704571913867602840a1828e0a4', 81), ('rechunk-split-efe00704571913867602840a1828e0a4', 42), 'original-open_dataset-air-fff2bd54a32e2d63201e7b4ba41f3513', ('rechunk-split-efe00704571913867602840a1828e0a4', 44), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 6, 0, 0)} (stimulus_id='handle-worker-cleanup-1741972076.7167778')\n2025-03-14 17:07:56,720 - distributed.nanny - WARNING - Restarting worker\n2025-03-14 17:07:58,864 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:38721 (pid=8076) exceeded 95% memory budget. Restarting...\n2025-03-14 17:07:58,887 - distributed.scheduler - ERROR - Task ('rechunk-split-efe00704571913867602840a1828e0a4', 19) marked as failed because 4 workers died while trying to run it\n2025-03-14 17:07:58,888 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:38721' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('mean_combine-partial-5b30157349141bfba9ea6a11359a619d', 1, 1, 0), ('mean_chunk-7e9545b2fbb1fd48b1bbb550479d60f8', 11, 1, 0), ('mean_chunk-7e9545b2fbb1fd48b1bbb550479d60f8', 7, 1, 0), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 3, 0, 0), ('mean_agg-aggregate-ae7a197e4518c41cb22b6023e55ff376', 0), 'original-open_dataset-air-894f74a135fe7dddc6c109beb3dc0a5d', ('mean_agg-aggregate-ae7a197e4518c41cb22b6023e55ff376', 6), ('rechunk-split-efe00704571913867602840a1828e0a4', 70), ('rechunk-split-efe00704571913867602840a1828e0a4', 83), ('concatenate-c5f77d60c516216fbfd9f437c4ee2094', 5, 0, 0), ('rechunk-split-efe00704571913867602840a1828e0a4', 72)} (stimulus_id='handle-worker-cleanup-1741972078.887118')\n2025-03-14 17:07:58,906 - distributed.nanny - WARNING - Restarting worker\n\n\n\n---------------------------------------------------------------------------\nKilledWorker                              Traceback (most recent call last)\nCell In[3], line 4\n      2 client = Client()  # Starts a local Dask cluster\n      3 ds_chunk = ds.chunk({'time': 1000, 'lat': 25, 'lon': -1})\n----&gt; 4 mean_all_years = ds_chunk.sel(time=slice(\"1948\", \"1958\")).mean(dim=['lat', 'lon']).compute()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:1073, in Dataset.compute(self, **kwargs)\n   1049 \"\"\"Manually trigger loading and/or computation of this dataset's data\n   1050 from disk or a remote source into memory and return a new dataset.\n   1051 Unlike load, the original dataset is left unaltered.\n   (...)\n   1070 dask.compute\n   1071 \"\"\"\n   1072 new = self.copy(deep=False)\n-&gt; 1073 return new.load(**kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/dask/base.py:660, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    657     postcomputes.append(x.__dask_postcompute__())\n    659 with shorten_traceback():\n--&gt; 660     results = schedule(dsk, keys, **kwargs)\n    662 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/client.py:2427, in Client._gather(self, futures, errors, direct, local_worker)\n   2425     exception = st.exception\n   2426     traceback = st.traceback\n-&gt; 2427     raise exception.with_traceback(traceback)\n   2428 if errors == \"skip\":\n   2429     bad_keys.add(key)\n\nKilledWorker: Attempted to run task ('rechunk-split-efe00704571913867602840a1828e0a4', 19) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:38721. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.\n\n\n\n\nclient.close()\n\n2025-03-14 17:08:26,324 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 1269, in heartbeat\n    response = await retry_operation(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 441, in retry_operation\n    return await retry(\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 420, in retry\n    return await coro()\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1259, in send_recv_from_rpc\n    return await send_recv(comm=comm, op=key, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56270 remote=tcp://127.0.0.1:44601&gt;: Stream is closed\n2025-03-14 17:08:26,527 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 1269, in heartbeat\n    response = await retry_operation(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 441, in retry_operation\n    return await retry(\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 420, in retry\n    return await coro()\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1259, in send_recv_from_rpc\n    return await send_recv(comm=comm, op=key, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60454 remote=tcp://127.0.0.1:44601&gt;: Stream is closed\n2025-03-14 17:08:26,529 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 225, in read\n    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntornado.iostream.StreamClosedError: Stream is closed\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/worker.py\", line 1269, in heartbeat\n    response = await retry_operation(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 441, in retry_operation\n    return await retry(\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/utils_comm.py\", line 420, in retry\n    return await coro()\n           ^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1259, in send_recv_from_rpc\n    return await send_recv(comm=comm, op=key, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/core.py\", line 1018, in send_recv\n    response = await comm.read(deserializers=deserializers)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 236, in read\n    convert_stream_closed_error(self, e)\n  File \"/srv/conda/envs/notebook/lib/python3.12/site-packages/distributed/comm/tcp.py\", line 142, in convert_stream_closed_error\n    raise CommClosedError(f\"in {obj}: {exc}\") from exc\ndistributed.comm.core.CommClosedError: in &lt;TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:41388 remote=tcp://127.0.0.1:44601&gt;: Stream is closed\n\n\n\nmean_all_years\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 mean_all_years\n\nNameError: name 'mean_all_years' is not defined\n\n\n\n\nfrom pympler import muppy, summary\n\nall_objects = muppy.get_objects()\nsummary.print_(summary.summarize(all_objects))\n\n                                        types |   # objects |   total size\n============================================= | =========== | ============\n                                          str |      136046 |     22.66 MB\n                                         code |       51246 |     19.58 MB\n                                         dict |       47311 |     14.23 MB\n                                         type |        6829 |      8.27 MB\n  pandas.core.indexes.datetimes.DatetimeIndex |           2 |      5.01 MB\n            pandas._libs.index.DatetimeEngine |           2 |      4.03 MB\n                                        tuple |       54950 |      3.43 MB\n                                         list |       18366 |      1.61 MB\n                                  abc.ABCMeta |         715 |      1.09 MB\n                                          set |        2189 |      1.02 MB\n                                numpy.ndarray |          91 |    951.98 KB\n                                          int |       28479 |    816.06 KB\n                        weakref.ReferenceType |       10107 |    789.61 KB\n     _cython_3_0_11.cython_function_or_method |        2767 |    562.05 KB\n                          function (__init__) |        3406 |    532.19 KB\n\n\n\npip install pympler\n\nCollecting pympler\n  Downloading Pympler-1.1-py3-none-any.whl.metadata (3.6 kB)\nDownloading Pympler-1.1-py3-none-any.whl (165 kB)\nInstalling collected packages: pympler\nSuccessfully installed pympler-1.1\nNote: you may need to restart the kernel to use updated packages.\n\n\n\ndef compute_mean(subset):\n    return subset.mean(dim=['lat', 'lon'])\n\nds_chunk = ds.chunk({'time': 365, 'lat': 25, 'lon': 50})  # Chunk by year\n\n# Apply function to each year\nyearly_means = ds_chunk.groupby(\"time.year\").map(compute_mean)\n\n# Compute results\nyearly_means = yearly_means.compute()\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[10], line 10\n      7 yearly_means = ds_chunk.groupby(\"time.year\").map(compute_mean)\n      9 # Compute results\n---&gt; 10 yearly_means = yearly_means.compute()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:1073, in Dataset.compute(self, **kwargs)\n   1049 \"\"\"Manually trigger loading and/or computation of this dataset's data\n   1050 from disk or a remote source into memory and return a new dataset.\n   1051 Unlike load, the original dataset is left unaltered.\n   (...)\n   1070 dask.compute\n   1071 \"\"\"\n   1072 new = self.copy(deep=False)\n-&gt; 1073 return new.load(**kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:900, in Dataset.load(self, **kwargs)\n    897 chunkmanager = get_chunked_array_type(*lazy_data.values())\n    899 # evaluate all the chunked arrays simultaneously\n--&gt; 900 evaluated_data: tuple[np.ndarray[Any, Any], ...] = chunkmanager.compute(\n    901     *lazy_data.values(), **kwargs\n    902 )\n    904 for k, data in zip(lazy_data, evaluated_data, strict=False):\n    905     self.variables[k].data = data\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/daskmanager.py:85, in DaskManager.compute(self, *data, **kwargs)\n     80 def compute(\n     81     self, *data: Any, **kwargs: Any\n     82 ) -&gt; tuple[np.ndarray[Any, _DType_co], ...]:\n     83     from dask.array import compute\n---&gt; 85     return compute(*data, **kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/dask/base.py:660, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    657     postcomputes.append(x.__dask_postcompute__())\n    659 with shorten_traceback():\n--&gt; 660     results = schedule(dsk, keys, **kwargs)\n    662 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile /srv/conda/envs/notebook/lib/python3.12/queue.py:171, in Queue.get(self, block, timeout)\n    169 elif timeout is None:\n    170     while not self._qsize():\n--&gt; 171         self.not_empty.wait()\n    172 elif timeout &lt; 0:\n    173     raise ValueError(\"'timeout' must be a non-negative number\")\n\nFile /srv/conda/envs/notebook/lib/python3.12/threading.py:355, in Condition.wait(self, timeout)\n    353 try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    354     if timeout is None:\n--&gt; 355         waiter.acquire()\n    356         gotit = True\n    357     else:\n\nKeyboardInterrupt: \n\n\n\n\nimport xarray as xr\nimport gc\n\n# Open dataset with explicit chunking\nds_chunk = ds.chunk({'time': 365, 'lat': 25, 'lon': 50})  # One year per chunk\n\n# Process year by year\nmean_results = []\nfor year in range(1948, 1959):  # Process one year at a time\n    print(f\"Processing year {year}\")\n    ds_year = ds_chunk.sel(time=str(year))\n    yearly_mean = ds_year.mean(dim=['lat', 'lon']).compute()\n    \n    mean_results.append(yearly_mean)  # Store only computed value\n    \n    # Release memory\n    del yearly_mean\n    del ds_year\n    gc.collect()\n\n# Combine results (optional)\nfinal_mean = xr.concat(mean_results, dim=\"time\")\n\n# Cleanup\ndel ds_chunk, mean_results\ngc.collect()\n\nProcessing year 1948\nProcessing year 1949\nProcessing year 1950\nProcessing year 1951\nProcessing year 1952\nProcessing year 1953\nProcessing year 1954\nProcessing year 1955\nProcessing year 1956\nProcessing year 1957\nProcessing year 1958\n\n\n0\n\n\n\nimport xarray as xr\nimport gc\n\n# Process year by year\nmean_results = []\nfor year in range(1948, 1959):  # Process one year at a time\n    print(f\"Processing year {year}\")\n    ds_year = ds.sel(time=str(year)).load()\n    yearly_mean = ds_year.mean(dim=['lat', 'lon']).compute()\n    \n    mean_results.append(yearly_mean)  # Store only computed value\n    \n    # Release memory\n    del yearly_mean\n    del ds_year\n    gc.collect()\n\n# Combine results (optional)\nfinal_mean = xr.concat(mean_results, dim=\"time\")\n\n# Cleanup\ndel mean_results\ngc.collect()\n\nProcessing year 1948\nProcessing year 1949\nProcessing year 1950\nProcessing year 1951\nProcessing year 1952\nProcessing year 1953\nProcessing year 1954\nProcessing year 1955\nProcessing year 1956\nProcessing year 1957\nProcessing year 1958\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 22\n     19 final_mean = xr.concat(mean_results, dim=\"time\")\n     21 # Cleanup\n---&gt; 22 del ds_chunk, mean_results\n     23 gc.collect()\n\nNameError: name 'ds_chunk' is not defined\n\n\n\n\nds.load()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes Jupyter Hub and you will not need to install anything.\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "HackHours 2025",
    "section": "Schedule",
    "text": "Schedule\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Using large language models in Python to create data dashboards like this (Carl Boettiger, UC Berkeley) \nMar 21 - Parallel processing NODD model data with Coiled (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs"
  },
  {
    "objectID": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "href": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "title": "NMFS HackHours 2025",
    "section": "Why would I want to work in the cloud?",
    "text": "Why would I want to work in the cloud?\nWatch this video on “Enabling Analysis in the Cloud Using NASA Earth Science Data” by Michele Thorton, a NASA Openscapes mentor from the Oak Ridge National Laboratory Distributed Active Archive Center.\n\n\nMore earth data tutorials to explore!\nThe content and tutorials is a mix of content from workshops by the NASA Openscapes mentors (for example 2023 Cloud AGU Workshop), content developed by Carl Boettiger for NASA TOPS-T Cloud Native Geospatial in R & Python, content by NMFS CoastWatch, and other internal and external tutorials.\nHow do I get these tutorials into the JupyterHub? clone this https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/ and then look in the examples folder.\nHow do I clone? Since these are Jupyter/Python notebooks, easiest is cloning via JupyterLab.\n\nGo to JupyterLab. How? I closed the tab. Open the JupyterHub url again.\nClick on the little file icon on left until you are at the home directory.\nClick on the little Git icon on left. Which is it? Click on all the icons until you find it.\nWhen you see the ‘Clone repository’ button, click that. Paste in the url of the GitHub repo. I don’t see ‘Clone repository’. Go back to step 1. You are not in the home directory yet.\n\n\n\nFAQ\n\nCan I bring my own content/code to the event and JupyterHub? Absolutely, please do!! You can clone a GitHub repo or just upload files into the hub."
  },
  {
    "objectID": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "href": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "title": "HackHours in R and Python",
    "section": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET",
    "text": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET\nUse this form to sign-up to be alerted for future hackdays and Intro to JupyterHubs sessions: SIGN-UP FORM\nContact or questions: Eli Holmes (NOAA) - Type my name in your NOAA email, and my contact will pop up. Note, it uses “Eli” not “Elizabeth”.\nDuring these 1 hour hackhours, we will learn to do cloud computing with a JupyterHub set-up with geospatial packages and data. These sessions will get you more familiar with cloud-computing, JupyterHubs, Jupyter notebooks, and Python for geospatial analysis.\nAdd event to your calendar\nClick “HackHour 2024” for list of events and dates"
  },
  {
    "objectID": "content/reuse.html",
    "href": "content/reuse.html",
    "title": "Reuse Statement",
    "section": "",
    "text": "This content is released under CC0 Creative Commons.\nPermissive Re-Mix and No Attribution Needed: You may reuse the NMFS Open Science content in this repository—excluding the NMFS Open Science logo and any NOAA logos—in any way you like. You do not need permission. You do not need to give attribution, but if you use large parts of tutorials or content it is polite to give acknowledgement of the source. Please check each repository for its reuse statement. Some of the content is from outside of NMFS Open Science. This will be noted and you should check the original content for its reuse statement.",
    "crumbs": [
      "Reuse Statement"
    ]
  },
  {
    "objectID": "content/hackhours.html",
    "href": "content/hackhours.html",
    "title": "Hackhours",
    "section": "",
    "text": "These sessions are for NOAA staff to gain more familiarity with Jupyter Hubs and working with spatial data, esp big data hosted in the cloud in databases, via code and via geospatial packages in R and Python.\nFor more data science trainings and resource, see: * https://sites.google.com/noaa.gov/nmfs-hq-st-open-science/trainings * https://coastwatch.noaa.gov/cwn/training-courses.html * https://ioos.github.io/ioos_code_lab/content/intro.html",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/hackhours.html#about-nmfs-open-science",
    "href": "content/hackhours.html#about-nmfs-open-science",
    "title": "Hackhours",
    "section": "About NMFS Open Science",
    "text": "About NMFS Open Science\nWe provide technical and infrastructure support for any groups within NOAA Fisheries who would like computing support for their workshops or trainings; See our training page (NOAA internal). In September 2024, we launched a JupyterHub with a variety of specialized computing environments tailored to needs in fisheries and ocean modeling. We also support the development of a Docker stack tailored to R and Python workflows. We run regular workshops and trainings in reproducible science. See NMFS Openscapes and NMFS Open Science. If at NOAA see our internal site and the tabs for News and Training.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to providing a harassment-free learning experience for everyone. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Chats. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that harass based on sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private channel.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "content/jhub.html",
    "href": "content/jhub.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub is managed by Openscapes and developed in partnership with the International Interactive Computing Collaboration 2i2c. Launched in September 2024, the NMFS Openscapes JupyterHub joins the NASA Openscapes JupyterHub in providing a curated interactive computing platform to support training in earth and life science visualization, computing and analysis. The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries.",
    "crumbs": [
      "JupyterHub Skills",
      "About the Hub"
    ]
  },
  {
    "objectID": "content/setup.html",
    "href": "content/setup.html",
    "title": "Quick Start",
    "section": "",
    "text": "For those already familiar with JupyterLab and unix.\n\nGitHub Account\nA GitHub account is required to gain access to the JupyterHub and to clone the tutorials used in the Hackhours.\nHow do I get the tutorials into the JupyterHub?\n\nYou can upload files.\nEasiest is probably cloning a repo into the hub. See the JupyterHub Skills section if you do not know how to do this.\n\n\n\nAuthenticating to GitHub\nThis is a little different on the JupyterHub. See git authentication.\nFor content that uses the NASA Earthdata repository, you will need an Earthdata Login account. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create. Please jot down your username and password, as you need to enter it in the tutorials.\n\n\nWhen done, please stop the JupyterHub\nIf you are in JupyterLab in the browser:\n\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you still have the JupyterLab tab open in your browser:\n\nGo to the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you do not have the JupyterLab tab open in your browser because you closed that tab:\n\nGo to the url https://&lt;jupyterhub url&gt;/user/&lt;your username in the hub&gt;/lab/ That will open the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server",
    "crumbs": [
      "JupyterHub Skills",
      "Quick Start"
    ]
  },
  {
    "objectID": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "href": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "title": "NASA AGU 2023 Workshop Slides",
    "section": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "text": "Enabling Analysis in the Cloud Using NASA Earth Science Data"
  },
  {
    "objectID": "content/workshops.html",
    "href": "content/workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries. See the CoastWatch Training for all their events.\n\nOct-Dec 2024 Quarto Workshop\nOctober 25, 2024 Oceanographic Satellite and Animal Telemetry Data Training Course\nMay 17, 2024 Introduction to using earth data in the cloud for scientific workflows"
  },
  {
    "objectID": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "href": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "title": "Topic",
    "section": "Tutorials",
    "text": "Tutorials\nCarl will be talking about integrating LLMs into data dashboards specifically this dashboard (source code).\nUse Carl’s image\nWhen you start up the Jupyter Hub\n\nSelect Other for image\nIn the box, put rocker/binder:latest\n\nClone Carl’s demo repo\ncd ~\ngit clone https://github.com/boettiger-lab/geo-llm-r"
  },
  {
    "objectID": "topics-2025/2025-03-14-Boettiger/index.html#how-to-download-and-open-the-tutorials",
    "href": "topics-2025/2025-03-14-Boettiger/index.html#how-to-download-and-open-the-tutorials",
    "title": "Topic",
    "section": "How to download and open the tutorials",
    "text": "How to download and open the tutorials\n\nJupyter Hub\n\nStart the Jupyter Hub server &lt;nmfs-openscapes.2i2c.cloud&gt;\nClick the orange Open in Jupyter Hub button\n\n\n\nColab\n\nClick the Open in Colab button\n\n\n\nDownload\n\nDownload to your local computer\nYou will need to have Python and Jupyter Lab installed\nInstall any needed packages\n\n\n\nHow to clone the git repository\nAfter cloning, you will need to navigate to the tutorials in the topics-2025 directory.\nNever cloned the NMFSHackDays-2025 repo?\ncd ~\ngit clone https://github.com/nmfs-opensci/NMFSHackDays-2025\nHave cloned it but need to update? This is going to destroy any changes that you made to the repo to make it match the current state of the repo on GitHub.\ncd ~/NMFSHackDays-2025\ngit fetch origin\ngit reset --hard origin/main"
  },
  {
    "objectID": "topics-2025/index.html",
    "href": "topics-2025/index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes JupyterHub and you will not need to install anything. About the HackHours\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)\nDownload"
  },
  {
    "objectID": "topics-2025/index.html#schedule-links-to-content-on-left",
    "href": "topics-2025/index.html#schedule-links-to-content-on-left",
    "title": "HackHours 2025",
    "section": "Schedule (links to content on left)",
    "text": "Schedule (links to content on left)\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Introduction to the Nautilus HyperCluster for running containerized Big Data Applications (Carl Boettiger, UC Berkeley) \nMar 21 - Parallel processing NODD model data with Coiled (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs"
  },
  {
    "objectID": "topics-skills/02-git-clinic.html",
    "href": "topics-skills/02-git-clinic.html",
    "title": "Git Clinic",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-clinic.html#step-3",
    "href": "topics-skills/02-git-clinic.html#step-3",
    "title": "Git Clinic",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#prerequisites",
    "href": "topics-skills/02-git-jupyter.html#prerequisites",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "href": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to authenticate. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "href": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Git extension in JupyterLab",
    "text": "Git extension in JupyterLab\nWhen the instructions say to use or open or click the Git GUI, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#the-key-skills",
    "href": "topics-skills/02-git-jupyter.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "href": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub account (https://www.github.com/yourusername).\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nFirst make sure you are at the home directory level. Look at the folder icon under the blue launcher button. It should show, folder icon only like in this image. If not, then click on the folder icon.\n\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nClick on the git icon and then click “Clone a Repository” \nPaste in the URL of your repo from Step 1\nClick Clone. You can stay with the defaults for the checkboxes.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nClick on the README.md file in the Test repo.\nMake some changes to the file.\nClick the Git icon (in left navbar), and stage the change(s) by checking the “+” next to the files listed.\nAdd a commit message in the box.\nClick the Commit button at bottom.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git icon, look for the little cloud at the top. It is rather small. Click that to push changes.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in JupyterLab)\nFrom Git icon, click on the little cloud with a down arrow.\n\n\n\nActivity 1\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\n\nIn the Test repo, create a file called to &lt;yourname&gt;.md.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#footnotes",
    "href": "topics-skills/02-git-jupyter.html#footnotes",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#prerequisites",
    "href": "topics-skills/02-git-terminal.html#prerequisites",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#create-a-github-account",
    "href": "topics-skills/02-git-terminal.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "href": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Git in the terminal",
    "text": "Git in the terminal\nYou will need to open a terminal in JupyterLab or RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#the-key-skills",
    "href": "topics-skills/02-git-terminal.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "href": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\nThis skill is done on GitHub.com.\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nThese skills are done in a terminal from JupyterLab or RStudio.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nOpen a terminal.\nMake sure you are at the home directory level. Type this: cd ~\nClone the repo with this command. Replace yourname with your username. git clone https://www.github.com/yourname/Test\n\n\n\nSkill 3: Make some changes and commit your changes\nDo step 1 in your editor, JupyterLab or RStudio.\n\nMake some changes to the README.md file in the Test repo.\nGo to the terminal and make sure you are in your Test repo. cd ~/Test\nSee what has changed. You should see that README.md has changed. git status\nStage the change to the README.md git add README.md\nCommit the change. `git commit -m “small change”\n\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom the terminal, type git push\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub.com and commit\nFrom the terminal, type git pull\n\n\n\nActivity 1\nDo steps 1 to 3 in your editor, JupyterLab or RStudio, and steps 4 and 5 in the terminal on the JupyterHub.\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\nDo steps 1-3 on GitHub and step 4 from the terminal on the JupyterHub.\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nPull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#footnotes",
    "href": "topics-skills/02-git-terminal.html#footnotes",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html",
    "href": "topics-skills/02-intro-to-lab.html",
    "title": "Intro to JupyterLab",
    "section": "",
    "text": "When you start the JupyterHub, you will be in JupyterLab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#terminalshell",
    "href": "topics-skills/02-intro-to-lab.html#terminalshell",
    "title": "Intro to JupyterLab",
    "section": "Terminal/Shell",
    "text": "Terminal/Shell\nLog into the JupyterHub. If you do not see something like this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\nIf you have no experience working in a terminal, check out this self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\nBasic shell commands:\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\nrm filename remove a file\nrm -r directoryname remove a directory\nrm -rf directoryname force remove a directory; careful no recovery\n\nClose the terminal by clicking on the X in the terminal tab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#file-navigation",
    "href": "topics-skills/02-intro-to-lab.html#file-navigation",
    "title": "Intro to JupyterLab",
    "section": "File Navigation",
    "text": "File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on file icon below the blue button with a +. Now you see files in your home directory.\nClick on the folder icon that looks like this. Click on the actual folder image. \nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\n\nDelete a file\n\nDelete a file by right-clicking on it and clicking “Delete”",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "href": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "title": "Intro to JupyterLab",
    "section": "Create a new Jupyter notebook",
    "text": "Create a new Jupyter notebook\nFrom Launcher, click on the “Python 3” button, this will open a new Jupyter notebook.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "href": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "title": "Intro to JupyterLab",
    "section": "Basic Jupyter notebook navigation",
    "text": "Basic Jupyter notebook navigation\nA Jupyter notebook is a series of cells than can be code (default), markdown or raw text.\n\nLook at the top cell, this is a code cell which I could see if I click on the cell and look at the top navbar. Next to “Download”, it says “Code”. I can click that dropdown and change the cell type to markdown or raw.\nTo the left of the “Save” icon in the top navbar is a “+”. This will add a new cell.\nWithin a cell, you will see some icons on the right. Roll over these icons to see what they do.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "href": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "title": "Intro to JupyterLab",
    "section": "Running code in a cell",
    "text": "Running code in a cell\nTo run code in a cell, click in the cell and then hit “Shift Return”. You can also click “Run” in the menu or click the little right arrow in the top navbar above the cells.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "href": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "title": "Intro to JupyterLab",
    "section": "Creating and rendering markdown",
    "text": "Creating and rendering markdown\nCreate an new cell (you can click the “+” in the top navbar) and then change to markdown by clicking the dropdown next to “Download” in the top navbar. Type in some markdown and the run the cell (see above on how to run cells).",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "href": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "title": "Intro to JupyterLab",
    "section": "Running all cells in a notebook",
    "text": "Running all cells in a notebook\nUse the “Run” menu.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#install-packages",
    "href": "topics-skills/02-intro-to-lab.html#install-packages",
    "title": "Intro to JupyterLab",
    "section": "Install packages",
    "text": "Install packages\nUse pip install in a cell. This will not persist between sessions.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#learn-more",
    "href": "topics-skills/02-intro-to-lab.html#learn-more",
    "title": "Intro to JupyterLab",
    "section": "Learn more",
    "text": "Learn more\nThere are lots of tutorials on JupyterLab out there. Do a search to find content that works for you.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html",
    "href": "topics-skills/03-ScratchBucket.html",
    "title": "Using the S3 Scratch Bucket",
    "section": "",
    "text": "The JupyterHub has a preconfigured S3 “Scratch Bucket” that automatically deletes files after 7 days. This is a great resource for experimenting with large datasets and working collaboratively on a shared dataset with other users.",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "href": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "title": "Using the S3 Scratch Bucket",
    "section": "Access the scratch bucket",
    "text": "Access the scratch bucket\nThe scratch bucket is hosted at s3://nmfs-openscapes-scratch. The JupyterHub automatically sets an environment variable SCRATCH_BUCKET that appends a suffix to the s3 url with your GitHub username. This is intended to keep track of file ownership, stay organized, and prevent users from overwriting data!\nEveryone has full access to the scratch bucket, so be careful not to overwrite data from other users when uploading files. Also, any data you put there will be deleted 7 days after it is uploaded\nIf you need more permanent S3 bucket storage refer to AWS_S3_bucket documentation (left) to configure your own S3 Bucket.\nWe’ll use the S3FS Python package, which provides a nice interface for interacting with S3 buckets.\n\nimport os\nimport s3fs\nimport fsspec\nimport boto3\nimport xarray as xr\nimport geopandas as gpd\n\n\n# My GitHub username is `eeholmes`\nscratch = os.environ['SCRATCH_BUCKET']\nscratch \n\n's3://nmfs-openscapes-scratch/eeholmes'\n\n\n\n# But you can set a different S3 object prefix to use:\nscratch = 's3://nmfs-openscapes-scratch/hackhours'",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#uploading-data",
    "href": "topics-skills/03-ScratchBucket.html#uploading-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Uploading data",
    "text": "Uploading data\nIt’s great to store data in S3 buckets because this storage features very high network throughput. If many users are simultaneously accessing the same file on a spinning networked harddrive (/home/jovyan/shared) performance can be quite slow. S3 has much higher performance for such cases.\n\nUpload single file\n\nlocal_file = '~/NOAAHackDays/topics-2025/2025-02-14-earthdata/littlecube.nc'\n\nremote_object = f\"{scratch}/littlecube.nc\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\nOnce a bucket has files, I can list them. If the bucket is empty, you will get errors instead of [].\n\ns3 = s3fs.S3FileSystem()\ns3.ls(scratch)\n\n['nmfs-openscapes-scratch/hackhours/littlecube.nc']\n\n\n\ns3.stat(remote_object)\n\n{'Key': 'nmfs-openscapes-scratch/hackhours/littlecube.nc',\n 'LastModified': datetime.datetime(2025, 2, 13, 21, 41, 5, tzinfo=tzlocal()),\n 'ETag': '\"d73616d9e3ad84cf58a4a676b1e3d454\"',\n 'ChecksumAlgorithm': ['CRC32'],\n 'ChecksumType': 'FULL_OBJECT',\n 'Size': 50224,\n 'StorageClass': 'STANDARD',\n 'type': 'file',\n 'size': 50224,\n 'name': 'nmfs-openscapes-scratch/hackhours/littlecube.nc'}\n\n\n\n\nUpload a directory\n\nlocal_dir = '~/NOAAHackDays/topics-2025/resources'\n\n!ls -lh {local_dir}\n\ntotal 5.9M\n-rw-r--r-- 1 jovyan jovyan 5.9M Feb 12 21:05 e_sst.nc\ndrwxr-xr-x 3 jovyan jovyan  281 Feb 12 21:18 longhurst_v4_2010\n\n\n\ns3.upload(local_dir, scratch, recursive=True)\n\n[None, None, None, None, None, None, None, None, None]\n\n\nThe directory name is the directory name (only) of the local directory.\n\ns3.ls(f'{scratch}/resources')\n\n['nmfs-openscapes-scratch/hackhours/resources/e_sst.nc',\n 'nmfs-openscapes-scratch/hackhours/resources/longhurst_v4_2010']",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#accessing-data",
    "href": "topics-skills/03-ScratchBucket.html#accessing-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Accessing Data",
    "text": "Accessing Data\nSome software packages allow you to stream data directly from S3 Buckets. But you can always pull objects from S3 and work with local file paths.\nThis download-first, then analyze workflow typically works well for older file formats like HDF and netCDF that were designed to perform well on local hard drives rather than Cloud storage systems like S3.\nFor best performance do not work with data in your home directory. Instead use a local scratch space like `/tmp`\n\nremote_object\n\n's3://nmfs-openscapes-scratch/hackhours/littlecube.nc'\n\n\n\nlocal_object = '/tmp/test.nc'\ns3.download(remote_object, local_object)\n\n[None]\n\n\n\nds = xr.open_dataset(local_object)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)\n\n\nIf you don't want to think about downloading files you can let `fsspec` handle this behind the scenes for you! This way you only need to think about remote paths\n\nfs = fsspec.filesystem(\"simplecache\", \n                       cache_storage='/tmp/files/',\n                       same_names=True,  \n                       target_protocol='s3',\n                       )\n\n\n# The `simplecache` setting above will download the full file to /tmp/files\nprint(remote_object)\nwith fs.open(remote_object) as f:\n    ds = xr.open_dataset(f.name) # NOTE: pass f.name for local cached path\n\ns3://nmfs-openscapes-scratch/hackhours/littlecube.nc\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "href": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "title": "Using the S3 Scratch Bucket",
    "section": "Cloud-optimized formats",
    "text": "Cloud-optimized formats\nOther formats like COG, ZARR, Parquet are ‘Cloud-optimized’ and allow for very efficient streaming directly from S3. In other words, you do not need to download entire files and instead can easily read subsets of the data.\nThe example below reads a Parquet file directly into memory (RAM) from S3 without using a local disk:\n\n# first upload the file\nlocal_file = '~/NOAAHackDays/topics-2025/resources/example.parquet'\n\nremote_object = f\"{scratch}/example.parquet\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\n\ngf = gpd.read_parquet(remote_object)\ngf.head(2)\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n889953.0\nOceania\nFiji\nFJI\n5496\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\n\n\n1\n58005463.0\nAfrica\nTanzania\nTZA\n63177\nPOLYGON ((33.90371 -0.95, 34.07262 -1.05982, 3...",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "href": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "title": "Using the S3 Scratch Bucket",
    "section": "Advanced: Access Scratch bucket outside of JupyterHub",
    "text": "Advanced: Access Scratch bucket outside of JupyterHub\nLet’s say you have a lot of files on your laptop you want to work with. The S3 Bucket is a convient way to upload large datasets for collaborative analysis. To do this, you need to copy AWS Credentials from the JupyterHub to use on other machines. More extensive documentation on this workflow can be found in this repository https://github.com/scottyhq/jupyter-cloud-scoped-creds.\nThe following code must be run on the JupyterHub to get temporary credentials:\n\nclient = boto3.client('sts')\n\nwith open(os.environ['AWS_WEB_IDENTITY_TOKEN_FILE']) as f:\n    TOKEN = f.read()\n\nresponse = client.assume_role_with_web_identity(\n    RoleArn=os.environ['AWS_ROLE_ARN'],\n    RoleSessionName=os.environ['JUPYTERHUB_CLIENT_ID'],\n    WebIdentityToken=TOKEN,\n    DurationSeconds=3600\n)\n\nreponse will be a python dictionary that looks like this:\n{'Credentials': {'AccessKeyId': 'ASIAYLNAJMXY2KXXXXX',\n  'SecretAccessKey': 'J06p5IOHcxq1Rgv8XE4BYCYl8TG1XXXXXXX',\n  'SessionToken': 'IQoJb3JpZ2luX2VjEDsaCXVzLXdlc////0dsD4zHfjdGi/0+s3XKOUKkLrhdXgZ8nrch2KtzKyYyb...',\n  'Expiration': datetime.datetime(2023, 7, 21, 19, 51, 56, tzinfo=tzlocal())},\n  ...\nYou can copy and paste the values to another computer, and use them to configure your access to S3:\n\ns3 = s3fs.S3FileSystem(key=response['Credentials']['AccessKeyId'],\n                       secret=response['Credentials']['SecretAccessKey'],\n                       token=response['Credentials']['SessionToken'] )\n\n\n# Confirm your credentials give you access\ns3.ls('nmfs-openscapes-scratch', refresh=True)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html",
    "href": "topics-skills/04-learning.html",
    "title": "Learning Resources",
    "section": "",
    "text": "You are welcome to use the JupyterHub outside of the HackHours and workshops in order to facilitate your data science and cloud-computing learning. Here are some ideas:",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#python",
    "href": "topics-skills/04-learning.html#python",
    "title": "Learning Resources",
    "section": "Python",
    "text": "Python\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\n\nI did Python for Data Science and Machine Learning Bootcamp in Udemy\n\nThe same have deep-learning and ML courses\nHarvard https://pll.harvard.edu/catalog and MIT https://ocw.mit.edu/search/?q=python have lots of free material\nGeosciences\n\nhttps://cookbooks.projectpythia.org/\nhttps://nasa-openscapes.github.io/earthdata-cloud-cookbook/\nhttps://ioos.github.io/ioos_code_lab/content/intro.html\nhttps://github.com/coastwatch-training/CoastWatch-Tutorials\nhttps://github.com/NASAARSET\nhttps://earth-env-data-science.github.io/intro.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#r",
    "href": "topics-skills/04-learning.html#r",
    "title": "Learning Resources",
    "section": "R",
    "text": "R\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\nGeosciences\n\nhttps://github.com/USGS-R\nhttps://pmarchand1.github.io/atelier_rgeo/rgeo_workshop.html\nhttps://datacarpentry.github.io/r-raster-vector-geospatial/\nhttps://r.geocompx.org/\nhttps://bookdown.org/mcwimberly/gdswr-book/\nhttps://rspatial.org/index.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/index.html",
    "href": "topics-skills/index.html",
    "title": "JupyterHub",
    "section": "",
    "text": "Explore the topics in the left navigation bar to learn how to use JupyterLab, RStudio and Git in the JupyterHub plus the other resources available.",
    "crumbs": [
      "JupyterHub Skills"
    ]
  },
  {
    "objectID": "topics-2025/2025-planetarycomputer-r/index.html",
    "href": "topics-2025/2025-planetarycomputer-r/index.html",
    "title": "Microsoft Planetary Computer in R",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac-r/"
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html",
    "href": "topics-skills/01-intro-to-jupyterhub.html",
    "title": "Intro to JupyterHubs",
    "section": "",
    "text": "In this tutorial, you will get an overview of our JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://nmfs-openscapes.2i2c.cloud/. Click “Login to continue”. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\n\n\nNMFS Openscapes JupyterHub Login\n\n\n\nImage type: Python or R\nNext you select your image type from the drop-down. The default is a geospatial image with Python and R.\n\n\nVirtual Machine size\nYou’ll see a dropdown that allows you to choose the size of virtual machine. For the tutorials, you will only need the smallest virtual machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our virtual machine. This may take several minutes.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "href": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "title": "Intro to JupyterHubs",
    "section": "The Launcher",
    "text": "The Launcher\nWhen you are in the JupyterLab tab (note the Jupyter Logo), you will see a Launcher page. If you don’t see this, go to File &gt; New Launcher or click the blue button on the top left. From the Launcher, you will see a buttons to open a new Jupyter Notebook, RStudio, Desktop and VSCode on the top row and buttons to open a Terminal and other file types below.\n Clicking on the “Python 3”, Terminal, Text File and Markdown File buttons will open a new tab in JupyterLab. You can also use the File dropdown menu for these.\nTo get an overview of JupyterLab, go here: Intro to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "href": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "title": "Intro to JupyterHubs",
    "section": "RStudio",
    "text": "RStudio\nIf you click the RStudio button in Launcher, RStudio will open in a new browser tab.\n\n\n\nRStudio\n\n\nTo get an overview of RStudio, go here: Intro to RStudio",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "href": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day you should log out of the JupyterHub, although it will log you out automatically after 90 minutes. You will not lose work; your home directory is persistent. When you keep a session active it uses up AWS resources (costs money) because it keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the JupyterLab tab not the RStudio tab.\n\n\nFrom the JupyterLab browser tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click “Log Out”!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan’t find the JupyterLab tab? Go to https://nmfs-openscapes.2i2c.cloud/hub/home\n\n\nLogging out or stopping your server will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the JupyterLab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn’t lose your work unless you were uploading a file.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files that we have prepared for you.\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don’t work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "href": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "title": "Intro to JupyterHubs",
    "section": "Getting help",
    "text": "Getting help\n\nDiscussions https://github.com/nmfs-opensci/NOAAHackDays/discussions\nAdd an issue if you see something amiss or you would like a package added https://github.com/nmfs-opensci/NOAAHackDays/issues\nVisit the NMFS Open Science Google Space (NOAA only). Search Google Spaces to find.\nNMFS staff: The NMFS Python User Group and NMFS R User Group are great places to get help from fellow users for Python and R questions.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "href": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘Jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your files.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute, CryoCloud and Openscapes.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  }
]