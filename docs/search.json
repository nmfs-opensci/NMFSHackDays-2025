[
  {
    "objectID": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#basic-navigation",
    "href": "topics-skills/02-rstudio.html#basic-navigation",
    "title": "RStudio",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "href": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click “New Project” to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to “No”",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#installing-packages",
    "href": "topics-skills/02-rstudio.html#installing-packages",
    "title": "RStudio",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn’t have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‘piping’ in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won’t see it much in R from 10 years ago.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "href": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the “cog” icon in the Files tab of the bottom right panel. Then click Export.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#creating-files",
    "href": "topics-skills/02-rstudio.html#creating-files",
    "title": "RStudio",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let’s practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on “New Blank File” and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking “Run” 9. Try selecting lines of code and running that by clicking “Run”\n\n\ncsv file\n\nFrom the file panel, click on “New Blank File” and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let’s create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Presentation” on left of the popup and click “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#more-tips",
    "href": "topics-skills/02-rstudio.html#more-tips",
    "title": "RStudio",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-2025/2025-virtualizarr/index.html",
    "href": "topics-2025/2025-virtualizarr/index.html",
    "title": "VirtualiZarr",
    "section": "",
    "text": "https://virtualizarr.readthedocs.io/en/stable/index.html\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1677256"
  },
  {
    "objectID": "topics-2025/2025-icechunk/index.html",
    "href": "topics-2025/2025-icechunk/index.html",
    "title": "Icechunk",
    "section": "",
    "text": "https://earthmover.io/blog/icechunk/ https://icechunk.io/en/latest/ https://icechunk.io/en/latest/icechunk-python/quickstart/\n1 hr talk https://youtu.be/i-73e3_irpY?feature=shared Pangeo showcase: https://youtu.be/l_y8YOn8hm8?feature=shared"
  },
  {
    "objectID": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "href": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Other images on the hub",
    "text": "Other images on the hub\nUse the dropdown to select a non-default image on the hub. There are a variety. You can learn about them on the NMFS Open Science container images repo.",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "href": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Using other images not on the hub",
    "text": "Using other images not on the hub\nThe JupyterHub can run other images that are compatible with a JupyterHub, e.g. Binder images. When you start the hub, use the image dropdown to select “Other”:\n\nYou can add a url to a Docker image to this. For example, if you wanted to use the Pangeo notebook docker images image, you would paste one of these into the “Custom image” box.\nFrom DockerHub: pangeo/pangeo-notebook From Quay.io quay.io/pangeo/pangeo-notebook\nOther common data science images:\n\nJupyter Docker Stacks\nNASA Openscapes python\nRocker Binder image\nPangeo\ngeocompx\nGPU accelerated data science docker images",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-a-github-repo",
    "href": "topics-skills/04-other-images.html#using-a-github-repo",
    "title": "Using other images on the JupyterHub",
    "section": "Using a GitHub repo",
    "text": "Using a GitHub repo\nYou can also create an environment with a MyBinder.org compatible GitHub repo. By selecting the “Build your own image” option.\n\n\nSimple example for Python\nThere are two ways to do this. Either via a conda environment or a pip install.\nconda example\n\nPut an environment.yml file in your GitHub repo at the base level with your Python packages that you need.\nCopy the url to your repo and paste that into the “Repository” box (above).\n\nenvironment.yml\nname: example-environment\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.11\n  - numpy\n  - psutil\n  - toolz\n  - matplotlib\n  - dill\n  - pandas\n  - partd\n  - bokeh\n  - dask\npip install example\nYou will need requirements.txt for packages and runtime.txt for the Python version.\nrequirements.txt\nnumpy\nmatplotlib==3.*\nseaborn==0.13.*\npandas\nruntime.txt\npython-3.10\n\n\nSimple example for R\nr example\nYou will need install.R for packages and runtime.txt for the R version.\ninstall.R\ninstall.packages(\"tidyverse\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"httr\")\ninstall.packages(\"shinydashboard\")\ninstall.packages(\"leaflet\")\nruntime.txt\nr-4.3.2-2024-01-10",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html",
    "href": "topics-skills/03-earthdata.html",
    "title": "Earthdata Login",
    "section": "",
    "text": "NASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nTo programmatically access NASA data from within your Python or R scripts, you will need to enter your Earthdata username and password.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nRun the following commands on the JupyterHub:\n\n\n\n\n\n\nImportant\n\n\n\nIn the below command, replace EARTHDATA_LOGIN with your personal username and EARTHDATA_PASSWORD with your password\n\n\necho 'machine urs.earthdata.nasa.gov login \"EARTHDATA_LOGIN\" password \"EARTHDATA_PASSWORD\"' &gt; ~/.netrc\nchmod 0600 ~/.netrc",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html",
    "href": "topics-skills/03-AWS_S3_bucket.html",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "",
    "text": "This set of instructions will walk through how to setup an AWS S3 bucket for a specific project and how to configure that bucket to allow all members of the project team to have access.\nThis notebook is from the CryoCloud documentation. THE CODE WILL NOT WORK SINCE YOU NEED TO AUTHENTICATE TO THE S3 BUCKET.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create an AWS account and S3 bucket",
    "text": "Create an AWS account and S3 bucket\nThe first step is to create an AWS account that will be billed to your particular project. This can be done using these instructions.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create AWS S3 bucket",
    "text": "Create AWS S3 bucket\nWithin your new AWS account, create an new S3 bucket:\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nChoose Create bucket\nName the bucket and select us-west-2 for the region\nLeave all other default options\nClick Create Bucket",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create a user",
    "text": "Create a user\nWithin the same AWS account, create a new IAM user:\n\nOn the AWS Console Home page, select the IAM service\nIn the navigation pane, select Users and then select Add users\nName the user and click Next\nAttach policies directly\nDo not select any policies\nClick Next\nCreate user\n\nOnce the user has been created, find the user’s ARN and copy it.\nNow, create access keys for this user:\n\nSelect Users and click the user that you created\nOpen the Security Credentials tab\nCreate access key\nSelect Command Line Interface (CLI)\nCheck the box to agree to the recommendation and click Next\nLeave the tag blank and click Create access key\nIMPORTANT: Copy the access key and the secret access key. This will be used later.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create the bucket policy",
    "text": "Create the bucket policy\nConfigure a policy for this S3 bucket that will allow the newly created user to access it.\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nSelect the new S3 bucket that you created\nOpen the Permissions tab\nAdd the following bucket policy, replacing USER_ARN with the ARN that you copied above and BUCKET_ARN with the bucket ARN, found on the Edit bucket policy page on the AWS console:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBucket\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"BUCKET_ARN\"\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:*Object\",\n            \"Resource\": \"BUCKET_ARN/*\"\n        }\n    ]\n}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Reading from the S3 bucket",
    "text": "Reading from the S3 bucket\n\nExample: ls bucket using s3fs\n\nimport s3fs\ns3 = s3fs.S3FileSystem(anon=False, profile='icesat2')\n\n\n\nExample: open HDF5 file using xarray\n\nimport s3fs\nimport xarray as xr\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nprint(ssh_ds)\n\n&lt;xarray.Dataset&gt;\nDimensions:      (Longitude: 2160, nv: 2, Latitude: 960, Time: 1)\nCoordinates:\n  * Longitude    (Longitude) float32 0.08333 0.25 0.4167 ... 359.6 359.8 359.9\n  * Latitude     (Latitude) float32 -79.92 -79.75 -79.58 ... 79.58 79.75 79.92\n  * Time         (Time) datetime64[ns] 1992-10-10T12:00:00\nDimensions without coordinates: nv\nData variables:\n    Lon_bounds   (Longitude, nv) float32 ...\n    Lat_bounds   (Latitude, nv) float32 ...\n    Time_bounds  (Time, nv) datetime64[ns] ...\n    SLA          (Time, Latitude, Longitude) float32 ...\n    SLA_ERR      (Time, Latitude, Longitude) float32 ...\nAttributes: (12/21)\n    Conventions:            CF-1.6\n    ncei_template_version:  NCEI_NetCDF_Grid_Template_v2.0\n    Institution:            Jet Propulsion Laboratory\n    geospatial_lat_min:     -79.916664\n    geospatial_lat_max:     79.916664\n    geospatial_lon_min:     0.083333336\n    ...                     ...\n    version_number:         2205\n    Data_Pnts_Each_Sat:     {\"16\": 661578, \"1001\": 636257}\n    source_version:         commit dc95db885c920084614a41849ce5a7d417198ef3\n    SLA_Global_MEAN:        -0.0015108844021796562\n    SLA_Global_STD:         0.09098986023297456\n    latency:                final\n\n\n\nimport s3fs\n\nimport xarray as xr\n\nimport hvplot.xarray\nimport holoviews as hv\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_da = ssh_ds.SLA\n\nssh_da.hvplot.image(x='Longitude', y='Latitude', cmap='Spectral_r', geo=True, tiles='ESRI', global_extent=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nExample: read a geotiff using rasterio\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsession = rasterio.env.Env(profile_name='icesat2')\n\nurl = 's3://gris-outlet-glacier-seasonality-icesat2/out.tif'\n\nwith session:\n    with rasterio.open(url) as ds:\n        print(ds.profile)\n        band1 = ds.read(1)\n        \nband1[band1==-9999] = np.nan\nplt.imshow(band1)\nplt.colorbar()\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -9999.0, 'width': 556, 'height': 2316, 'count': 1, 'crs': CRS.from_epsg(3413), 'transform': Affine(50.0, 0.0, -204376.0,\n       0.0, -50.0, -2065986.0), 'blockysize': 3, 'tiled': False, 'interleave': 'band'}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Writing to the S3 bucket",
    "text": "Writing to the S3 bucket\n\ns3 = s3fs.core.S3FileSystem(profile='icesat2')\n\nwith s3.open('gris-outlet-glacier-seasonality-icesat2/new-file', 'wb') as f:\n    f.write(2*2**20 * b'a')\n    f.write(2*2**20 * b'a') # data is flushed and file closed\n\ns3.du('gris-outlet-glacier-seasonality-icesat2/new-file')\n\n4194304",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#what-is-git-and-github",
    "href": "topics-skills/02-git.html#what-is-git-and-github",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‘container’ for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#some-basic-git-jargon",
    "href": "topics-skills/02-git.html#some-basic-git-jargon",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and “commit” that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#overview",
    "href": "topics-skills/02-git.html#overview",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a “trunk-based workflow”.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe’ll do this",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#the-key-skills",
    "href": "topics-skills/02-git.html#the-key-skills",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "The Key Skills",
    "text": "The Key Skills\nThese basic skills are all you need to learn to get started:\n\nSkill 1: Create a blank repo on GitHub (the remote or origin)\nSkill 2: Clone your GitHub repo to your local computer (in our case the JupyterHub)\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub (the remote or origin)\nSkill 1b: Create a new repo from some else’s GitHub repository\n\nIn the next tutorials, you will practice these in RStudio or JuptyerHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#prerequisites",
    "href": "topics-skills/02-git-rstudio.html#prerequisites",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "href": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "href": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Git tab in RStudio",
    "text": "Git tab in RStudio\nWhen the instructions say to use or open or click the Git tab, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#the-key-skills",
    "href": "topics-skills/02-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "href": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPaste in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nActivity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me in RStudio\n\n\nActivity 2\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nGo to RStudio and pull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#footnotes",
    "href": "topics-skills/02-git-rstudio.html#footnotes",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html",
    "href": "topics-skills/02-git-jupyter-old.html",
    "title": "Git in JupyterLab",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html#step-3",
    "href": "topics-skills/02-git-jupyter-old.html#step-3",
    "title": "Git in JupyterLab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "href": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "title": "GitHub Authentication",
    "section": "Tell Git who you are",
    "text": "Tell Git who you are\nFirst open a terminal and run these lines. Replace &lt;your email&gt; with your email and remove the angle brackets.\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#authentication",
    "href": "topics-skills/02-git-authentication.html#authentication",
    "title": "GitHub Authentication",
    "section": "Authentication",
    "text": "Authentication\nYou need to authenticate to GitHub so you can push your local changes up to GitHub. There are a few ways to do this. For the JupyterHub, we will mainly use gh-scroped-creds which is a secure app that temporarily stores your GitHub credentials on a JupyterHub. But we will also show you a way to store your credentials in a file that works on any computer, including a virtual computer like the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "href": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "title": "GitHub Authentication",
    "section": "Preferred: gh-scoped-creds",
    "text": "Preferred: gh-scoped-creds\nIf you get the error that it cannot find gh-scoped-creds, then type\npip install gh-scoped-creds\nin a termnal.\n\nOpen a terminal\nType gh-scoped-creds\nFollow the instructions\nFIRST TIME: Make sure to follow the second pop-up instructions and tell it what repos it is allowed to interact with. You have to go through a number of pop up windows.\n\nJump down to the “Test” section to test.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "href": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "title": "GitHub Authentication",
    "section": "Also works: Set up authentication with a Personal Token",
    "text": "Also works: Set up authentication with a Personal Token\nThis will store your credentials in a file on the hub. This is not as secure since the file is unencrypted but sometimes gh-scoped-creds will not be an option.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nOpen a terminal in JupyterLab or RStudio\nPaste these 3 lines of code into the terminal\n\ngit config --global credential.helper store",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#test",
    "href": "topics-skills/02-git-authentication.html#test",
    "title": "GitHub Authentication",
    "section": "Test",
    "text": "Test\n\nGo to https://github.com/new\nCreate a PRIVATE repo called “test”\nMake sure to check the “Add a README file” box!\nOpen a terminal and type these lines. Make sure to replace &lt;username&gt;\n\ngit clone https://github.com/&lt;username&gt;/test\n\nIf you properly authenticated, git will ask for your username and password. At the password, paste in the TOKEN not your actual password.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/test.html",
    "href": "topics-2025/2025-opendap/test.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "Cases where opendap uses authentication. Some use .dodsrc file to tell it to use .netrc auth. https://tds.mercator-ocean.fr/userguide/user_guide.html\n\nimport xarray as xr\nurl=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\ndatapgn = xr.open_dataset(url)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:211, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    210 try:\n--&gt; 211     file = self._cache[self._key]\n    212 except KeyError:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56, in LRUCache.__getitem__(self, key)\n     55 with self._lock:\n---&gt; 56     value = self._cache[key]\n     57     self._cache.move_to_end(key)\n\nKeyError: [&lt;class 'netCDF4._netCDF4.Dataset'&gt;, ('https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '0229c544-0e21-4f3a-a8e0-5fa1693d0a2f']\n\nDuring handling of the above exception, another exception occurred:\n\nOSError                                   Traceback (most recent call last)\nCell In[3], line 3\n      1 import xarray as xr\n      2 url=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\n----&gt; 3 datapgn = xr.open_dataset(url)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:666, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    644 def open_dataset(\n    645     self,\n    646     filename_or_obj: str | os.PathLike[Any] | ReadBuffer | AbstractDataStore,\n   (...)\n    663     autoclose=False,\n    664 ) -&gt; Dataset:\n    665     filename_or_obj = _normalize_path(filename_or_obj)\n--&gt; 666     store = NetCDF4DataStore.open(\n    667         filename_or_obj,\n    668         mode=mode,\n    669         format=format,\n    670         group=group,\n    671         clobber=clobber,\n    672         diskless=diskless,\n    673         persist=persist,\n    674         auto_complex=auto_complex,\n    675         lock=lock,\n    676         autoclose=autoclose,\n    677     )\n    679     store_entrypoint = StoreBackendEntrypoint()\n    680     with close_on_error(store):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:452, in NetCDF4DataStore.open(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\n    448     kwargs[\"auto_complex\"] = auto_complex\n    449 manager = CachingFileManager(\n    450     netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n    451 )\n--&gt; 452 return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:393, in NetCDF4DataStore.__init__(self, manager, group, mode, lock, autoclose)\n    391 self._group = group\n    392 self._mode = mode\n--&gt; 393 self.format = self.ds.data_model\n    394 self._filename = self.ds.filepath()\n    395 self.is_remote = is_remote_uri(self._filename)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:461, in NetCDF4DataStore.ds(self)\n    459 @property\n    460 def ds(self):\n--&gt; 461     return self._acquire()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:455, in NetCDF4DataStore._acquire(self, needs_lock)\n    454 def _acquire(self, needs_lock=True):\n--&gt; 455     with self._manager.acquire_context(needs_lock) as root:\n    456         ds = _nc4_require_group(root, self._group, self._mode)\n    457     return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/contextlib.py:137, in _GeneratorContextManager.__enter__(self)\n    135 del self.args, self.kwds, self.func\n    136 try:\n--&gt; 137     return next(self.gen)\n    138 except StopIteration:\n    139     raise RuntimeError(\"generator didn't yield\") from None\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:199, in CachingFileManager.acquire_context(self, needs_lock)\n    196 @contextlib.contextmanager\n    197 def acquire_context(self, needs_lock=True):\n    198     \"\"\"Context manager for acquiring a file.\"\"\"\n--&gt; 199     file, cached = self._acquire_with_cache_info(needs_lock)\n    200     try:\n    201         yield file\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:217, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    215     kwargs = kwargs.copy()\n    216     kwargs[\"mode\"] = self._mode\n--&gt; 217 file = self._opener(*self._args, **kwargs)\n    218 if self._mode == \"w\":\n    219     # ensure file doesn't get overridden when opened again\n    220     self._mode = \"a\"\n\nFile src/netCDF4/_netCDF4.pyx:2521, in netCDF4._netCDF4.Dataset.__init__()\n\nFile src/netCDF4/_netCDF4.pyx:2158, in netCDF4._netCDF4._ensure_nc_success()\n\nOSError: [Errno -77] NetCDF: Access failure: 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0'\n\n\n\n\n#url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, decode_times=True)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile src/netCDF4/_netCDF4.pyx:5079, in netCDF4._netCDF4.Variable.__getitem__()\n\nFile src/netCDF4/_netCDF4.pyx:6051, in netCDF4._netCDF4.Variable._get()\n\nFile src/netCDF4/_netCDF4.pyx:2164, in netCDF4._netCDF4._ensure_nc_success()\n\nRuntimeError: NetCDF: Access failure\n\nDuring handling of the above exception, another exception occurred:\n\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 2\n      1 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\n----&gt; 2 ds = xr.open_dataset(url, decode_times=True)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:681, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    679 store_entrypoint = StoreBackendEntrypoint()\n    680 with close_on_error(store):\n--&gt; 681     ds = store_entrypoint.open_dataset(\n    682         store,\n    683         mask_and_scale=mask_and_scale,\n    684         decode_times=decode_times,\n    685         concat_characters=concat_characters,\n    686         decode_coords=decode_coords,\n    687         drop_variables=drop_variables,\n    688         use_cftime=use_cftime,\n    689         decode_timedelta=decode_timedelta,\n    690     )\n    691 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    394 stack_char_dim = (\n    395     _item_or_default(concat_characters, k, True)\n    396     and v.dtype == \"S1\"\n    397     and v.ndim &gt; 0\n    398     and stackable(v.dims[-1])\n    399 )\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n    412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    224         if use_cftime is not None:\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n    237     var = variables.EndianCoder().decode(var)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1326 units = pop_to(attrs, encoding, \"units\")\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n   1338 data = lazy_elemwise_func(data, transform, dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    295 def _decode_cf_datetime_dtype(\n    296     data,\n    297     units: str,\n   (...)\n    303     # successfully. Otherwise, tracebacks end up swallowed by\n    304     # Dataset.__repr__ when users try to view their lazily decoded array.\n    305     values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306     example_value = np.concatenate(\n--&gt; 307         [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308     )\n    310     try:\n    311         result = decode_cf_datetime(\n    312             example_value, units, calendar, use_cftime, time_unit\n    313         )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     95 if isinstance(array, Variable):\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    136     return data\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    569 def __array__(\n    570     self, dtype: np.typing.DTypeLike = None, /, *, copy: bool | None = None\n    571 ) -&gt; np.ndarray:\n    572     if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573         return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574     else:\n    575         return np.asarray(self.get_duck_array(), dtype=dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    648     array = apply_indexer(self.array, self.key)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n    658 if isinstance(array, ExplicitlyIndexed):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:103, in NetCDF4ArrayWrapper.__getitem__(self, key)\n    102 def __getitem__(self, key):\n--&gt; 103     return indexing.explicit_indexing_adapter(\n    104         key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n    105     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n    991 \"\"\"Support explicit indexing by delegating to a raw indexing method.\n    992 \n    993 Outer and/or vectorized indexers are supported by indexing a second time\n   (...)\n   1010 Indexing result, in the form of a duck numpy-array.\n   1011 \"\"\"\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n   1016     indexable = NumpyIndexingAdapter(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:116, in NetCDF4ArrayWrapper._getitem(self, key)\n    114     with self.datastore.lock:\n    115         original_array = self.get_array(needs_lock=False)\n--&gt; 116         array = getitem(original_array, key)\n    117 except IndexError as err:\n    118     # Catch IndexError in netCDF4 and return a more informative\n    119     # error message.  This is most often called when an unsorted\n    120     # indexer is used before the data is loaded from disk.\n    121     msg = (\n    122         \"The indexing operation you are attempting to perform \"\n    123         \"is not valid on netCDF4.Variable object. Try loading \"\n    124         \"your data into memory first by calling .load().\"\n    125     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:267, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    262 msg = (\n    263     f\"getitem failed, waiting {next_delay} ms before trying again \"\n    264     f\"({max_retries - n} tries remaining). Full traceback: {traceback.format_exc()}\"\n    265 )\n    266 logger.debug(msg)\n--&gt; 267 time.sleep(1e-3 * next_delay)\n\nKeyboardInterrupt: \n\n\n\n\n#url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, decode_times=True)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile src/netCDF4/_netCDF4.pyx:5079, in netCDF4._netCDF4.Variable.__getitem__()\n\nFile src/netCDF4/_netCDF4.pyx:6051, in netCDF4._netCDF4.Variable._get()\n\nFile src/netCDF4/_netCDF4.pyx:2164, in netCDF4._netCDF4._ensure_nc_success()\n\nRuntimeError: NetCDF: Access failure\n\nDuring handling of the above exception, another exception occurred:\n\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[5], line 3\n      1 #url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\n      2 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n----&gt; 3 ds = xr.open_dataset(url, decode_times=True)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:681, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    679 store_entrypoint = StoreBackendEntrypoint()\n    680 with close_on_error(store):\n--&gt; 681     ds = store_entrypoint.open_dataset(\n    682         store,\n    683         mask_and_scale=mask_and_scale,\n    684         decode_times=decode_times,\n    685         concat_characters=concat_characters,\n    686         decode_coords=decode_coords,\n    687         drop_variables=drop_variables,\n    688         use_cftime=use_cftime,\n    689         decode_timedelta=decode_timedelta,\n    690     )\n    691 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    394 stack_char_dim = (\n    395     _item_or_default(concat_characters, k, True)\n    396     and v.dtype == \"S1\"\n    397     and v.ndim &gt; 0\n    398     and stackable(v.dims[-1])\n    399 )\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n    412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    224         if use_cftime is not None:\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n    237     var = variables.EndianCoder().decode(var)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1326 units = pop_to(attrs, encoding, \"units\")\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n   1338 data = lazy_elemwise_func(data, transform, dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    295 def _decode_cf_datetime_dtype(\n    296     data,\n    297     units: str,\n   (...)\n    303     # successfully. Otherwise, tracebacks end up swallowed by\n    304     # Dataset.__repr__ when users try to view their lazily decoded array.\n    305     values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306     example_value = np.concatenate(\n--&gt; 307         [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308     )\n    310     try:\n    311         result = decode_cf_datetime(\n    312             example_value, units, calendar, use_cftime, time_unit\n    313         )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     95 if isinstance(array, Variable):\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    136     return data\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    569 def __array__(\n    570     self, dtype: np.typing.DTypeLike = None, /, *, copy: bool | None = None\n    571 ) -&gt; np.ndarray:\n    572     if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573         return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574     else:\n    575         return np.asarray(self.get_duck_array(), dtype=dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    648     array = apply_indexer(self.array, self.key)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n    658 if isinstance(array, ExplicitlyIndexed):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:103, in NetCDF4ArrayWrapper.__getitem__(self, key)\n    102 def __getitem__(self, key):\n--&gt; 103     return indexing.explicit_indexing_adapter(\n    104         key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n    105     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n    991 \"\"\"Support explicit indexing by delegating to a raw indexing method.\n    992 \n    993 Outer and/or vectorized indexers are supported by indexing a second time\n   (...)\n   1010 Indexing result, in the form of a duck numpy-array.\n   1011 \"\"\"\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n   1016     indexable = NumpyIndexingAdapter(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:116, in NetCDF4ArrayWrapper._getitem(self, key)\n    114     with self.datastore.lock:\n    115         original_array = self.get_array(needs_lock=False)\n--&gt; 116         array = getitem(original_array, key)\n    117 except IndexError as err:\n    118     # Catch IndexError in netCDF4 and return a more informative\n    119     # error message.  This is most often called when an unsorted\n    120     # indexer is used before the data is loaded from disk.\n    121     msg = (\n    122         \"The indexing operation you are attempting to perform \"\n    123         \"is not valid on netCDF4.Variable object. Try loading \"\n    124         \"your data into memory first by calling .load().\"\n    125     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:267, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    262 msg = (\n    263     f\"getitem failed, waiting {next_delay} ms before trying again \"\n    264     f\"({max_retries - n} tries remaining). Full traceback: {traceback.format_exc()}\"\n    265 )\n    266 logger.debug(msg)\n--&gt; 267 time.sleep(1e-3 * next_delay)\n\nKeyboardInterrupt: \n\n\n\n\nimport pydap\nfrom pydap.util.urs import install_basic_client\ninstall_basic_client()\nfrom pydap.client import open_url\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[7], line 2\n      1 import pydap\n----&gt; 2 from pydap.util.urs import install_basic_client\n      3 install_basic_client()\n      4 from pydap.client import open_url\n\nModuleNotFoundError: No module named 'pydap.util'\n\n\n\n\n# this works\nfrom pydap.client import open_url\n\ndataset_url=\"https://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\npydap_dataset = open_url(dataset_url, protocol=\"dap4\")\n\n\nimport requests\nfrom pydap.client import open_url\n\n\n# this give redirect\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, engine=\"pydap\")\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    305 values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306 example_value = np.concatenate(\n--&gt; 307     [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308 )\n    310 try:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    572 if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573     return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574 else:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:47, in PydapArrayWrapper.__getitem__(self, key)\n     46 def __getitem__(self, key):\n---&gt; 47     return indexing.explicit_indexing_adapter(\n     48         key, self.shape, indexing.IndexingSupport.BASIC, self._getitem\n     49     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:55, in PydapArrayWrapper._getitem(self, key)\n     54 array = getattr(self.array, \"array\", self.array)\n---&gt; 55 result = robust_getitem(array, key, catch=ValueError)\n     56 result = np.asarray(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:341, in BaseType.__getitem__(self, index)\n    340 out = copy.copy(self)\n--&gt; 341 out.data = self._get_data_index(index)\n    342 if type(self.data).__name__ == \"BaseProxyDap4\":\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:373, in BaseType._get_data_index(self, index)\n    372 else:\n--&gt; 373     return self._data[index]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:379, in BaseProxyDap2.__getitem__(self, index)\n    371 r = GET(\n    372     url,\n    373     self.application,\n   (...)\n    376     verify=self.verify,\n    377 )\n--&gt; 379 raise_for_status(r)\n    380 dds, data = safe_dds_and_data(r, self.user_charset)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/net.py:51, in raise_for_status(response)\n     50     text = \"\"\n---&gt; 51 raise HTTPError(\n     52     detail=(\n     53         response.status\n     54         + \"\\n\"\n     55         + text\n     56         + \"\\n\"\n     57         + \"This is redirect error. These should not usually raise \"\n     58         + \"an error in pydap beacuse redirects are handled \"\n     59         + \"implicitly. If it failed it is likely due to a \"\n     60         + \"circular redirect.\"\n     61     ),\n     62     headers=response.headers,\n     63     comment=response.body,\n     64 )\n\nHTTPError: 302 Found\n&lt;!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;title&gt;302 Found&lt;/title&gt;\n&lt;/head&gt;&lt;body&gt;\n&lt;h1&gt;Found&lt;/h1&gt;\n&lt;p&gt;The document has moved &lt;a href=\"https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&amp;app_type=401&amp;client_id=e2WVk8Pw6weeLUKZYOxvTQ&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fgoldsmr4.gesdisc.eosdis.nasa.gov%2Fdata-redirect&amp;state=aHR0cHM6Ly9nb2xkc21yNC5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9vcGVuZGFwL2h5cmF4L01FUlJBMi9NMkkxTlhBU00uNS4xMi40LzE5ODAvMDEvTUVSUkEyXzEwMC5pbnN0MV8yZF9hc21fTnguMTk4MDAxMDEubmM0LmRvZHM%2FdGltZSU1QjA6MTowJTVE\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\nThis is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.\n\nThe above exception was the direct cause of the following exception:\n\nHTTPError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n----&gt; 2 ds = xr.open_dataset(url, engine=\"pydap\")\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:203, in PydapBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, application, session, output_grid, timeout, verify, user_charset)\n    201 store_entrypoint = StoreBackendEntrypoint()\n    202 with close_on_error(store):\n--&gt; 203     ds = store_entrypoint.open_dataset(\n    204         store,\n    205         mask_and_scale=mask_and_scale,\n    206         decode_times=decode_times,\n    207         concat_characters=concat_characters,\n    208         decode_coords=decode_coords,\n    209         drop_variables=drop_variables,\n    210         use_cftime=use_cftime,\n    211         decode_timedelta=decode_timedelta,\n    212     )\n    213     return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:412, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n   (...)\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n--&gt; 412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n    413 if decode_coords in [True, \"coordinates\", \"all\"]:\n    414     var_attrs = new_vars[k].attrs\n\nHTTPError: Failed to decode variable 'time': 302 Found\n&lt;!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;title&gt;302 Found&lt;/title&gt;\n&lt;/head&gt;&lt;body&gt;\n&lt;h1&gt;Found&lt;/h1&gt;\n&lt;p&gt;The document has moved &lt;a href=\"https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&amp;app_type=401&amp;client_id=e2WVk8Pw6weeLUKZYOxvTQ&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fgoldsmr4.gesdisc.eosdis.nasa.gov%2Fdata-redirect&amp;state=aHR0cHM6Ly9nb2xkc21yNC5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9vcGVuZGFwL2h5cmF4L01FUlJBMi9NMkkxTlhBU00uNS4xMi40LzE5ODAvMDEvTUVSUkEyXzEwMC5pbnN0MV8yZF9hc21fTnguMTk4MDAxMDEubmM0LmRvZHM%2FdGltZSU1QjA6MTowJTVE\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\nThis is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.\n\n\n\n\nfrom pydap.client import open_url\n\ndataset_url=\"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n\npydap_dataset = open_url(dataset_url, protocol=\"dap4\")\n\n\nfrom pydap.client import open_url\n\nurl=\"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\nds = xr.open_dataset(url, engine=\"pydap\")\n\n\nfrom pydap.client import open_url\n\nurls=[\"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\", \n      \"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"]\n\nds = xr.open_dataset(url, engine=\"pydap\")\n\n&lt;DatasetType with children 'U2M', 'TROPT', 'TROPPB', 'T2M', 'TQL', 'TOX', 'PS', 'V50M', 'DISPH', 'TO3', 'TS', 'T10M', 'TROPPT', 'TQI', 'SLP', 'TQV', 'V2M', 'TROPQ', 'V10M', 'U50M', 'U10M', 'QV2M', 'TROPPV', 'QV10M', 'lat', 'lon', 'time'&gt;\n\n\n\n%%time\nurl_DAP4 = \"dap4://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0310/PACE_OCI.20240310.L3m.DAY.CHL.V2_0.chlor_a.4km.NRT.nc\"\nds_full = open_url(url_DAP4)\n\nCPU times: user 23.4 ms, sys: 0 ns, total: 23.4 ms\nWall time: 1.16 s\n\n\n\n%time\nds = xr.open_dataset(url_DAP4, engine=\"pydap\")\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 5.01 µs\n\n\n\nprint('uncompressed dataset size [GBs]: ', ds['chlor_a'].nbytes / 1e9)\n\nuncompressed dataset size [GBs]:  0.1492992\n\n\n\nds['chlor_a']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'chlor_a' (/lat: 4320, /lon: 8640)&gt; Size: 149MB\n[37324800 values with dtype=float32]\nDimensions without coordinates: /lat, /lon\nAttributes:\n    long_name:      Chlorophyll Concentration, OCI Algorithm\n    units:          mg m^-3\n    standard_name:  mass_concentration_of_chlorophyll_in_sea_water\n    valid_min:      0.00100000005\n    valid_max:      100.0\n    reference:      Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a alg...\n    display_scale:  log\n    display_min:    0.00999999978\n    display_max:    20.0\n    Maps:           ('/lat', '/lon')xarray.DataArray'chlor_a'/lat: 4320/lon: 8640...[37324800 values with dtype=float32]Coordinates: (0)Indexes: (0)Attributes: (10)long_name :Chlorophyll Concentration, OCI Algorithmunits :mg m^-3standard_name :mass_concentration_of_chlorophyll_in_sea_watervalid_min :0.00100000005valid_max :100.0reference :Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a algorithms for oligotrophic oceans: A novel approach based on three-band reflectance difference, J. Geophys. Res., 117, C01011, doi:10.1029/2011JC007395.display_scale :logdisplay_min :0.00999999978display_max :20.0Maps :('/lat', '/lon')\n\n\n\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 40MB\nDimensions:  (time: 24, lat: 361, lon: 576)\nDimensions without coordinates: time, lat, lon\nData variables:\n    T2M      (time, lat, lon) float64 40MB ...\nAttributes: (12/36)\n    History:                           Original file generated: Sat May 31 17...\n    Comment:                           GMAO filename: d5124_m2_jan79.tavg1_2d...\n    Filename:                          MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    created:                           2025-01-07T19:09:32Z\n    build_dmrpp:                       3.21.0-526\n    bes:                               3.21.0-526\n    libdap:                            libdap-3.21.0-120\n    configuration:                     \\n# TheBESKeys::get_as_config()\\nAllow...\n    invocation:                        build_dmrpp -c /tmp/bes_conf_t086 -f /...xarray.DatasetDimensions:time: 24lat: 361lon: 576Coordinates: (0)Data variables: (1)T2M(time, lat, lon)float64...long_name :2-meter_air_temperatureunits :Kfmissing_value :999999987000000.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]Maps :('/time', '/lat', '/lon')[4990464 values with dtype=float64]Indexes: (0)Attributes: (36)History :Original file generated: Sat May 31 17:43:10 2014 GMTComment :GMAO filename: d5124_m2_jan79.tavg1_2d_slv_Nx.19800101.nc4Filename :MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4ProductionDateTime :Original file generated: Sat May 31 17:43:10 2014 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :1980-01-01RangeBeginningTime :00:00:00.000000RangeEndingDate :1980-01-01RangeEndingTime :23:59:59.000000created :2025-01-07T19:09:32Zbuild_dmrpp :3.21.0-526bes :3.21.0-526libdap :libdap-3.21.0-120configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.RootDirectory=/tmp/tmp3z1dyeym/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Data.RootDirectory=/dev/null\nBES.LogName=./bes.log\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.EnableCF=false\nH5.EnableCheckNameClashing=true\ninvocation :build_dmrpp -c /tmp/bes_conf_t086 -f /tmp/tmp3z1dyeym//MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4 -r /tmp/dmr__iclKFq -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\n\nds[\"T2M\"].isel(time=1).plot()\n\n\n\n\n\n\n\n\n\nurl = \"https://opendap.earthdata.nasa.gov/collections/C1276812863-GES_DISC/granules/M2T1NXSLV.5.12.4%3AMERRA2_100.tavg1_2d_slv_Nx.19800101.nc4?dap4.ce=/T2M\"\n\n\n%time\nds = xr.open_dataset(url, engine=\"pydap\", decode_cf=True)\n\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 4.53 µs\n\n\n\nprint(ds.dims)\n\nFrozenMappingWarningOnValuesAccess({'/time': 24, '/lat': 361, '/lon': 576})\n\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[32], line 1\n----&gt; 1 ds.isel(time=1).plot()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:3100, in Dataset.isel(self, indexers, drop, missing_dims, **indexers_kwargs)\n   3096     return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n   3098 # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n   3099 # lists, or zero or one-dimensional np.ndarray's\n-&gt; 3100 indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n   3102 variables = {}\n   3103 dims: dict[Hashable, int] = {}\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/utils.py:844, in drop_dims_from_indexers(indexers, dims, missing_dims)\n    842     invalid = indexers.keys() - set(dims)\n    843     if invalid:\n--&gt; 844         raise ValueError(\n    845             f\"Dimensions {invalid} do not exist. Expected one or more of {dims}\"\n    846         )\n    848     return indexers\n    850 elif missing_dims == \"warn\":\n    851     # don't modify input\n\nValueError: Dimensions {'time'} do not exist. Expected one or more of FrozenMappingWarningOnValuesAccess({'/time': 24, '/lat': 361, '/lon': 576})\n\n\n\n\n%time\nurl=\"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXFLX.5.12.4/1980/01/MERRA2_100.tavg1_2d_flx_Nx.19800101.nc4\"\n#ds = xr.open_dataset(url, engine=\"pydap\")\npydap_dataset = open_url(url, protocol=\"dap4\")\n\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 4.53 µs\n\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 xr.open_dataset(pydap_dataset)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:667, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    664     kwargs.update(backend_kwargs)\n    666 if engine is None:\n--&gt; 667     engine = plugins.guess_engine(filename_or_obj)\n    669 if from_array_kwargs is None:\n    670     from_array_kwargs = {}\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/plugins.py:194, in guess_engine(store_spec)\n    186 else:\n    187     error_msg = (\n    188         \"found the following matches with the input file in xarray's IO \"\n    189         f\"backends: {compatible_engines}. But their dependencies may not be installed, see:\\n\"\n    190         \"https://docs.xarray.dev/en/stable/user-guide/io.html \\n\"\n    191         \"https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\"\n    192     )\n--&gt; 194 raise ValueError(error_msg)\n\nValueError: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'kerchunk', 'pydap', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#overview",
    "href": "topics-2025/2025-opendap/5-usgs.html#overview",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nThe USGS LPDAAC server requires NASA Earthdata login authentication, but don’t require a EULA (as far as I know). However they have similar redirect issues as data that does require a EULA. The solution used for datasets that require a EULA seems to work for access.\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages\n\nimport xarray as xr\nimport pydap.client",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#usgs-lpdaac",
    "href": "topics-2025/2025-opendap/5-usgs.html#usgs-lpdaac",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "USGS LPDAAC",
    "text": "USGS LPDAAC\nTheir OPeNDAP server also uses NASA Earthdata login authentication.\n\n# load username and password\nimport netrc\n# Get credentials from .netrc\nauth_host = \"urs.earthdata.nasa.gov\"\ntry:\n    credentials = netrc.netrc().authenticators(auth_host)\n    if credentials:\n        username, _, password = credentials\n    else:\n        raise ValueError(\"No credentials found in .netrc!\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(\"Could not find ~/.netrc. Ensure it exists and is configured correctly.\")\n\n\nurl=\"https://opendap.cr.usgs.gov/opendap/hyrax/MOD13Q1.061/h09v06.ncml\"\n\n\nimport pydap\nfrom pydap.client import open_url\nfrom pydap.cas.urs import setup_session\nsession = setup_session(username, password)\n\n\n# this works\npydap_ds = open_url(url, session=session, protocol=\"dap4\")",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#conclusion",
    "href": "topics-2025/2025-opendap/5-usgs.html#conclusion",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with USGS data on its OPeNDAP server is hard if you want an xarray object.",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#references",
    "href": "topics-2025/2025-opendap/5-usgs.html#references",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://github.com/pydap/pydap/issues/188\nhttps://nsidc.org/data/user-resources/help-center/how-do-i-access-data-using-opendap#anchor-using-a-command-line-interface",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#overview",
    "href": "topics-2025/2025-opendap/3-nasa.html#overview",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nNASA servers require authentication and some require End User Licence Agreement (EULA). They are so so hard to work with. None of the standard ways to create xarrays with OPeNDAP links work. The only way I have successfully authenticated is with pydap but it seems to garble the netcdf dim names.\nPersonally, I would try first to find the data on https://search.earthdata.nasa.gov/search and try to use earthaccess which would point you to cloud links. The earthaccess workflow for creating data cubes with xarray.open_mfdataset is quite a bit easier with fewer gotchas.\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages\n\nimport xarray as xr\nimport pydap.client",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#using-data-on-opendap.earthdata.nasa.gov",
    "href": "topics-2025/2025-opendap/3-nasa.html#using-data-on-opendap.earthdata.nasa.gov",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Using data on opendap.earthdata.nasa.gov",
    "text": "Using data on opendap.earthdata.nasa.gov\nThis works.\n\n# doesn't point to specific file but we will spec protocol\nurl=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\n\n# this is another test url that works for this example\n# url=\"https://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\n\n%%time\n#from pydap.client import open_url\npydap_ds = pydap.client.open_url(url, protocol=\"dap4\")\n\nCPU times: user 33 ms, sys: 631 µs, total: 33.6 ms\nWall time: 1.44 s\n\n\n\n%%time\n# getting the data does work\ntest_ds = pydap_ds['lat'][:]\n\nCPU times: user 24.6 ms, sys: 6.92 ms, total: 31.5 ms\nWall time: 3.57 s\n\n\n\n%%time\n# this works too but garbles my variables; also I have to do that dap4 replace\n# too many gotchas with that dap4 replace\nds = xr.open_dataset(url.replace(\"https\", \"dap4\", 1), engine=\"pydap\", decode_cf=False)\nds\n\nCPU times: user 27.1 ms, sys: 8.29 ms, total: 35.3 ms\nWall time: 1.4 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 144MB\nDimensions:                    (/time: 1, /lat: 2400, /lon: 2400)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/19)\n    wind_speed                 (/time, /lat, /lon) int8 6MB ...\n    lon                        (/lon) float32 10kB ...\n    or_longitude               (/time, /lat, /lon) int16 12MB ...\n    time                       (/time) int32 4B ...\n    sses_standard_deviation    (/time, /lat, /lon) int8 6MB ...\n    sst_dtime                  (/time, /lat, /lon) int32 23MB ...\n    ...                         ...\n    dt_analysis                (/time, /lat, /lon) int8 6MB ...\n    satellite_zenith_angle     (/time, /lat, /lon) int8 6MB ...\n    lat                        (/lat) float32 10kB ...\n    adi_dtime_from_sst         (/time, /lat, /lon) int8 6MB ...\n    or_latitude                (/time, /lat, /lon) int16 12MB ...\n    sses_bias                  (/time, /lat, /lon) int8 6MB ...\nAttributes: (12/53)\n    Conventions:                CF-1.4\n    title:                      Sea Surface Temperature\n    summary:                    The L3C product derived from GOES16/ABI brigh...\n    references:                 Geostationary Sea Surface Temperature Product...\n    institution:                OSISAF\n    comment:                    None\n    ...                         ...\n    netcdf_version_id:          4.6.3\n    build_dmrpp:                3.20.9-91\n    bes:                        3.20.9-91\n    libdap:                     libdap-3.20.8-41\n    configuration:              \\n# TheBESKeys::get_as_config()\\nAllowedHosts...\n    invocation:                 build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648...xarray.DatasetDimensions:/time: 1/lat: 2400/lon: 2400Coordinates: (0)Data variables: (19)wind_speed(/time, /lat, /lon)int8..._FillValue :-128long_name :10m wind speedstandard_name :wind_speedunits :m s-1height :10 madd_offset :0.0scale_factor :1.0valid_min :0valid_max :127time_offset :0.0source :WSP-ECMWF-Forecastcomment :These wind speeds were created by the ECMWF and represent winds at 10 metres above the sea surface. Maps :()[5760000 values with dtype=int8]lon(/lon)float32...long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectionMaps :()[2400 values with dtype=float32]or_longitude(/time, /lat, /lon)int16..._FillValue :-32768long_name :original longitude of the SST valuestandard_name :longitudeunits :degrees_eastadd_offset :0.0scale_factor :0.01valid_min :-18000valid_max :18000comment :Original longitude of the SST valueMaps :()[5760000 values with dtype=int16]time(/time)int32...long_name :reference time of sst filestandard_name :timeaxis :Tunits :seconds since 1981-01-01 00:00:00Maps :()[1 values with dtype=int32]sses_standard_deviation(/time, /lat, /lon)int8..._FillValue :-128long_name :SSES standard deviationunits :kelvinadd_offset :1.0scale_factor :0.01valid_min :-127valid_max :127comment :Standard deviation estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()[5760000 values with dtype=int8]sst_dtime(/time, /lat, /lon)int32..._FillValue :-2147483648long_name :time difference from reference timeunits :secondsadd_offset :0.0scale_factor :1.0valid_min :-2147483647valid_max :2147483647comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981Maps :()[5760000 values with dtype=int32]solar_zenith_angle(/time, /lat, /lon)int8..._FillValue :-128long_name :solar zenith angleunits :angular_degreeadd_offset :90.0scale_factor :1.0valid_min :-90valid_max :90comment :The solar zenith angle at the time of the SST observations.Maps :()[5760000 values with dtype=int8]sea_ice_fraction(/time, /lat, /lon)int8..._FillValue :-128long_name :sea ice fractionstandard_name :sea_ice_area_fractionunits :Noneadd_offset :0.0scale_factor :0.01valid_min :0valid_max :100time_offset :0.0source :ICE-OSISAFcomment :Fractional sea ice cover from OSISAF ice productMaps :()[5760000 values with dtype=int8]l2p_flags(/time, /lat, /lon)int32...long_name :L2P flagsvalid_min :0valid_max :15flag_meanings :microwave land ice lakeflag_masks :[1, 2, 4, 8]comment :These flags are important to properly use the data.Maps :()[5760000 values with dtype=int32]sources_of_adi(/time, /lat, /lon)int8..._FillValue :-128long_name :sources of aerosol dynamic indicatorvalid_min :0valid_max :2flag_meanings :no_data AOD-NAAPS-ADI SDI-OSISAF-ADIflag_values :[0, 1, 2]comment :This variable provides a pixel by pixel description of where aerosol optical depth were derived from.Maps :()[5760000 values with dtype=int8]aerosol_dynamic_indicator(/time, /lat, /lon)int8..._FillValue :-128long_name :aerosol dynamic indicatorunits :Noneadd_offset :0.0scale_factor :0.1valid_min :0valid_max :127source :sources_of_adicomment :NoneMaps :()[5760000 values with dtype=int8]sea_surface_temperature(/time, /lat, /lon)int16..._FillValue :-32768long_name :sea surface subskin temperaturestandard_name :sea_surface_subskin_temperatureunits :kelvinadd_offset :273.15scale_factor :0.01valid_min :-300valid_max :4500depth :1 millimetersource :GOES_Imagercomment :Temperature of the subskin of the oceanMaps :()[5760000 values with dtype=int16]quality_level(/time, /lat, /lon)int8..._FillValue :-128long_name :quality level of SST pixelvalid_min :0valid_max :5flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualityflag_values :[0, 1, 2, 3, 4, 5]comment :These are the overall quality indicators and are used for all GHRSST SSTsMaps :()[5760000 values with dtype=int8]dt_analysis(/time, /lat, /lon)int8..._FillValue :-128long_name :deviation from SST analysis or reference climatologyunits :kelvinadd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127reference :OSTIAcomment :The difference between this SST and the previous day's SST analysisMaps :()[5760000 values with dtype=int8]satellite_zenith_angle(/time, /lat, /lon)int8..._FillValue :-128long_name :satellite zenith angleunits :angular_degreeadd_offset :0.0scale_factor :1.0valid_min :-90valid_max :90comment :The satellite zenith angle at the time of the SST observations.Maps :()[5760000 values with dtype=int8]lat(/lat)float32...long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectionMaps :()[2400 values with dtype=float32]adi_dtime_from_sst(/time, /lat, /lon)int8..._FillValue :-128long_name :time difference of ADI data from sst measurementunits :houradd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127comment :Difference in hours between the ADI and SST dataMaps :()[5760000 values with dtype=int8]or_latitude(/time, /lat, /lon)int16..._FillValue :-32768long_name :original latitude of the SST valuestandard_name :latitudeunits :degrees_northadd_offset :0.0scale_factor :0.01valid_min :-9000valid_max :9000comment :Original latitude of the SST valueMaps :()[5760000 values with dtype=int16]sses_bias(/time, /lat, /lon)int8..._FillValue :-128long_name :SSES bias estimateunits :kelvinadd_offset :0.0scale_factor :0.01valid_min :-127valid_max :127comment :Bias estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()[5760000 values with dtype=int8]Indexes: (0)Attributes: (53)Conventions :CF-1.4title :Sea Surface Temperaturesummary :The L3C product derived from GOES16/ABI brightness temperatures.references :Geostationary Sea Surface Temperature Product User Manual, http://www.osi-saf.orginstitution :OSISAFcomment :Nonelicense :All intellectual property rights of the Ocean & Sea Ice SAF products belong to EUMETSAT. The use of these products is granted to every user, free of charge. If users wish to use these products, EUMETSAT's copyright credit must be shown by displaying the words 'Copyright EUMETSAT' under each of the products shown. EUMETSAT offers no warranty and accepts no liability in respect of the Ocean & Sea Ice SAF products. EUMETSAT neither commits to nor guarantees the continuity, availability, or quality or suitability for any purpose of, the Ocean & Sea Ice SAF products.id :GOES16-OSISAF-L3C-v1.0product_id :OSI-207-bnaming_authority :org.ghrsstproduct_version :1.0gds_version_id :2.0file_quality_level :3spatial_resolution :0.05 degreenorthernmost_latitude :60.0southernmost_latitude :-60.0easternmost_longitude :-15.0westernmost_longitude :-135.0source :GOES_ABIplatform :GOES16sensor :GOES_ABIMetadata_Conventions :Unidata Dataset Discovery v1.0metadata_link :N/Akeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature keywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.05geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.05acknowledgment :In case SAF data (pre-operational or operational) has been used for the study described in a paper the following sentence would be an appropriate reference to the funding coming from EUMETSAT: The data from the EUMETSAT Satellite Application Facility on Ocean & Sea Ice  used in this study are accessible through the SAF's homepage http://www.osi-saf.orgcreator_name :O&SI SAFcreator_email :osi-saf.helpdesk@meteo.frcreator_url :http://www.osi-saf.orgproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L3Ccdm_data_type :gridhistory :METEO-FRANCE GEOSAFO v1.1.8uuid :DF556788-19E1-11ED-A08A-48DF370DAD10date_created :20220812T015542Zstart_time :20220812T004042Ztime_coverage_start :20220812T004042Zstop_time :20220812T011929Ztime_coverage_end :20220812T011929Znetcdf_version_id :4.6.3build_dmrpp :3.20.9-91bes :3.20.9-91libdap :libdap-3.20.8-41configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.Exclude=^\\..*;\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.Include=;\nBES.Catalog.catalog.RootDirectory=/tmp/tmph648lz17/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Container.Persistence=strict\nBES.Data.RootDirectory=/dev/null\nBES.DefaultResponseMethod=POST\nBES.FollowSymLinks=Yes\nBES.Group=group_name\nBES.Info.Buffered=no\nBES.Info.Type=xml\nBES.LogName=./bes.log\nBES.LogVerbose=no\nBES.Memory.GlobalArea.ControlHeap=no\nBES.Memory.GlobalArea.EmergencyPoolSize=1\nBES.Memory.GlobalArea.MaximumHeapSize=20\nBES.Memory.GlobalArea.Verbose=no\nBES.ProcessManagerMethod=multiple\nBES.ServerAdministrator=admin.email.address@your.domain.name\nBES.Uncompress.NumTries=10\nBES.Uncompress.Retry=2000\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.User=user_name\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.Cache.latlon.path=/tmp/latlon\nH5.Cache.latlon.prefix=l\nH5.Cache.latlon.size=20000\nH5.CheckIgnoreObj=false\nH5.DefaultHandleDimension=true\nH5.DisableStructMetaAttr=true\nH5.DiskCacheComp=true\nH5.DiskCacheCompThreshold=2.0\nH5.DiskCacheCompVarSize=10000\nH5.DiskCacheDataPath=/tmp\nH5.DiskCacheFilePrefix=c\nH5.DiskCacheFloatOnlyComp=true\nH5.DiskCacheSize=10000\nH5.DiskMetaDataCachePath=/tmp\nH5.EnableAddPathAttrs=true\nH5.EnableCF=false\nH5.EnableCFDMR=true\nH5.EnableCheckNameClashing=true\nH5.EnableDiskDDSCache=false\nH5.EnableDiskDataCache=false\nH5.EnableDiskMetaDataCache=false\nH5.EnableDropLongString=true\nH5.EnableEOSGeoCacheFile=false\nH5.EnableFillValueCheck=true\nH5.KeepVarLeadingUnderscore=false\nH5.LargeDataMemCacheEntries=0\nH5.MetaDataMemCacheEntries=300\nH5.SmallDataMemCacheEntries=0\ninvocation :build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648lz17//20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0.nc -r /tmp/dmr_HJF1 -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\n\n# Ug, I have to rename variables\n# Ug the dims do not use lat lon values\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"wind_speed\"].plot();\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/utils.py:260: RuntimeWarning: overflow encountered in scalar absolute\n  vlim = max(abs(vmin - center), abs(vmax - center))\n\n\n\n\n\n\n\n\n\n\n%%time\n# This is doing the same thing as engine=\"pydap\" I think but I don't have to know the dap4 replacement\n# ds created is the same though\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store, decode_cf=False)\n\nCPU times: user 1.54 ms, sys: 0 ns, total: 1.54 ms\nWall time: 1.55 ms",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#data-with-a-eula",
    "href": "topics-2025/2025-opendap/3-nasa.html#data-with-a-eula",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Data with a EULA",
    "text": "Data with a EULA\nThis fails with .netrc when there is a EULA. I have accepted the EULA. Works if I explicitly set my username and password. If I don’t, and let pydap use .netrc automatically, I get a redirect error.\n\nPrerequisites\nMake sure you have the GESDISC EULA accepted.\n\nLog into https://urs.earthdata.nasa.gov\nThen go here https://urs.earthdata.nasa.gov/profile\nThen click EULAs\nGo to unaccepted EULAs and make sure that GESDISC is accepted\n\n\n# the GESDISC data requires a EULA\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\n\n\n\nGet password from netrc file\n\nimport netrc\n# Get credentials from .netrc\nauth_host = \"urs.earthdata.nasa.gov\"\ntry:\n    credentials = netrc.netrc().authenticators(auth_host)\n    if credentials:\n        username, _, password = credentials\n    else:\n        raise ValueError(\"No credentials found in .netrc!\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(\"Could not find ~/.netrc. Ensure it exists and is configured correctly.\")\n\nNow I set up a session with that username and password.\n\n%%time\n# 4 seconds!\nimport pydap\nfrom pydap.client import open_url\nfrom pydap.cas.urs import setup_session\nsession = setup_session(username, password, check_url=eula_url)\npydap_ds = open_url(eula_url, session=session, protocol=\"dap4\")\n\nCPU times: user 72.4 ms, sys: 3.19 ms, total: 75.6 ms\nWall time: 4.43 s\n\n\n\n# but at least is works when we try to get data\ntest_ds = pydap_ds['lat'][:]\n\n\n%%time\n# this works but is a lot of syntax to remember\nds = xr.open_dataset(eula_url.replace(\"https\", \"dap4\", 1), engine=\"pydap\", decode_cf=False, session=session)\n\nCPU times: user 43.5 ms, sys: 389 µs, total: 43.8 ms\nWall time: 997 ms\n\n\n\n# yeah! I can get the data\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).plot();\n\n\n\n\n\n\n\n\n\n%%time\nimport xarray as xr\n# This works too but is so much slower\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)\n\nCPU times: user 40.7 ms, sys: 4.32 ms, total: 45 ms\nWall time: 1.76 s\n\n\n\n# I have to rename variables\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).plot();\n\n\n\n\n\n\n\n\n\n\nTrying to have pydap auto use my .netrc file\nThis fails when I try to get the data. I get redirect errors.\n\n%%time\n# this part works\nfrom pydap.client import open_url\nimport xarray as xr\npydap_ds = open_url(eula_url, protocol=\"dap4\")\n\nCPU times: user 38 ms, sys: 3.67 ms, total: 41.6 ms\nWall time: 1.04 s\n\n\nBut if I try to get the data, I get redirect errors\ntest_ds = pydap_ds['lat'][:]\nThis gives this error\n“This is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.”\nThis then also gives the same error.\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#creating-data-cubes",
    "href": "topics-2025/2025-opendap/3-nasa.html#creating-data-cubes",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Creating data cubes",
    "text": "Creating data cubes\nOur goal is not individual files rather data cubes from multiple files. We will use xarray.open_mfdataset but we need to make some tweaks.\n\nWe need to use dap4 instead of https. I don’t know what to do if that doesn’t work.\nWe need to concatenate using \\time not time since the dim name has that slash in it.\n\n\nExample 1 Non-EULA data\n\n# doesn't point to specific file but we will spec protocol\nurl1=\"dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\nurl2=\"dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220813010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220813_010000-v02.0-fv01.0\"\nurls = [url1, url2]\nurls\n\n['dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0',\n 'dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220813010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220813_010000-v02.0-fv01.0']\n\n\n\nds = xr.open_mfdataset(urls, engine=\"pydap\", combine=\"nested\", concat_dim=\"/time\", decode_cf=False)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 288MB\nDimensions:                    (/time: 2, /lat: 2400, /lon: 2400)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/19)\n    wind_speed                 (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    lon                        (/time, /lon) float32 19kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    or_longitude               (/time, /lat, /lon) int16 23MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    time                       (/time) int32 8B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    sses_standard_deviation    (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sst_dtime                  (/time, /lat, /lon) int32 46MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    ...                         ...\n    dt_analysis                (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    satellite_zenith_angle     (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    lat                        (/time, /lat) float32 19kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    adi_dtime_from_sst         (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    or_latitude                (/time, /lat, /lon) int16 23MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sses_bias                  (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\nAttributes: (12/53)\n    Conventions:                CF-1.4\n    title:                      Sea Surface Temperature\n    summary:                    The L3C product derived from GOES16/ABI brigh...\n    references:                 Geostationary Sea Surface Temperature Product...\n    institution:                OSISAF\n    comment:                    None\n    ...                         ...\n    netcdf_version_id:          4.6.3\n    build_dmrpp:                3.20.9-91\n    bes:                        3.20.9-91\n    libdap:                     libdap-3.20.8-41\n    configuration:              \\n# TheBESKeys::get_as_config()\\nAllowedHosts...\n    invocation:                 build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648...xarray.DatasetDimensions:/time: 2/lat: 2400/lon: 2400Coordinates: (0)Data variables: (19)wind_speed(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :10m wind speedstandard_name :wind_speedunits :m s-1height :10 madd_offset :0.0scale_factor :1.0valid_min :0valid_max :127time_offset :0.0source :WSP-ECMWF-Forecastcomment :These wind speeds were created by the ECMWF and represent winds at 10 metres above the sea surface. Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nlon(/time, /lon)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.75 kiB\n9.38 kiB\n\n\nShape\n(2, 2400)\n(1, 2400)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n          2400 2\n\n\n\n\nor_longitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original longitude of the SST valuestandard_name :longitudeunits :degrees_eastadd_offset :0.0scale_factor :0.01valid_min :-18000valid_max :18000comment :Original longitude of the SST valueMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :reference time of sst filestandard_name :timeaxis :Tunits :seconds since 1981-01-01 00:00:00Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n8 B\n4 B\n\n\nShape\n(2,)\n(1,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          2 1\n\n\n\n\nsses_standard_deviation(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES standard deviationunits :kelvinadd_offset :1.0scale_factor :0.01valid_min :-127valid_max :127comment :Standard deviation estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsst_dtime(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-2147483648long_name :time difference from reference timeunits :secondsadd_offset :0.0scale_factor :1.0valid_min :-2147483647valid_max :2147483647comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n43.95 MiB\n21.97 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsolar_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :solar zenith angleunits :angular_degreeadd_offset :90.0scale_factor :1.0valid_min :-90valid_max :90comment :The solar zenith angle at the time of the SST observations.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsea_ice_fraction(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sea ice fractionstandard_name :sea_ice_area_fractionunits :Noneadd_offset :0.0scale_factor :0.01valid_min :0valid_max :100time_offset :0.0source :ICE-OSISAFcomment :Fractional sea ice cover from OSISAF ice productMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nl2p_flags(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;long_name :L2P flagsvalid_min :0valid_max :15flag_meanings :microwave land ice lakeflag_masks :[1, 2, 4, 8]comment :These flags are important to properly use the data.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n43.95 MiB\n21.97 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsources_of_adi(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sources of aerosol dynamic indicatorvalid_min :0valid_max :2flag_meanings :no_data AOD-NAAPS-ADI SDI-OSISAF-ADIflag_values :[0, 1, 2]comment :This variable provides a pixel by pixel description of where aerosol optical depth were derived from.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\naerosol_dynamic_indicator(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :aerosol dynamic indicatorunits :Noneadd_offset :0.0scale_factor :0.1valid_min :0valid_max :127source :sources_of_adicomment :NoneMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsea_surface_temperature(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :sea surface subskin temperaturestandard_name :sea_surface_subskin_temperatureunits :kelvinadd_offset :273.15scale_factor :0.01valid_min :-300valid_max :4500depth :1 millimetersource :GOES_Imagercomment :Temperature of the subskin of the oceanMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nquality_level(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :quality level of SST pixelvalid_min :0valid_max :5flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualityflag_values :[0, 1, 2, 3, 4, 5]comment :These are the overall quality indicators and are used for all GHRSST SSTsMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\ndt_analysis(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :deviation from SST analysis or reference climatologyunits :kelvinadd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127reference :OSTIAcomment :The difference between this SST and the previous day's SST analysisMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsatellite_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :satellite zenith angleunits :angular_degreeadd_offset :0.0scale_factor :1.0valid_min :-90valid_max :90comment :The satellite zenith angle at the time of the SST observations.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nlat(/time, /lat)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.75 kiB\n9.38 kiB\n\n\nShape\n(2, 2400)\n(1, 2400)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n          2400 2\n\n\n\n\nadi_dtime_from_sst(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :time difference of ADI data from sst measurementunits :houradd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127comment :Difference in hours between the ADI and SST dataMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nor_latitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original latitude of the SST valuestandard_name :latitudeunits :degrees_northadd_offset :0.0scale_factor :0.01valid_min :-9000valid_max :9000comment :Original latitude of the SST valueMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsses_bias(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES bias estimateunits :kelvinadd_offset :0.0scale_factor :0.01valid_min :-127valid_max :127comment :Bias estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nIndexes: (0)Attributes: (53)Conventions :CF-1.4title :Sea Surface Temperaturesummary :The L3C product derived from GOES16/ABI brightness temperatures.references :Geostationary Sea Surface Temperature Product User Manual, http://www.osi-saf.orginstitution :OSISAFcomment :Nonelicense :All intellectual property rights of the Ocean & Sea Ice SAF products belong to EUMETSAT. The use of these products is granted to every user, free of charge. If users wish to use these products, EUMETSAT's copyright credit must be shown by displaying the words 'Copyright EUMETSAT' under each of the products shown. EUMETSAT offers no warranty and accepts no liability in respect of the Ocean & Sea Ice SAF products. EUMETSAT neither commits to nor guarantees the continuity, availability, or quality or suitability for any purpose of, the Ocean & Sea Ice SAF products.id :GOES16-OSISAF-L3C-v1.0product_id :OSI-207-bnaming_authority :org.ghrsstproduct_version :1.0gds_version_id :2.0file_quality_level :3spatial_resolution :0.05 degreenorthernmost_latitude :60.0southernmost_latitude :-60.0easternmost_longitude :-15.0westernmost_longitude :-135.0source :GOES_ABIplatform :GOES16sensor :GOES_ABIMetadata_Conventions :Unidata Dataset Discovery v1.0metadata_link :N/Akeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature keywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.05geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.05acknowledgment :In case SAF data (pre-operational or operational) has been used for the study described in a paper the following sentence would be an appropriate reference to the funding coming from EUMETSAT: The data from the EUMETSAT Satellite Application Facility on Ocean & Sea Ice  used in this study are accessible through the SAF's homepage http://www.osi-saf.orgcreator_name :O&SI SAFcreator_email :osi-saf.helpdesk@meteo.frcreator_url :http://www.osi-saf.orgproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L3Ccdm_data_type :gridhistory :METEO-FRANCE GEOSAFO v1.1.8uuid :DF556788-19E1-11ED-A08A-48DF370DAD10date_created :20220812T015542Zstart_time :20220812T004042Ztime_coverage_start :20220812T004042Zstop_time :20220812T011929Ztime_coverage_end :20220812T011929Znetcdf_version_id :4.6.3build_dmrpp :3.20.9-91bes :3.20.9-91libdap :libdap-3.20.8-41configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.Exclude=^\\..*;\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.Include=;\nBES.Catalog.catalog.RootDirectory=/tmp/tmph648lz17/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Container.Persistence=strict\nBES.Data.RootDirectory=/dev/null\nBES.DefaultResponseMethod=POST\nBES.FollowSymLinks=Yes\nBES.Group=group_name\nBES.Info.Buffered=no\nBES.Info.Type=xml\nBES.LogName=./bes.log\nBES.LogVerbose=no\nBES.Memory.GlobalArea.ControlHeap=no\nBES.Memory.GlobalArea.EmergencyPoolSize=1\nBES.Memory.GlobalArea.MaximumHeapSize=20\nBES.Memory.GlobalArea.Verbose=no\nBES.ProcessManagerMethod=multiple\nBES.ServerAdministrator=admin.email.address@your.domain.name\nBES.Uncompress.NumTries=10\nBES.Uncompress.Retry=2000\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.User=user_name\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.Cache.latlon.path=/tmp/latlon\nH5.Cache.latlon.prefix=l\nH5.Cache.latlon.size=20000\nH5.CheckIgnoreObj=false\nH5.DefaultHandleDimension=true\nH5.DisableStructMetaAttr=true\nH5.DiskCacheComp=true\nH5.DiskCacheCompThreshold=2.0\nH5.DiskCacheCompVarSize=10000\nH5.DiskCacheDataPath=/tmp\nH5.DiskCacheFilePrefix=c\nH5.DiskCacheFloatOnlyComp=true\nH5.DiskCacheSize=10000\nH5.DiskMetaDataCachePath=/tmp\nH5.EnableAddPathAttrs=true\nH5.EnableCF=false\nH5.EnableCFDMR=true\nH5.EnableCheckNameClashing=true\nH5.EnableDiskDDSCache=false\nH5.EnableDiskDataCache=false\nH5.EnableDiskMetaDataCache=false\nH5.EnableDropLongString=true\nH5.EnableEOSGeoCacheFile=false\nH5.EnableFillValueCheck=true\nH5.KeepVarLeadingUnderscore=false\nH5.LargeDataMemCacheEntries=0\nH5.MetaDataMemCacheEntries=300\nH5.SmallDataMemCacheEntries=0\ninvocation :build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648lz17//20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0.nc -r /tmp/dmr_HJF1 -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\nLet’s do some slicing and see how fast we can get the data.\n\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\n\n\n%%time\ntest = ds[\"wind_speed\"].isel(time=1).load()\n\nCPU times: user 51.7 ms, sys: 16.8 ms, total: 68.5 ms\nWall time: 3.87 s\n\n\n\n\nExample 2 EULA data\nWe have to do a bit of set-up so that things don’t go south with re-directs.\n\nimport netrc\n# Get credentials from .netrc\nauth_host = \"urs.earthdata.nasa.gov\"\ntry:\n    credentials = netrc.netrc().authenticators(auth_host)\n    if credentials:\n        username, _, password = credentials\n    else:\n        raise ValueError(\"No credentials found in .netrc!\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(\"Could not find ~/.netrc. Ensure it exists and is configured correctly.\")\n\n\nimport pydap\nfrom pydap.client import open_url\nfrom pydap.cas.urs import setup_session\nsession = setup_session(username, password)\n\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\n%%time\n# very fast and this is 1Tb of data\nds = xr.open_mfdataset(eula_urls, engine=\"pydap\", combine=\"nested\", concat_dim=\"/time\", decode_cf=False, session=session)\nds\n\nCPU times: user 137 ms, sys: 11.7 ms, total: 149 ms\nWall time: 2.25 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (/time: 48, /lat: 361, /lon: 576)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/50)\n    U2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    V250      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPT     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPPB    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    T2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TQL       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    ...        ...\n    T2MWET    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    U500      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    QV10M     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    lat       (/time, /lat) float64 139kB dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;\n    lon       (/time, /lon) float64 221kB dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;\n    time      (/time) int32 192B dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;\nAttributes: (12/31)\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000\n    Unlimited_Dimension:               timexarray.DatasetDimensions:/time: 48/lat: 361/lon: 576Coordinates: (0)Data variables: (50)U2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_temperature_using_blended_TROPP_estimateunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPB(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_blended_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPBMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_liquid_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_500_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTOX(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_odd_oxygenunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOXMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nOMEGA500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :omega_at_500_hPaunits :Pa s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_250_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_250_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MDEW(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :dew_point_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEWMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPBLTOP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :pbltop_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDPRS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_500_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nDISPH(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :zero_plane_displacement_heightunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPHMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH1000(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_1000_mbunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTO3(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_ozoneunits :Dobsons_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_skin_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_thermal_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQI(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_ice_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQIMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nSLP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :sea_level_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_850_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nZLCL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :lifting_condensation_levelunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_water_vaporunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_250_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPQ(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_850_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_850_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDTMP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_EPV_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_500_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MWET(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :wet_bulb_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWETMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nlat(/time, /lat)float64dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n135.38 kiB\n67.69 kiB\n\n\nShape\n(48, 361)\n(24, 361)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          361 48\n\n\n\n\nlon(/time, /lon)float64dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/lonMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n216.00 kiB\n108.00 kiB\n\n\nShape\n(48, 576)\n(24, 576)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          576 48\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;long_name :timeunits :minutes since 2016-06-01 00:30:00time_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n192 B\n96 B\n\n\nShape\n(48,)\n(24,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          48 1\n\n\n\n\nIndexes: (0)Attributes: (31)History :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000Unlimited_Dimension :time\n\n\n\nprint(f\"Dataset size: {ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 1876.77 MB\n\n\nBUT we have that redirect error again if we try to get some data from our data cube. With open_dataset, it recognized the session (with our password) and dealt with the redirects, but with open_mfdataset is is not working.\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).load()\ngives the error again.\n“This is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.”\n\n\nExample 3 from PyDap documentation\nIn this example, constraint expression is used just to get certain variables. See full notebook here.\n\nbaseURL = 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/'\nTemp_Salt = \"ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_\"\nyear = '2017-'\nmonth = '01'\nend_ = '_ECCO_V4r4_native_llc0090'\nCE = '?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time'\n\nTemp_2017 = [baseURL + Temp_Salt + year + f'{i:02}' + end_ + CE for i in range(1, 4)]\nTemp_2017\n\n['dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-02_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-03_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time']\n\n\nCreate data cube with open_mfdataset but not concat dim is /time not time. This takes a really long time, but if we didn’t do the constraint expression part, it would take much longer. So it is good to do that step.\n\n%%time\n# 13 seconds to assemble the data cube for a 126Mb dataset...slow\ntheta_salt_ds = xr.open_mfdataset(\n    Temp_2017, \n    engine='pydap',\n    parallel=True, \n    combine='nested', \n    concat_dim='/time',\n)\ntheta_salt_ds\n\nCPU times: user 610 ms, sys: 24.1 ms, total: 634 ms\nWall time: 9.17 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 126MB\nDimensions:  (/time: 3, /k: 50, /tile: 13, /j: 90, /i: 90)\nCoordinates:\n    time     (/time) datetime64[ns] 24B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nDimensions without coordinates: /time, /k, /tile, /j, /i\nData variables:\n    SALT     (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    THETA    (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    i        (/time, /i) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    j        (/time, /j) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    k        (/time, /k) int32 600B dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;\n    tile     (/time, /tile) int32 156B dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;\nAttributes: (12/62)\n    acknowledgement:                 This research was carried out by the Jet...\n    author:                          Ian Fenty and Ou Wang\n    cdm_data_type:                   Grid\n    comment:                         Fields provided on the curvilinear lat-l...\n    Conventions:                     CF-1.8, ACDD-1.3\n    coordinates_comment:             Note: the global 'coordinates' attribute...\n    ...                              ...\n    time_coverage_duration:          P1M\n    time_coverage_end:               2017-02-01T00:00:00\n    time_coverage_resolution:        P1M\n    time_coverage_start:             2017-01-01T00:00:00\n    title:                           ECCO Ocean Temperature and Salinity - Mo...\n    uuid:                            f5b7028c-4181-11eb-b7e6-0cc47a3f47b1xarray.DatasetDimensions:/time: 3/k: 50/tile: 13/j: 90/i: 90Coordinates: (1)time(/time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :center time of averaging periodaxis :Tbounds :time_bndscoverage_content_type :coordinatestandard_name :timeorigname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n24 B\n8 B\n\n\nShape\n(3,)\n(1,)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n           3 1\n\n\n\n\nData variables: (6)SALT(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Salinityunits :1e-3coverage_content_type :modelResultstandard_name :sea_water_salinitycomment :Defined using CF convention 'Sea water salinity is the salt content of sea water, often on the Practical Salinity Scale of 1978. However, the unqualified term 'salinity' is generic and does not necessarily imply any particular method of calculation. The units of salinity are dimensionless and the units attribute should normally be given as 1e-3 or 0.001 i.e. parts per thousand.' see https://cfconventions.org/Data/cf-standard-names/73/build/cf-standard-name-table.htmlvalid_min :17.106637954711914valid_max :41.26802444458008origname :SALTfullnamepath :/SALTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\nTHETA(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Potential temperature units :degree_Ccoverage_content_type :modelResultstandard_name :sea_water_potential_temperaturecomment :Sea water potential temperature is the temperature a parcel of sea water would have if moved adiabatically to sea level pressure. Note: the equation of state is a modified UNESCO formula by Jackett and McDougall (1995), which uses the model variable potential temperature as input assuming a horizontally and temporally constant pressure of $p_0=-g ho_{0} z$.valid_min :-2.2909388542175293valid_max :36.032955169677734origname :THETAfullnamepath :/THETAMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\ni(/time, /i)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Xlong_name :grid index in x for variables at tracer and 'v' locationsswap_dim :XCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'v' variables (e.g., VVEL) have the same x coordinate on the model grid.coverage_content_type :coordinateorigname :ifullnamepath :/iMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nj(/time, /j)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Ylong_name :grid index in y for variables at tracer and 'u' locationsswap_dim :YCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'u' variables (e.g., UVEL) have the same y coordinate on the model grid.coverage_content_type :coordinateorigname :jfullnamepath :/jMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nk(/time, /k)int32dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;axis :Zlong_name :grid index in z for tracer variablesswap_dim :Zcoverage_content_type :coordinateorigname :kfullnamepath :/kMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n600 B\n200 B\n\n\nShape\n(3, 50)\n(1, 50)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           50 3\n\n\n\n\ntile(/time, /tile)int32dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;long_name :lat-lon-cap tile indexcomment :The ECCO V4 horizontal model grid is divided into 13 tiles of 90x90 cells for convenience.coverage_content_type :coordinateorigname :tilefullnamepath :/tileMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n156 B\n52 B\n\n\nShape\n(3, 13)\n(1, 13)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           13 3\n\n\n\n\nIndexes: (0)Attributes: (62)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on the curvilinear lat-lon-cap 90 (llc90) native grid used in the ECCO model.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T14:39:59date_issued :2020-12-18T14:39:59date_metadata_modified :2021-03-15T21:56:21date_modified :2021-03-15T21:56:21geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :variablegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :variablegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :-6134.5geospatial_vertical_positive :upgeospatial_vertical_resolution :variablegeospatial_vertical_units :meterhistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECL5M-OTS44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE &gt; OCEANS &gt; SALINITY/DENSITY &gt; SALINITY, EARTH SCIENCE SERVICES &gt; MODELS &gt; EARTH SCIENCE REANALYSES/ASSIMILATION MODELS, EARTH SCIENCE &gt; OCEANS &gt; OCEAN TEMPERATURE &gt; POTENTIAL TEMPERATUREkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_TEMP_SALINITY_LLC0090GRID_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged ocean potential temperature and salinity on the lat-lon-cap 90 (llc90) native model grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2017-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2017-01-01T00:00:00title :ECCO Ocean Temperature and Salinity - Monthly Mean llc90 Grid (Version 4 Release 4)uuid :f5b7028c-4181-11eb-b7e6-0cc47a3f47b1\n\n\n\nprint(f\"Dataset size: {theta_salt_ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 126.36 MB\n\n\n\ntheta_salt_ds = theta_salt_ds.rename({\"/time\": \"time\", \"/j\": \"j\", \"/i\": \"i\", \"/tile\": \"tile\", \"/k\": \"k\"})\n\n/tmp/ipykernel_1511/704975482.py:1: UserWarning: rename '/time' to 'time' does not create an index anymore. Try using swap_dims instead or use set_index after rename to create an indexed coordinate.\n  theta_salt_ds = theta_salt_ds.rename({\"/time\": \"time\", \"/j\": \"j\", \"/i\": \"i\", \"/tile\": \"tile\", \"/k\": \"k\"})\n\n\n\ntheta_salt_ds[\"THETA\"].isel(time=1, tile=10, k=1).plot()",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#conclusion",
    "href": "topics-2025/2025-opendap/3-nasa.html#conclusion",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with NASA OPeNDAP servers is hard and if there is a EULA for that data source, it is even harder.",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#references",
    "href": "topics-2025/2025-opendap/3-nasa.html#references",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://pydap.github.io/pydap/intro.html\nhttps://opendap.github.io/documentation/tutorials/ClientAuthentication.html#_pydap\nhttps://github.com/OPENDAP/NASA-tutorials/tree/main",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers - authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#overview",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#overview",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nTHREDDS is a common type of data server that usually includes multiple ways to access the data. One of those ways is via the OPeNDAP protocol which allows you to subset the data, instead of downloading the whole data file. Here you will learn to use xarray’s open_mfdataset to create data cubes on THREDDS servers using the OPeNDAP protocol. This tutorial uses an example where the server doesn’t require authentication (username and password).\nAcknowledgments: This example was adapted from Ryan Abernathy https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#ncep-ncar-reanalysis-1",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#ncep-ncar-reanalysis-1",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "NCEP-NCAR Reanalysis 1",
    "text": "NCEP-NCAR Reanalysis 1\nFor our first example, we will use 4xDaily Air temperature at sigma level 995 data (air.sig995) from the NCEP-NCAR Reanalysis 1. First look at the THREDDS catalog to orient yourself to the file naming convention. We can click on one of the files an see our [access options]. We are looking for the OPeNDAP information as we need to get the url for that. Clicking the OPeNDAP link reveals the url format of the files:\nhttps://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995.1948.nc\nNow we can create the file urls.\n\nimport xarray as xr\n\n\nbase_url = 'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995'\n\nfiles = [f'{base_url}.{year}.nc' for year in range(1948, 2025)]\nlen(files)\n\n77\n\n\nOpen a single file.\n\nfiles[0]\n\n'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995.1948.nc'\n\n\n\n%%time\nds = xr.open_dataset(files[0]);\nds\n\nCPU times: user 119 ms, sys: 8.16 ms, total: 127 ms\nWall time: 392 ms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 62MB\nDimensions:  (lon: 144, time: 1464, lat: 73)\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 12kB 1948-01-01 ... 1948-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nData variables:\n    air      (time, lat, lon) float32 62MB ...\nAttributes:\n    Conventions:                     COARDS\n    title:                           4x daily NMC reanalysis (1948)\n    description:                     Data is from NMC initialized reanalysis\\...\n    platform:                        Model\n    history:                         created 99/05/11 by Hoop (netCDF2.3)\\nCo...\n    dataset_title:                   NCEP-NCAR Reanalysis 1\n    References:                      http://www.psl.noaa.gov/data/gridded/dat...\n    _NCProperties:                   version=2,netcdf=4.6.3,hdf5=1.10.5\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lon: 144time: 1464lat: 73Coordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 1948-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1948-12-31T06:00:00.000000000',\n       '1948-12-31T12:00:00.000000000', '1948-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144][15389568 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1948-12-29 12:00:00', '1948-12-29 18:00:00',\n               '1948-12-30 00:00:00', '1948-12-30 06:00:00',\n               '1948-12-30 12:00:00', '1948-12-30 18:00:00',\n               '1948-12-31 00:00:00', '1948-12-31 06:00:00',\n               '1948-12-31 12:00:00', '1948-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=1464, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (9)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelhistory :created 99/05/11 by Hoop (netCDF2.3)\nConverted to chunked, deflated non-packed NetCDF4 2014/09dataset_title :NCEP-NCAR Reanalysis 1References :http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis.html_NCProperties :version=2,netcdf=4.6.3,hdf5=1.10.5DODS_EXTRA.Unlimited_Dimension :time\n\n\nWe will load the file metadata with open_mfdataset and create our virtual data cube. It will take 10 seconds or so but it doesn’t use much memory as we are only reading and loading the file metadata.\n\n%%time\nds = xr.open_mfdataset(files, parallel=True);\n\nCPU times: user 1.71 s, sys: 137 ms, total: 1.84 s\nWall time: 8.94 s\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 5GB\nDimensions:  (time: 112500, lat: 73, lon: 144)\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 900kB 1948-01-01 ... 2024-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nData variables:\n    air      (time, lat, lon) float32 5GB dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;\nAttributes:\n    Conventions:                     COARDS\n    title:                           4x daily NMC reanalysis (1948)\n    description:                     Data is from NMC initialized reanalysis\\...\n    platform:                        Model\n    history:                         created 99/05/11 by Hoop (netCDF2.3)\\nCo...\n    dataset_title:                   NCEP-NCAR Reanalysis 1\n    References:                      http://www.psl.noaa.gov/data/gridded/dat...\n    _NCProperties:                   version=2,netcdf=4.6.3,hdf5=1.10.5\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:time: 112500lat: 73lon: 144Coordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 2024-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '2024-12-31T06:00:00.000000000',\n       '2024-12-31T12:00:00.000000000', '2024-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Data variables: (1)air(time, lat, lon)float32dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.41 GiB\n58.71 MiB\n\n\nShape\n(112500, 73, 144)\n(1464, 73, 144)\n\n\nDask graph\n77 chunks in 155 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             144 73 112500\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '2024-12-29 12:00:00', '2024-12-29 18:00:00',\n               '2024-12-30 00:00:00', '2024-12-30 06:00:00',\n               '2024-12-30 12:00:00', '2024-12-30 18:00:00',\n               '2024-12-31 00:00:00', '2024-12-31 06:00:00',\n               '2024-12-31 12:00:00', '2024-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=112500, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (9)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelhistory :created 99/05/11 by Hoop (netCDF2.3)\nConverted to chunked, deflated non-packed NetCDF4 2014/09dataset_title :NCEP-NCAR Reanalysis 1References :http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis.html_NCProperties :version=2,netcdf=4.6.3,hdf5=1.10.5DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nds['air'].isel(time=1).plot();\n\n\n\n\n\n\n\n\nThe data set is not that huge, but it is bigger than the 2Gb RAM in our minimal Jupyter Hub.\n\nprint(f\"{ds.nbytes / 1e9} Gb\")\n\n4.731300868 Gb\n\n\nBut even a small dataset would crash our 2Gb RAM.\n\nds_sub = ds.sel(time=slice(\"1948\",\"1958\"))\nprint(f\"{ds_sub.nbytes / 1e9} Gb\")\n# ds_sub.load() # this would crash a 2Gb RAM\n\n0.6759249 Gb",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#creating-daily-means",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#creating-daily-means",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Creating daily means",
    "text": "Creating daily means\nFor one year, we can create a daily mean since 1 year is not that much data and we can fit that into memory.\n\nds_mean = ds[\"air\"].sel(time=\"1949\").mean(dim=['lat', 'lon'])\nds_mean.plot();\n\n\n\n\n\n\n\n\n\n# resample to daily\nds_mean.resample(time='D').mean().plot();\n\n\n\n\n\n\n\n\nBut if we try to do all years at once, we will run out of memory.\n\nChunking the data\nFortunately, xarray will process our data in chunks rather than loading the whole data into memory. The data were chunked automatically into yearly chunks by xarray since we created using open_mfdataset. The chunks are 1460 = 4 x 365 days since the files are one year.\n\nprint(ds[\"air\"].chunks)\n\n((1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464), (73,), (144,))\n\n\nI am going to rechunk smaller to get to about 1 Mb sized chunks. This should make it go a little faster and use a little less memory.\n\nimport dask\nds_chunk = ds[\"air\"].sel(time=slice(\"1948\", \"1958\")).chunk({'time': 24, 'lat': -1, 'lon': -1})\nds_chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 16072, lat: 73, lon: 144)&gt; Size: 676MB\ndask.array&lt;rechunk-merge, shape=(16072, 73, 144), dtype=float32, chunksize=(24, 73, 144), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 129kB 1948-01-01 ... 1958-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nAttributes: (12/13)\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    ...            ...\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    valid_range:   [185.16 331.16]\n    dataset:       NCEP Reanalysis\n    level_desc:    0.995 sigma\n    _ChunkSizes:   [  1  73 144]xarray.DataArray'air'time: 16072lat: 73lon: 144dask.array&lt;chunksize=(24, 73, 144), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n644.49 MiB\n0.96 MiB\n\n\nShape\n(16072, 73, 144)\n(24, 73, 144)\n\n\nDask graph\n670 chunks in 157 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             144 73 16072\n\n\n\n\nCoordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 1958-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1958-12-31T06:00:00.000000000',\n       '1958-12-31T12:00:00.000000000', '1958-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Indexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1958-12-29 12:00:00', '1958-12-29 18:00:00',\n               '1958-12-30 00:00:00', '1958-12-30 06:00:00',\n               '1958-12-30 12:00:00', '1958-12-30 18:00:00',\n               '1958-12-31 00:00:00', '1958-12-31 06:00:00',\n               '1958-12-31 12:00:00', '1958-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=16072, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (13)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\n# This is takes about 4 minutes; 1.4 Gb\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    mean_all_years = ds_chunk.mean(dim=['lat', 'lon']).compute()\n\n[########################################] | 100% Completed | 218.60 s\n\n\n\nmean_all_years.plot();\n\n\n\n\n\n\n\n\n\n# resample to monthly\nmean_all_years.resample(time='ME').mean().plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#conclusion",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#conclusion",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWe practiced creating data cubes with urls to files on a THREDDS server via the OPeNDAP protocol.",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#references",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#references",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://www.jamstec.go.jp/ridinfo/xarray-and-opendap/\nMore opendap + xarray debugging https://github.com/stuckyb/gcdl/issues/24\nAnother opendap example https://github.com/ornldaac/daymet-python-opendap-xarray/blob/master/1_daymetv4_discovery_access_subsetting.ipynb\nNice example https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes Jupyter Hub and you will not need to install anything.\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "HackHours 2025",
    "section": "Schedule",
    "text": "Schedule\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Using large language models in Python to create data dashboards like this (Carl Boettiger, UC Berkeley) \nMar 21 - Parallel processing NODD model data with Coiled (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs"
  },
  {
    "objectID": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "href": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "title": "NMFS HackHours 2025",
    "section": "Why would I want to work in the cloud?",
    "text": "Why would I want to work in the cloud?\nWatch this video on “Enabling Analysis in the Cloud Using NASA Earth Science Data” by Michele Thorton, a NASA Openscapes mentor from the Oak Ridge National Laboratory Distributed Active Archive Center.\n\n\nMore earth data tutorials to explore!\nThe content and tutorials is a mix of content from workshops by the NASA Openscapes mentors (for example 2023 Cloud AGU Workshop), content developed by Carl Boettiger for NASA TOPS-T Cloud Native Geospatial in R & Python, content by NMFS CoastWatch, and other internal and external tutorials.\nHow do I get these tutorials into the JupyterHub? clone this https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/ and then look in the examples folder.\nHow do I clone? Since these are Jupyter/Python notebooks, easiest is cloning via JupyterLab.\n\nGo to JupyterLab. How? I closed the tab. Open the JupyterHub url again.\nClick on the little file icon on left until you are at the home directory.\nClick on the little Git icon on left. Which is it? Click on all the icons until you find it.\nWhen you see the ‘Clone repository’ button, click that. Paste in the url of the GitHub repo. I don’t see ‘Clone repository’. Go back to step 1. You are not in the home directory yet.\n\n\n\nFAQ\n\nCan I bring my own content/code to the event and JupyterHub? Absolutely, please do!! You can clone a GitHub repo or just upload files into the hub."
  },
  {
    "objectID": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "href": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "title": "HackHours in R and Python",
    "section": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET",
    "text": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET\nUse this form to sign-up to be alerted for future hackdays and Intro to JupyterHubs sessions: SIGN-UP FORM\nContact or questions: Eli Holmes (NOAA) - Type my name in your NOAA email, and my contact will pop up. Note, it uses “Eli” not “Elizabeth”.\nDuring these 1 hour hackhours, we will learn to do cloud computing with a JupyterHub set-up with geospatial packages and data. These sessions will get you more familiar with cloud-computing, JupyterHubs, Jupyter notebooks, and Python for geospatial analysis.\nAdd event to your calendar\nClick “HackHour 2024” for list of events and dates"
  },
  {
    "objectID": "content/reuse.html",
    "href": "content/reuse.html",
    "title": "Reuse Statement",
    "section": "",
    "text": "This content is released under CC0 Creative Commons.\nPermissive Re-Mix and No Attribution Needed: You may reuse the NMFS Open Science content in this repository—excluding the NMFS Open Science logo and any NOAA logos—in any way you like. You do not need permission. You do not need to give attribution, but if you use large parts of tutorials or content it is polite to give acknowledgement of the source. Please check each repository for its reuse statement. Some of the content is from outside of NMFS Open Science. This will be noted and you should check the original content for its reuse statement.",
    "crumbs": [
      "Reuse Statement"
    ]
  },
  {
    "objectID": "content/hackhours.html",
    "href": "content/hackhours.html",
    "title": "Hackhours",
    "section": "",
    "text": "These sessions are for NOAA staff to gain more familiarity with Jupyter Hubs and working with spatial data, esp big data hosted in the cloud in databases, via code and via geospatial packages in R and Python.\nFor more data science trainings and resource, see: * https://sites.google.com/noaa.gov/nmfs-hq-st-open-science/trainings * https://coastwatch.noaa.gov/cwn/training-courses.html * https://ioos.github.io/ioos_code_lab/content/intro.html",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/hackhours.html#about-nmfs-open-science",
    "href": "content/hackhours.html#about-nmfs-open-science",
    "title": "Hackhours",
    "section": "About NMFS Open Science",
    "text": "About NMFS Open Science\nWe provide technical and infrastructure support for any groups within NOAA Fisheries who would like computing support for their workshops or trainings; See our training page (NOAA internal). In September 2024, we launched a JupyterHub with a variety of specialized computing environments tailored to needs in fisheries and ocean modeling. We also support the development of a Docker stack tailored to R and Python workflows. We run regular workshops and trainings in reproducible science. See NMFS Openscapes and NMFS Open Science. If at NOAA see our internal site and the tabs for News and Training.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to providing a harassment-free learning experience for everyone. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Chats. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that harass based on sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private channel.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "content/jhub.html",
    "href": "content/jhub.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub is managed by Openscapes and developed in partnership with the International Interactive Computing Collaboration 2i2c. Launched in September 2024, the NMFS Openscapes JupyterHub joins the NASA Openscapes JupyterHub in providing a curated interactive computing platform to support training in earth and life science visualization, computing and analysis. The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries.",
    "crumbs": [
      "JupyterHub Skills",
      "About the Hub"
    ]
  },
  {
    "objectID": "content/setup.html",
    "href": "content/setup.html",
    "title": "Quick Start",
    "section": "",
    "text": "For those already familiar with JupyterLab and unix.\n\nGitHub Account\nA GitHub account is required to gain access to the JupyterHub and to clone the tutorials used in the Hackhours.\nHow do I get the tutorials into the JupyterHub?\n\nYou can upload files.\nEasiest is probably cloning a repo into the hub. See the JupyterHub Skills section if you do not know how to do this.\n\n\n\nAuthenticating to GitHub\nThis is a little different on the JupyterHub. See git authentication.\nFor content that uses the NASA Earthdata repository, you will need an Earthdata Login account. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create. Please jot down your username and password, as you need to enter it in the tutorials.\n\n\nWhen done, please stop the JupyterHub\nIf you are in JupyterLab in the browser:\n\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you still have the JupyterLab tab open in your browser:\n\nGo to the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you do not have the JupyterLab tab open in your browser because you closed that tab:\n\nGo to the url https://&lt;jupyterhub url&gt;/user/&lt;your username in the hub&gt;/lab/ That will open the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server",
    "crumbs": [
      "JupyterHub Skills",
      "Quick Start"
    ]
  },
  {
    "objectID": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "href": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "title": "NASA AGU 2023 Workshop Slides",
    "section": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "text": "Enabling Analysis in the Cloud Using NASA Earth Science Data"
  },
  {
    "objectID": "content/workshops.html",
    "href": "content/workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries. See the CoastWatch Training for all their events.\n\nOct-Dec 2024 Quarto Workshop\nOctober 25, 2024 Oceanographic Satellite and Animal Telemetry Data Training Course\nMay 17, 2024 Introduction to using earth data in the cloud for scientific workflows"
  },
  {
    "objectID": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "href": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "title": "Topic",
    "section": "Tutorials",
    "text": "Tutorials\nCarl will be talking about integrating LLMs into data dashboards specifically this dashboard (source code).\nUse Carl’s image\nWhen you start up the Jupyter Hub\n\nSelect Other for image\nIn the box, put rocker/binder:latest\n\nClone Carl’s demo repo\ncd ~\ngit clone https://github.com/boettiger-lab/geo-llm-r\nCarl’s tutorial\nHe went mainly through this example:\n\nhttps://github.com/boettiger-lab/geo-llm-r/blob/main/test.R\n\nchat &lt;- ellmer::chat_vllm(\n  base_url = \"https://llm.nrp-nautilus.io/\",\n  model = \"llama3\",\n  api_key = \"&lt;token&gt;\"\n)",
    "crumbs": [
      "Guest Speakers",
      "3/14 Carl Boettiger - Integrating LLMs into your data dashboards"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#overview",
    "href": "topics-2025/2025-opendap/2-dbofs.html#overview",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Overview",
    "text": "Overview\nThis example is very similar to the first tutorial using NCEP-NCAR Reanalysis 1, but the netcdfs are slightly different and you will get more practice. This tutorial uses an example where the server doesn’t require authentication (username and password).",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#deleware-bay-forecast",
    "href": "topics-2025/2025-opendap/2-dbofs.html#deleware-bay-forecast",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Deleware Bay Forecast",
    "text": "Deleware Bay Forecast\nWe will create a data cube for data from the Delaware Bay Operational Forecast System (DBOFS). The approach is the same. We go to the THREDDS server for NOS and navigate through until we find the OPeNDAP page for a file. Then we need to get the url format for each file. Here is an example for one day. Note they only keep recent data so this url will break after March 2025.\nhttps://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t18z.20250314.regulargrid.n001.nc\nFirst step is to create some file urls.\n\nimport xarray as xr\n\n\nbase = 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/\\\nNOAA/DBOFS/MODELS/2025/03/14/dbofs.t%2.2dz.20250314.regulargrid.n001.nc'\nfiles = [base % d for d in range(0,24,6)]\nfiles\n\n['https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t06z.20250314.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t12z.20250314.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t18z.20250314.regulargrid.n001.nc']\n\n\nThen we can open these as usual with xarray.\n\n%%time\nds = xr.open_mfdataset(files, parallel=True)\nds\n\nCPU times: user 293 ms, sys: 53.8 ms, total: 347 ms\nWall time: 6.62 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 392MB\nDimensions:      (time: 4, ny: 487, nx: 529, Depth: 22)\nCoordinates:\n  * Depth        (Depth) float64 176B 0.0 1.0 2.0 4.0 ... 80.0 90.0 100.0 125.0\n    Latitude     (ny, nx) float64 2MB dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;\n    Longitude    (ny, nx) float64 2MB dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;\n  * time         (time) datetime64[ns] 32B 2025-03-13T19:00:00 ... 2025-03-14...\nDimensions without coordinates: ny, nx\nData variables:\n    h            (time, ny, nx) float64 8MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    mask         (time, ny, nx) float64 8MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    zeta         (time, ny, nx) float32 4MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    zetatomllw   (time, ny, nx) float32 4MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    u_eastward   (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    v_northward  (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    temp         (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    salt         (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\nAttributes: (12/36)\n    file:                            dbofs.t00z.20250314.fields.nowcast_0002.nc\n    format:                          netCDF-4/HDF5 file\n    Conventions:                     CF-1.4, SGRID-0.3\n    type:                            ROMS/TOMS history file\n    title:                           dbofs nowcast RUN in operational mode\n    var_info:                        varinfo.yaml\n    ...                              ...\n    compiler_flags:                  -fp-model precise -ip -O3\n    tiling:                          008x016\n    history:                         ROMS/TOMS, Version 4.2, Friday - March 1...\n    ana_file:                        ROMS/Functionals/ana_btflux.h, ROMS/Func...\n    CPP_options:                     mode, ADD_FSOBC, ADD_M2OBC, ANA_BSFLUX, ...\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:time: 4ny: 487nx: 529Depth: 22Coordinates: (4)Depth(Depth)float640.0 1.0 2.0 ... 90.0 100.0 125.0long_name :Depths of Standard Layerstandard_name :depthunits :mpositive :downaxis :Zarray([  0.,   1.,   2.,   4.,   6.,   8.,  10.,  12.,  15.,  20.,  25.,  30.,\n        35.,  40.,  45.,  50.,  60.,  70.,  80.,  90., 100., 125.])Latitude(ny, nx)float64dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;long_name :Latitude in common gridstandard_name :latitudeunits :degrees_north\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.97 MiB\n1.97 MiB\n\n\nShape\n(487, 529)\n(487, 529)\n\n\nDask graph\n1 chunks in 15 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         529 487\n\n\n\n\nLongitude(ny, nx)float64dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;long_name :Longitude in common gridstandard_name :longitudeunits :degrees_east\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.97 MiB\n1.97 MiB\n\n\nShape\n(487, 529)\n(487, 529)\n\n\nDask graph\n1 chunks in 15 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         529 487\n\n\n\n\ntime(time)datetime64[ns]2025-03-13T19:00:00 ... 2025-03-...long_name :time since initializationfield :time, scalar, series_ChunkSizes :512array(['2025-03-13T19:00:00.000000000', '2025-03-14T01:00:00.000000000',\n       '2025-03-14T07:00:00.000000000', '2025-03-14T13:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (8)h(time, ny, nx)float64dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :bathymetrystandard_name :sea_floor_depth_below_mean_sea_levelunits :meterpositive :downgrid :gridlocation :facefield :bath, scaler\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n7.86 MiB\n1.97 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 13 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nmask(time, ny, nx)float64dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :maskstandard_name :sea_binary_maskflag_values :[0. 1.]flag_meanings :land, watergrid :gridlocation :face\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n7.86 MiB\n1.97 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 13 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nzeta(time, ny, nx)float32dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :free-surface referenced to mean sea levelstandard_name :sea_surface_elevationunits :meterstime :timegrid :gridlocation :facefield :free-surface, scalar, series_ChunkSizes :[  1 487 529]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.93 MiB\n0.98 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nzetatomllw(time, ny, nx)float32dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :free-surface referenced to mean lower low waterunits :meterstime :timegrid :gridlocation :facefield :free-surface, scalar, series_ChunkSizes :[  1 487 529]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.93 MiB\n0.98 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nu_eastward(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :eastward momentum componentstandard_name :eastward_sea_water_velocityunits :meters second-1time :timegrid :gridlocation :facefield :u_eastward, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nv_northward(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :northward momentum componentstandard_name :northward_sea_water_velocityunits :meters second-1time :timegrid :gridlocation :facefield :v_northward, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\ntemp(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :potential temperaturestandard_name :sea_water_temperatureunits :Celsiustime :timegrid :gridlocation :facefield :temperature, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nsalt(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :salinitystandard_name :sea_water_salinityunits :PSUtime :timegrid :gridlocation :facefield :salinity, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nIndexes: (2)DepthPandasIndexPandasIndex(Index([  0.0,   1.0,   2.0,   4.0,   6.0,   8.0,  10.0,  12.0,  15.0,  20.0,\n        25.0,  30.0,  35.0,  40.0,  45.0,  50.0,  60.0,  70.0,  80.0,  90.0,\n       100.0, 125.0],\n      dtype='float64', name='Depth'))timePandasIndexPandasIndex(DatetimeIndex(['2025-03-13 19:00:00', '2025-03-14 01:00:00',\n               '2025-03-14 07:00:00', '2025-03-14 13:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (36)file :dbofs.t00z.20250314.fields.nowcast_0002.ncformat :netCDF-4/HDF5 fileConventions :CF-1.4, SGRID-0.3type :ROMS/TOMS history filetitle :dbofs nowcast RUN in operational modevar_info :varinfo.yamlrst_file :dbofs.t00z.20250314.rst.nowcast.nchis_base :dbofs.t00z.20250314.fields.nowcaststa_file :dbofs.t00z.20250314.stations.nowcast.ncgrd_file :dbofs.romsgrid.ncini_file :dbofs.t00z.20250314.init.nowcast.ncriver_file :dbofs.t00z.20250314.river.nctide_file :dbofs.roms.tides.ncfrc_file_01 :dbofs.t00z.20250314.met.nowcast.ncbry_file_01 :dbofs.t00z.20250314.obc.ncscript_file :dbofs_ROMS_nowcast.inspos_file :dbofs.stations.inNLM_TADV :\nADVECTION:   HORIZONTAL   VERTICAL     \ntemp:        HSIMT        HSIMT        \nsalt:        HSIMT        HSIMTNLM_LBC :\nEDGE:  WEST   SOUTH  EAST   NORTH  \nzeta:  Cha    Cha    Cha    Clo    \nubar:  Fla    Fla    Fla    Clo    \nvbar:  Fla    Fla    Fla    Clo    \nu:     Rad    Rad    Rad    Clo    \nv:     Rad    Rad    Rad    Clo    \ntemp:  RadNud RadNud RadNud Clo    \nsalt:  RadNud RadNud RadNud Clo    \ntke:   Gra    Gra    Gra    Clogit_url :https://github.com/NOAA-CO-OPS/2024-NOS-Code-Package_v3.6.0git_rev :c1e184cd03c2208a7b6924f2ef264d1c9feeb838svn_url :https://www.myroms.org/svn/src/trunksvn_rev :1209code_dir :/lfs/h1/ops/para/packages/nosofs.v3.6.1/sorc/ROMS.fdheader_dir :/lfs/h1/ops/para/packages/nosofs.v3.6.1/includeheader_file :dbofs.hos :Linuxcpu :x86_64compiler_system :ftn-intelcompiler_command :/opt/cray/pe/craype/2.7.17/bin/ftncompiler_flags :-fp-model precise -ip -O3tiling :008x016history :ROMS/TOMS, Version 4.2, Friday - March 14, 2025 - 12:48:04 AMana_file :ROMS/Functionals/ana_btflux.h, ROMS/Functionals/ana_rain.h, ROMS/Functionals/ana_stflux.hCPP_options :mode, ADD_FSOBC, ADD_M2OBC, ANA_BSFLUX, ANA_BTFLUX, ANA_RAIN, ANA_SSFLUX, ASSUMED_SHAPE, ATM_PRESS, BOUNDARY_ALLREDUCE, BULK_FLUXES, COLLECT_ALLGATHER, CURVGRID, DIFF_GRID, DJ_GRADPS, DOUBLE_PRECISION, EMINUSP, HDF5, LIMIT_STFLX_COOLING, KANTHA_CLAYSON, LONGWAVE_OUT, MASKING, MIX_GEO_TS, MIX_S_UV, MPI, MY25_MIXING, NONLINEAR, NONLIN_EOS, NO_LBC_ATT, N2S2_HORAVG, PERFECT_RESTART, POWER_LAW, PROFILE, K_GSCHEME, RADIATION_2D, REDUCE_ALLREDUCE, !RST_SINGLE, SALINITY, STEP2D_LF_AM3, SOLAR_SOURCE, SOLVE3D, SSH_TIDES, STATIONS, TS_DIF2, UV_ADV, UV_COR, UV_U3HADVECTION, UV_C4VADVECTION, UV_DRAG_GRID, UV_QDRAG, UV_TIDES, UV_VIS2, VAR_RHO_2D, VISC_GRIDDODS_EXTRA.Unlimited_Dimension :time\n\n\n\n# each file is about 100Mb\nprint(f\"{ds.isel(time=1).nbytes / 1e6} Mb\")\n\n100.9884 Mb\n\n\nNext we can plot a map of the temperature at one time point.\n\nds.temp.isel(Depth=1, time=1).plot(x='Longitude', y='Latitude');\n\n\n\n\n\n\n\n\nAnd we can get the mean temperature for the 4 time points.\n\n%%time\nds_mean = ds[\"temp\"].isel(Depth=1).mean(dim=['ny', 'nx'])\nds_mean.plot();\n\nCPU times: user 58.2 ms, sys: 3.28 ms, total: 61.5 ms\nWall time: 1.19 s\n\n\n\n\n\n\n\n\n\n\nA plot of temperature by depth\nHere I will make a plot of temperature by depth in the middle of the bay.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Plot the full dataset\nfig, ax = plt.subplots(figsize=(8, 6))\nds2.temp.isel(Depth=1, time=1).plot(x=\"Longitude\", y=\"Latitude\", ax=ax)\n\n# Define the slice box coordinates\nlon_min, lon_max = -74.5, -74\nlat_min, lat_max = 38, 38.5\n\n# Create a rectangular patch (bounding box)\nbox = patches.Rectangle(\n    (lon_min, lat_min),  # Bottom-left corner (lon, lat)\n    lon_max - lon_min,   # Width (longitude range)\n    lat_max - lat_min,   # Height (latitude range)\n    linewidth=2, edgecolor='red', facecolor='none', linestyle=\"--\"\n)\n\n# Add the box to the plot\nax.add_patch(box)\n\n# Customize the plot\nax.set_title(\"Temperature with Slice Region Highlighted\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFirst, I am going to fix the indices to use lat/lon.\n\n# because I want to slice with actual lat lon\nlat_values = ds.isel(time=1, Depth=1, nx=1).Latitude.values\nlon_values = ds.isel(time=1, Depth=1, ny=1).Longitude.values\nds = ds.assign_coords({\"ny\": lat_values, \"nx\": lon_values})\nds = ds.rename({\"ny\": \"lat\", \"nx\": \"lon\"})\n\nCreate a mean by depth for each time period.\n\ndepth_slice = ds[\"temp\"].sel(lon=slice(-74.5,-74), lat=slice(38, 38.5))\n\n\ndepth_slice.mean(dim=['lat', 'lon']).plot.line(x=\"Depth\");",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#data-on-aws",
    "href": "topics-2025/2025-opendap/2-dbofs.html#data-on-aws",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Data on AWS",
    "text": "Data on AWS\nThe data are also available on AWS in a S3 bucket. Let’s compare data access to that. https://noaa-nos-ofs-pds.s3.amazonaws.com/index.html The data are here but you need to know how to make s3 urls.\nhttps://noaa-nos-ofs-pds.s3.amazonaws.com/dbofs/netcdf/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc\nbecomes this\ns3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc\nTo open netcdf on s3, we need to create a “fileset”; we cannot just us the list of urls like we can for the OPeNDAP links.\n\n# create the file urls to s3 bucket\nbase = 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t%2.2dz.20250314.regulargrid.n001.nc'\ns3_files = [base % d for d in range(0,24,6)]\ns3_files\n\n['s3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t06z.20250314.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t12z.20250314.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t18z.20250314.regulargrid.n001.nc']\n\n\n\n# each file is about 100Mb\nfs.size(s3_files[1])/1e6  # MB#\n\n101.415959\n\n\n\n# Run this code once to set up s3 access\nimport s3fs \nfs = s3fs.S3FileSystem(anon=True)\n\n# Create a fileset\nfileset = [fs.open(file) for file in s3_files]\n\nWe open up the fileset.\n\n%%time\nds2 = xr.open_mfdataset(fileset)\n\nCPU times: user 2.17 s, sys: 996 ms, total: 3.17 s\nWall time: 28.4 s\n\n\nWe know have a data cube that we can work with same as with our data cube from the OPeNDAP server. The data are only loaded when we need them (to plot or compute). Data access is considerably slower than for the OPeNDAP server. I don’t know why that is.\n\n%%time\nds2_mean = ds2[\"temp\"].isel(Depth=1).mean(dim=['ny', 'nx'])\nds2_mean.plot();\n\nCPU times: user 1.89 s, sys: 877 ms, total: 2.77 s\nWall time: 22.7 s",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#conclusion",
    "href": "topics-2025/2025-opendap/2-dbofs.html#conclusion",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Conclusion",
    "text": "Conclusion\nWe worked through another example of getting data off an OPeNDAP server and compared to getting the data off an S3 bucket.",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#references",
    "href": "topics-2025/2025-opendap/2-dbofs.html#references",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "References",
    "text": "References\n\nOpen files in S3 bucket https://nbviewer.org/gist/rsignell-usgs/111222351c4fee9e99827844351ab952\nhttps://www.jamstec.go.jp/ridinfo/xarray-and-opendap/\nMore opendap + xarray debugging https://github.com/stuckyb/gcdl/issues/24\nAnother opendap example https://github.com/ornldaac/daymet-python-opendap-xarray/blob/master/1_daymetv4_discovery_access_subsetting.ipynb\nNice example https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#overview",
    "href": "topics-2025/2025-opendap/4-nsidc.html#overview",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nThe National Snow and Ice Data Center servers require NASA Earthdata login authentication, but don’t require a EULA (as far as I know). However they have similar redirect issues as data that does require a EULA. The solution used for datasets that require a EULA seems to work for access.\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages and setup\n\nimport xarray as xr\nimport pydap.client\n\n\n# load username and password\nimport netrc\n# Get credentials from .netrc\nauth_host = \"urs.earthdata.nasa.gov\"\ntry:\n    credentials = netrc.netrc().authenticators(auth_host)\n    if credentials:\n        username, _, password = credentials\n    else:\n        raise ValueError(\"No credentials found in .netrc!\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(\"Could not find ~/.netrc. Ensure it exists and is configured correctly.\")",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#national-snow-and-ice-data-center",
    "href": "topics-2025/2025-opendap/4-nsidc.html#national-snow-and-ice-data-center",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "National Snow and Ice Data Center",
    "text": "National Snow and Ice Data Center\nTheir OPeNDAP server also uses NASA Earthdata login authentication. I struggled to find the right format for url that would open.\n\n#url = \"https://n5eil02u.ecs.nsidc.org/opendap/OTHR/NISE.004/2012.10.02/NISE_SSMISF17_20121002.HDFEOS\"\nurl = \"https://n5eil02u.ecs.nsidc.org/opendap/MOST/MOD10A1.006/2000.03.22/MOD10A1.A2000082.h00v08.006.2016061212345.hdf\"\n\n\nimport pydap\nfrom pydap.client import open_url\nfrom pydap.cas.urs import setup_session\nsession = setup_session(username, password, check_url=url)\n\n\npydap_ds = open_url(url, session=session, protocol=\"dap2\")\n\n\npydap_ds\n\n&lt;DatasetType with children 'NDSI_Snow_Cover', 'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA', 'NDSI', 'Snow_Albedo_Daily_Tile', 'orbit_pnt', 'granule_pnt', 'Latitude', 'Longitude', 'YDim', 'XDim', 'MOD_Grid_Snow_500m_eos_cf_projection'&gt;\n\n\n\n%%time\n# this works but odd errors\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'NDSI_Snow_Cover' has multiple fill values {np.int64(200), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'Snow_Albedo_Daily_Tile' has multiple fill values {np.int64(250), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n\n\nCPU times: user 604 ms, sys: 44.7 ms, total: 649 ms\nWall time: 1.18 s\n\n\n\n%%time\n]url=\"https://n5eil02u.ecs.nsidc.org/opendap/MOST/MOD10A1.006/2000.03.22/MOD10A1.A2000082.h00v08.006.2016061212345.hdf\"\nds = xr.open_dataset(url, engine=\"pydap\", session=session)\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'NDSI_Snow_Cover' has multiple fill values {np.int64(200), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'Snow_Albedo_Daily_Tile' has multiple fill values {np.int64(250), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n\n\nCPU times: user 55 ms, sys: 15.3 ms, total: 70.2 ms\nWall time: 1.09 s",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#conclusion",
    "href": "topics-2025/2025-opendap/4-nsidc.html#conclusion",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with NSIDC data on its OPeNDAP server is hard. Redirect issues though I can open files with xarray. But format is odd.",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#references",
    "href": "topics-2025/2025-opendap/4-nsidc.html#references",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://github.com/pydap/pydap/issues/188\nhttps://nsidc.org/data/user-resources/help-center/how-do-i-access-data-using-opendap#anchor-using-a-command-line-interface",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html",
    "href": "topics-2025/2025-opendap/index.html",
    "title": "OPeNDAP - Python",
    "section": "",
    "text": "In this session, you will get an introduction to accessing data on data servers that provide OPeNDAP for data access. What is OPeNDAP? “OPeNDAP is the commonly used name for a discipline-neutral data access protocol (DAP) that’s widely utilized both to access and to provide data over the internet. You will find lots of data servers that use OPeNDAP.\nNote, these tutorials are narrowly focused on an xarray workflow. The goal is to create a lazy (no data loaded) xarray data cubes via open_dataset and open_mfdataset functions.\nIf you use NASA data, they are migrating to serving their data to the cloud and you should look at the earthaccess examples for data access. If you use NOAA data, they use a lot of ERDDAP servers, which are built off OPeNDAP but have some extra features. Go to the ERDDAP examples. If your data are somewhere in a cloud-native format (Zarr, geotiff) use that. That is going to be a lot easier (and faster) to work with for the xarray workflows that I focus in the 2025 HackDays tutorials.",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#tutorials",
    "href": "topics-2025/2025-opendap/index.html#tutorials",
    "title": "OPeNDAP - Python",
    "section": "Tutorials",
    "text": "Tutorials\n\nSimple example 1 with no authentication needed. NCEP-NCAR\nSimple example 2 with no authentication needed. Delaware Bay Operational Forecast System\nNASA OPeNDAP servers, authentication needed. NASA OPeNDAP\nNSIDC serviers, authentication needed. Working on this. Lots of problems with redirects.",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#servers-that-use-nasa-earthdata-authentication",
    "href": "topics-2025/2025-opendap/index.html#servers-that-use-nasa-earthdata-authentication",
    "title": "OPeNDAP - Python",
    "section": "Servers that use NASA Earthdata authentication",
    "text": "Servers that use NASA Earthdata authentication\nThings with nasa.gov are obvious but a couple others without nasa.gov in the url also use it.\n\nlist of NASA opendap servers: https://www.earthdata.nasa.gov/engage/open-data-services-software/earthdata-developer-portal/opendap/servers\nNSIDC https://n5eil02u.ecs.nsidc.org/opendap/ docs\nUSGS LPDAAC https://opendap.cr.usgs.gov/opendap/hyrax/",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#references",
    "href": "topics-2025/2025-opendap/index.html#references",
    "title": "OPeNDAP - Python",
    "section": "References",
    "text": "References\n\nxarray support for opendap\npydap\nhttps://github.com/ornldaac/daymet-python-opendap-xarray",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/index.html",
    "href": "topics-2025/index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes JupyterHub and you will not need to install anything. About the HackHours\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)\nDownload"
  },
  {
    "objectID": "topics-2025/index.html#schedule-links-to-content-on-left",
    "href": "topics-2025/index.html#schedule-links-to-content-on-left",
    "title": "HackHours 2025",
    "section": "Schedule (links to content on left)",
    "text": "Schedule (links to content on left)\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Introduction to the Nautilus HyperCluster for running containerized Big Data Applications (Carl Boettiger, UC Berkeley) \nMar 21 - Parallel processing NODD model data with Coiled (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs"
  },
  {
    "objectID": "topics-skills/02-git-clinic.html",
    "href": "topics-skills/02-git-clinic.html",
    "title": "Git Clinic",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-clinic.html#step-3",
    "href": "topics-skills/02-git-clinic.html#step-3",
    "title": "Git Clinic",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#prerequisites",
    "href": "topics-skills/02-git-jupyter.html#prerequisites",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "href": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to authenticate. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "href": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Git extension in JupyterLab",
    "text": "Git extension in JupyterLab\nWhen the instructions say to use or open or click the Git GUI, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#the-key-skills",
    "href": "topics-skills/02-git-jupyter.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "href": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub account (https://www.github.com/yourusername).\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nFirst make sure you are at the home directory level. Look at the folder icon under the blue launcher button. It should show, folder icon only like in this image. If not, then click on the folder icon.\n\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nClick on the git icon and then click “Clone a Repository” \nPaste in the URL of your repo from Step 1\nClick Clone. You can stay with the defaults for the checkboxes.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nClick on the README.md file in the Test repo.\nMake some changes to the file.\nClick the Git icon (in left navbar), and stage the change(s) by checking the “+” next to the files listed.\nAdd a commit message in the box.\nClick the Commit button at bottom.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git icon, look for the little cloud at the top. It is rather small. Click that to push changes.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in JupyterLab)\nFrom Git icon, click on the little cloud with a down arrow.\n\n\n\nActivity 1\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\n\nIn the Test repo, create a file called to &lt;yourname&gt;.md.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#footnotes",
    "href": "topics-skills/02-git-jupyter.html#footnotes",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#prerequisites",
    "href": "topics-skills/02-git-terminal.html#prerequisites",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#create-a-github-account",
    "href": "topics-skills/02-git-terminal.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "href": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Git in the terminal",
    "text": "Git in the terminal\nYou will need to open a terminal in JupyterLab or RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#the-key-skills",
    "href": "topics-skills/02-git-terminal.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "href": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\nThis skill is done on GitHub.com.\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nThese skills are done in a terminal from JupyterLab or RStudio.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nOpen a terminal.\nMake sure you are at the home directory level. Type this: cd ~\nClone the repo with this command. Replace yourname with your username. git clone https://www.github.com/yourname/Test\n\n\n\nSkill 3: Make some changes and commit your changes\nDo step 1 in your editor, JupyterLab or RStudio.\n\nMake some changes to the README.md file in the Test repo.\nGo to the terminal and make sure you are in your Test repo. cd ~/Test\nSee what has changed. You should see that README.md has changed. git status\nStage the change to the README.md git add README.md\nCommit the change. `git commit -m “small change”\n\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom the terminal, type git push\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub.com and commit\nFrom the terminal, type git pull\n\n\n\nActivity 1\nDo steps 1 to 3 in your editor, JupyterLab or RStudio, and steps 4 and 5 in the terminal on the JupyterHub.\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\nDo steps 1-3 on GitHub and step 4 from the terminal on the JupyterHub.\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nPull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#footnotes",
    "href": "topics-skills/02-git-terminal.html#footnotes",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html",
    "href": "topics-skills/02-intro-to-lab.html",
    "title": "Intro to JupyterLab",
    "section": "",
    "text": "When you start the JupyterHub, you will be in JupyterLab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#terminalshell",
    "href": "topics-skills/02-intro-to-lab.html#terminalshell",
    "title": "Intro to JupyterLab",
    "section": "Terminal/Shell",
    "text": "Terminal/Shell\nLog into the JupyterHub. If you do not see something like this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\nIf you have no experience working in a terminal, check out this self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\nBasic shell commands:\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\nrm filename remove a file\nrm -r directoryname remove a directory\nrm -rf directoryname force remove a directory; careful no recovery\n\nClose the terminal by clicking on the X in the terminal tab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#file-navigation",
    "href": "topics-skills/02-intro-to-lab.html#file-navigation",
    "title": "Intro to JupyterLab",
    "section": "File Navigation",
    "text": "File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on file icon below the blue button with a +. Now you see files in your home directory.\nClick on the folder icon that looks like this. Click on the actual folder image. \nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\n\nDelete a file\n\nDelete a file by right-clicking on it and clicking “Delete”",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "href": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "title": "Intro to JupyterLab",
    "section": "Create a new Jupyter notebook",
    "text": "Create a new Jupyter notebook\nFrom Launcher, click on the “Python 3” button, this will open a new Jupyter notebook.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "href": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "title": "Intro to JupyterLab",
    "section": "Basic Jupyter notebook navigation",
    "text": "Basic Jupyter notebook navigation\nA Jupyter notebook is a series of cells than can be code (default), markdown or raw text.\n\nLook at the top cell, this is a code cell which I could see if I click on the cell and look at the top navbar. Next to “Download”, it says “Code”. I can click that dropdown and change the cell type to markdown or raw.\nTo the left of the “Save” icon in the top navbar is a “+”. This will add a new cell.\nWithin a cell, you will see some icons on the right. Roll over these icons to see what they do.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "href": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "title": "Intro to JupyterLab",
    "section": "Running code in a cell",
    "text": "Running code in a cell\nTo run code in a cell, click in the cell and then hit “Shift Return”. You can also click “Run” in the menu or click the little right arrow in the top navbar above the cells.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "href": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "title": "Intro to JupyterLab",
    "section": "Creating and rendering markdown",
    "text": "Creating and rendering markdown\nCreate an new cell (you can click the “+” in the top navbar) and then change to markdown by clicking the dropdown next to “Download” in the top navbar. Type in some markdown and the run the cell (see above on how to run cells).",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "href": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "title": "Intro to JupyterLab",
    "section": "Running all cells in a notebook",
    "text": "Running all cells in a notebook\nUse the “Run” menu.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#install-packages",
    "href": "topics-skills/02-intro-to-lab.html#install-packages",
    "title": "Intro to JupyterLab",
    "section": "Install packages",
    "text": "Install packages\nUse pip install in a cell. This will not persist between sessions.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#learn-more",
    "href": "topics-skills/02-intro-to-lab.html#learn-more",
    "title": "Intro to JupyterLab",
    "section": "Learn more",
    "text": "Learn more\nThere are lots of tutorials on JupyterLab out there. Do a search to find content that works for you.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html",
    "href": "topics-skills/03-ScratchBucket.html",
    "title": "Using the S3 Scratch Bucket",
    "section": "",
    "text": "The JupyterHub has a preconfigured S3 “Scratch Bucket” that automatically deletes files after 7 days. This is a great resource for experimenting with large datasets and working collaboratively on a shared dataset with other users.",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "href": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "title": "Using the S3 Scratch Bucket",
    "section": "Access the scratch bucket",
    "text": "Access the scratch bucket\nThe scratch bucket is hosted at s3://nmfs-openscapes-scratch. The JupyterHub automatically sets an environment variable SCRATCH_BUCKET that appends a suffix to the s3 url with your GitHub username. This is intended to keep track of file ownership, stay organized, and prevent users from overwriting data!\nEveryone has full access to the scratch bucket, so be careful not to overwrite data from other users when uploading files. Also, any data you put there will be deleted 7 days after it is uploaded\nIf you need more permanent S3 bucket storage refer to AWS_S3_bucket documentation (left) to configure your own S3 Bucket.\nWe’ll use the S3FS Python package, which provides a nice interface for interacting with S3 buckets.\n\nimport os\nimport s3fs\nimport fsspec\nimport boto3\nimport xarray as xr\nimport geopandas as gpd\n\n\n# My GitHub username is `eeholmes`\nscratch = os.environ['SCRATCH_BUCKET']\nscratch \n\n's3://nmfs-openscapes-scratch/eeholmes'\n\n\n\n# But you can set a different S3 object prefix to use:\nscratch = 's3://nmfs-openscapes-scratch/hackhours'",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#uploading-data",
    "href": "topics-skills/03-ScratchBucket.html#uploading-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Uploading data",
    "text": "Uploading data\nIt’s great to store data in S3 buckets because this storage features very high network throughput. If many users are simultaneously accessing the same file on a spinning networked harddrive (/home/jovyan/shared) performance can be quite slow. S3 has much higher performance for such cases.\n\nUpload single file\n\nlocal_file = '~/NOAAHackDays/topics-2025/2025-02-14-earthdata/littlecube.nc'\n\nremote_object = f\"{scratch}/littlecube.nc\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\nOnce a bucket has files, I can list them. If the bucket is empty, you will get errors instead of [].\n\ns3 = s3fs.S3FileSystem()\ns3.ls(scratch)\n\n['nmfs-openscapes-scratch/hackhours/littlecube.nc']\n\n\n\ns3.stat(remote_object)\n\n{'Key': 'nmfs-openscapes-scratch/hackhours/littlecube.nc',\n 'LastModified': datetime.datetime(2025, 2, 13, 21, 41, 5, tzinfo=tzlocal()),\n 'ETag': '\"d73616d9e3ad84cf58a4a676b1e3d454\"',\n 'ChecksumAlgorithm': ['CRC32'],\n 'ChecksumType': 'FULL_OBJECT',\n 'Size': 50224,\n 'StorageClass': 'STANDARD',\n 'type': 'file',\n 'size': 50224,\n 'name': 'nmfs-openscapes-scratch/hackhours/littlecube.nc'}\n\n\n\n\nUpload a directory\n\nlocal_dir = '~/NOAAHackDays/topics-2025/resources'\n\n!ls -lh {local_dir}\n\ntotal 5.9M\n-rw-r--r-- 1 jovyan jovyan 5.9M Feb 12 21:05 e_sst.nc\ndrwxr-xr-x 3 jovyan jovyan  281 Feb 12 21:18 longhurst_v4_2010\n\n\n\ns3.upload(local_dir, scratch, recursive=True)\n\n[None, None, None, None, None, None, None, None, None]\n\n\nThe directory name is the directory name (only) of the local directory.\n\ns3.ls(f'{scratch}/resources')\n\n['nmfs-openscapes-scratch/hackhours/resources/e_sst.nc',\n 'nmfs-openscapes-scratch/hackhours/resources/longhurst_v4_2010']",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#accessing-data",
    "href": "topics-skills/03-ScratchBucket.html#accessing-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Accessing Data",
    "text": "Accessing Data\nSome software packages allow you to stream data directly from S3 Buckets. But you can always pull objects from S3 and work with local file paths.\nThis download-first, then analyze workflow typically works well for older file formats like HDF and netCDF that were designed to perform well on local hard drives rather than Cloud storage systems like S3.\nFor best performance do not work with data in your home directory. Instead use a local scratch space like `/tmp`\n\nremote_object\n\n's3://nmfs-openscapes-scratch/hackhours/littlecube.nc'\n\n\n\nlocal_object = '/tmp/test.nc'\ns3.download(remote_object, local_object)\n\n[None]\n\n\n\nds = xr.open_dataset(local_object)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)\n\n\nIf you don't want to think about downloading files you can let `fsspec` handle this behind the scenes for you! This way you only need to think about remote paths\n\nfs = fsspec.filesystem(\"simplecache\", \n                       cache_storage='/tmp/files/',\n                       same_names=True,  \n                       target_protocol='s3',\n                       )\n\n\n# The `simplecache` setting above will download the full file to /tmp/files\nprint(remote_object)\nwith fs.open(remote_object) as f:\n    ds = xr.open_dataset(f.name) # NOTE: pass f.name for local cached path\n\ns3://nmfs-openscapes-scratch/hackhours/littlecube.nc\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "href": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "title": "Using the S3 Scratch Bucket",
    "section": "Cloud-optimized formats",
    "text": "Cloud-optimized formats\nOther formats like COG, ZARR, Parquet are ‘Cloud-optimized’ and allow for very efficient streaming directly from S3. In other words, you do not need to download entire files and instead can easily read subsets of the data.\nThe example below reads a Parquet file directly into memory (RAM) from S3 without using a local disk:\n\n# first upload the file\nlocal_file = '~/NOAAHackDays/topics-2025/resources/example.parquet'\n\nremote_object = f\"{scratch}/example.parquet\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\n\ngf = gpd.read_parquet(remote_object)\ngf.head(2)\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n889953.0\nOceania\nFiji\nFJI\n5496\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\n\n\n1\n58005463.0\nAfrica\nTanzania\nTZA\n63177\nPOLYGON ((33.90371 -0.95, 34.07262 -1.05982, 3...",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "href": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "title": "Using the S3 Scratch Bucket",
    "section": "Advanced: Access Scratch bucket outside of JupyterHub",
    "text": "Advanced: Access Scratch bucket outside of JupyterHub\nLet’s say you have a lot of files on your laptop you want to work with. The S3 Bucket is a convient way to upload large datasets for collaborative analysis. To do this, you need to copy AWS Credentials from the JupyterHub to use on other machines. More extensive documentation on this workflow can be found in this repository https://github.com/scottyhq/jupyter-cloud-scoped-creds.\nThe following code must be run on the JupyterHub to get temporary credentials:\n\nclient = boto3.client('sts')\n\nwith open(os.environ['AWS_WEB_IDENTITY_TOKEN_FILE']) as f:\n    TOKEN = f.read()\n\nresponse = client.assume_role_with_web_identity(\n    RoleArn=os.environ['AWS_ROLE_ARN'],\n    RoleSessionName=os.environ['JUPYTERHUB_CLIENT_ID'],\n    WebIdentityToken=TOKEN,\n    DurationSeconds=3600\n)\n\nreponse will be a python dictionary that looks like this:\n{'Credentials': {'AccessKeyId': 'ASIAYLNAJMXY2KXXXXX',\n  'SecretAccessKey': 'J06p5IOHcxq1Rgv8XE4BYCYl8TG1XXXXXXX',\n  'SessionToken': 'IQoJb3JpZ2luX2VjEDsaCXVzLXdlc////0dsD4zHfjdGi/0+s3XKOUKkLrhdXgZ8nrch2KtzKyYyb...',\n  'Expiration': datetime.datetime(2023, 7, 21, 19, 51, 56, tzinfo=tzlocal())},\n  ...\nYou can copy and paste the values to another computer, and use them to configure your access to S3:\n\ns3 = s3fs.S3FileSystem(key=response['Credentials']['AccessKeyId'],\n                       secret=response['Credentials']['SecretAccessKey'],\n                       token=response['Credentials']['SessionToken'] )\n\n\n# Confirm your credentials give you access\ns3.ls('nmfs-openscapes-scratch', refresh=True)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html",
    "href": "topics-skills/04-learning.html",
    "title": "Learning Resources",
    "section": "",
    "text": "You are welcome to use the JupyterHub outside of the HackHours and workshops in order to facilitate your data science and cloud-computing learning. Here are some ideas:",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#python",
    "href": "topics-skills/04-learning.html#python",
    "title": "Learning Resources",
    "section": "Python",
    "text": "Python\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\n\nI did Python for Data Science and Machine Learning Bootcamp in Udemy\n\nThe same have deep-learning and ML courses\nHarvard https://pll.harvard.edu/catalog and MIT https://ocw.mit.edu/search/?q=python have lots of free material\nGeosciences\n\nhttps://cookbooks.projectpythia.org/\nhttps://nasa-openscapes.github.io/earthdata-cloud-cookbook/\nhttps://ioos.github.io/ioos_code_lab/content/intro.html\nhttps://github.com/coastwatch-training/CoastWatch-Tutorials\nhttps://github.com/NASAARSET\nhttps://earth-env-data-science.github.io/intro.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#r",
    "href": "topics-skills/04-learning.html#r",
    "title": "Learning Resources",
    "section": "R",
    "text": "R\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\nGeosciences\n\nhttps://github.com/USGS-R\nhttps://pmarchand1.github.io/atelier_rgeo/rgeo_workshop.html\nhttps://datacarpentry.github.io/r-raster-vector-geospatial/\nhttps://r.geocompx.org/\nhttps://bookdown.org/mcwimberly/gdswr-book/\nhttps://rspatial.org/index.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/index.html",
    "href": "topics-skills/index.html",
    "title": "JupyterHub",
    "section": "",
    "text": "Explore the topics in the left navigation bar to learn how to use JupyterLab, RStudio and Git in the JupyterHub plus the other resources available.",
    "crumbs": [
      "JupyterHub Skills"
    ]
  },
  {
    "objectID": "topics-2025/2025-planetarycomputer-r/index.html",
    "href": "topics-2025/2025-planetarycomputer-r/index.html",
    "title": "Microsoft Planetary Computer in R",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac-r/"
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html",
    "href": "topics-skills/01-intro-to-jupyterhub.html",
    "title": "Intro to JupyterHubs",
    "section": "",
    "text": "In this tutorial, you will get an overview of our JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://nmfs-openscapes.2i2c.cloud/. Click “Login to continue”. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\n\n\nNMFS Openscapes JupyterHub Login\n\n\n\nImage type: Python or R\nNext you select your image type from the drop-down. The default is a geospatial image with Python and R.\n\n\nVirtual Machine size\nYou’ll see a dropdown that allows you to choose the size of virtual machine. For the tutorials, you will only need the smallest virtual machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our virtual machine. This may take several minutes.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "href": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "title": "Intro to JupyterHubs",
    "section": "The Launcher",
    "text": "The Launcher\nWhen you are in the JupyterLab tab (note the Jupyter Logo), you will see a Launcher page. If you don’t see this, go to File &gt; New Launcher or click the blue button on the top left. From the Launcher, you will see a buttons to open a new Jupyter Notebook, RStudio, Desktop and VSCode on the top row and buttons to open a Terminal and other file types below.\n Clicking on the “Python 3”, Terminal, Text File and Markdown File buttons will open a new tab in JupyterLab. You can also use the File dropdown menu for these.\nTo get an overview of JupyterLab, go here: Intro to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "href": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "title": "Intro to JupyterHubs",
    "section": "RStudio",
    "text": "RStudio\nIf you click the RStudio button in Launcher, RStudio will open in a new browser tab.\n\n\n\nRStudio\n\n\nTo get an overview of RStudio, go here: Intro to RStudio",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "href": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day you should log out of the JupyterHub, although it will log you out automatically after 90 minutes. You will not lose work; your home directory is persistent. When you keep a session active it uses up AWS resources (costs money) because it keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the JupyterLab tab not the RStudio tab.\n\n\nFrom the JupyterLab browser tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click “Log Out”!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan’t find the JupyterLab tab? Go to https://nmfs-openscapes.2i2c.cloud/hub/home\n\n\nLogging out or stopping your server will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the JupyterLab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn’t lose your work unless you were uploading a file.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files that we have prepared for you.\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don’t work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "href": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "title": "Intro to JupyterHubs",
    "section": "Getting help",
    "text": "Getting help\n\nDiscussions https://github.com/nmfs-opensci/NOAAHackDays/discussions\nAdd an issue if you see something amiss or you would like a package added https://github.com/nmfs-opensci/NOAAHackDays/issues\nVisit the NMFS Open Science Google Space (NOAA only). Search Google Spaces to find.\nNMFS staff: The NMFS Python User Group and NMFS R User Group are great places to get help from fellow users for Python and R questions.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "href": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘Jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your files.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute, CryoCloud and Openscapes.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  }
]