[
  {
    "objectID": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "href": "topics-skills/02-rstudio.html#open-rstudio-in-the-jupyterhub",
    "title": "RStudio",
    "section": "Open RStudio in the JupyterHub",
    "text": "Open RStudio in the JupyterHub\n\nLogin the JupyterHub\nClick on the RStudio button when the Launcher appears \nLook for the browser tab with the RStudio icon",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#basic-navigation",
    "href": "topics-skills/02-rstudio.html#basic-navigation",
    "title": "RStudio",
    "section": "Basic Navigation",
    "text": "Basic Navigation\n\n\n\nRStudio Panels",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "href": "topics-skills/02-rstudio.html#create-an-rstudio-project",
    "title": "RStudio",
    "section": "Create an RStudio project",
    "text": "Create an RStudio project\n\nOpen RStudio\nIn the file panel, click on the Home icon to make sure you are in your home directory\nFrom the file panel, click “New Project” to create a new project\nIn the pop up, select New Directory and then New Project\nName it sandbox\nClick on the dropdown in the upper right corner to select your sandbox project\nClick on Tools &gt; Project Options &gt; General and change the first 2 options about saving and restoring the workspace to “No”",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#installing-packages",
    "href": "topics-skills/02-rstudio.html#installing-packages",
    "title": "RStudio",
    "section": "Installing packages",
    "text": "Installing packages\nIn the bottom right panel, select the Packages tab, click install and then start typing the name of the package. Then click Install.\nThe JupyterHub comes with many packages already installed so you shouldn’t have to install many packages.\nWhen you want to use a package, you first need to load it with\nlibrary(hello)\nYou will see this in the tutorials. You might also see something like\nhello::thefunction()\nThis is using thefunction() from the hello package.\n\n\n\n\n\n\nNote\n\n\n\nPython users. In R, you will always call a function like funtion(object) and never like object.function(). The exception is something called ‘piping’ in R, which I have never seen in Python. In this case you pass objects left to right. Like object %&gt;% function(). Piping is very common in modern R but you won’t see it much in R from 10 years ago.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "href": "topics-skills/02-rstudio.html#uploading-and-downloading-files",
    "title": "RStudio",
    "section": "Uploading and downloading files",
    "text": "Uploading and downloading files\nNote, Upload and download is only for the JupyterHub not on RStudio on your computer.\n\nUploading is easy.\nLook for the Upload button in the Files tab of the bottom right panel.\n\n\nDownload is less intuitive.\n\nClick the checkbox next to the file you want to download. One only.\nClick the “cog” icon in the Files tab of the bottom right panel. Then click Export.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#creating-files",
    "href": "topics-skills/02-rstudio.html#creating-files",
    "title": "RStudio",
    "section": "Creating files",
    "text": "Creating files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/rstudio. Everyone has the same home directory but your files are separate and cannot be seen by others.\nPython users: If you open a Python image instead of the R image, your home is home/jovyan.\nThere are a number of different ways to create new files. Let’s practice making new files in RStudio.\n\nR Script\n\nOpen RStudio\nIn the upper right, make sure you are in your sandbox project.\nFrom the file panel, click on “New Blank File” and create a new R script.\nPaste\n\nprint(\"Hello World\")\n1+1\nin the script. 7. Click the Source button (upper left of your new script file) to run this code. 8. Try putting your cursor on one line and running that line of code by clicking “Run” 9. Try selecting lines of code and running that by clicking “Run”\n\n\ncsv file\n\nFrom the file panel, click on “New Blank File” and create a Text File.\nThe file will open in the top left corner. Paste in the following:\n\nname, place, value\nA, 1, 2\nB, 10, 20\nC, 100, 200\n\nClick the save icon (above your new file) to save your csv file\n\n\n\nA Rmarkdown document\nNow let’s create some more complicated files using the RStudio template feature.\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\nReference sheet for writing in RMarkdown or go to Help &gt; Markdown Quick Reference\n\n\nA Rmarkdown presentation\n\nFrom the upper left, click File -&gt; New File -&gt; RMarkdown\nClick “Presentation” on left of the popup and click “Ok” at the bottom.\nWhen the file opens, click Knit (icon at top of file).\nIt will ask for a name. Give it one and save.\nYou file will render into html.\n\n\n\n(advanced) An interactive application\n\nFrom the upper left, click File -&gt; New File -&gt; Shiny Web App\nIn the popup, give the app a name and make sure the app is saved to my-files\nWhen the file opens, Run App (icon at top of file).\n\n\n\nAnd many more\nPlay around with creating other types of documents using templates. Especially if you already use RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-rstudio.html#more-tips",
    "href": "topics-skills/02-rstudio.html#more-tips",
    "title": "RStudio",
    "section": "More tips",
    "text": "More tips\nLearn some tips and tricks from these\n\nhttps://colorado.posit.co/rsc/the-unknown/into-the-unknown.html\nhttps://www.dataquest.io/blog/rstudio-tips-tricks-shortcuts/",
    "crumbs": [
      "JupyterHub Skills",
      "RStudio"
    ]
  },
  {
    "objectID": "topics-2025/2025-virtualizarr/index.html",
    "href": "topics-2025/2025-virtualizarr/index.html",
    "title": "VirtualiZarr",
    "section": "",
    "text": "https://virtualizarr.readthedocs.io/en/stable/index.html\nhttps://agu.confex.com/agu/agu24/meetingapp.cgi/Paper/1677256"
  },
  {
    "objectID": "topics-2025/2025-icechunk/index.html",
    "href": "topics-2025/2025-icechunk/index.html",
    "title": "Icechunk",
    "section": "",
    "text": "https://earthmover.io/blog/icechunk/ https://icechunk.io/en/latest/ https://icechunk.io/en/latest/icechunk-python/quickstart/\n1 hr talk https://youtu.be/i-73e3_irpY?feature=shared Pangeo showcase: https://youtu.be/l_y8YOn8hm8?feature=shared"
  },
  {
    "objectID": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "href": "topics-skills/04-other-images.html#other-images-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Other images on the hub",
    "text": "Other images on the hub\nUse the dropdown to select a non-default image on the hub. There are a variety. You can learn about them on the NMFS Open Science container images repo.",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "href": "topics-skills/04-other-images.html#using-other-images-not-on-the-hub",
    "title": "Using other images on the JupyterHub",
    "section": "Using other images not on the hub",
    "text": "Using other images not on the hub\nThe JupyterHub can run other images that are compatible with a JupyterHub, e.g. Binder images. When you start the hub, use the image dropdown to select “Other”:\n\nYou can add a url to a Docker image to this. For example, if you wanted to use the Pangeo notebook docker images image, you would paste one of these into the “Custom image” box.\nFrom DockerHub: pangeo/pangeo-notebook From Quay.io quay.io/pangeo/pangeo-notebook\nOther common data science images:\n\nJupyter Docker Stacks\nNASA Openscapes python\nRocker Binder image\nPangeo\ngeocompx\nGPU accelerated data science docker images",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/04-other-images.html#using-a-github-repo",
    "href": "topics-skills/04-other-images.html#using-a-github-repo",
    "title": "Using other images on the JupyterHub",
    "section": "Using a GitHub repo",
    "text": "Using a GitHub repo\nYou can also create an environment with a MyBinder.org compatible GitHub repo. By selecting the “Build your own image” option.\n\n\nSimple example for Python\nThere are two ways to do this. Either via a conda environment or a pip install.\nconda example\n\nPut an environment.yml file in your GitHub repo at the base level with your Python packages that you need.\nCopy the url to your repo and paste that into the “Repository” box (above).\n\nenvironment.yml\nname: example-environment\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.11\n  - numpy\n  - psutil\n  - toolz\n  - matplotlib\n  - dill\n  - pandas\n  - partd\n  - bokeh\n  - dask\npip install example\nYou will need requirements.txt for packages and runtime.txt for the Python version.\nrequirements.txt\nnumpy\nmatplotlib==3.*\nseaborn==0.13.*\npandas\nruntime.txt\npython-3.10\n\n\nSimple example for R\nr example\nYou will need install.R for packages and runtime.txt for the R version.\ninstall.R\ninstall.packages(\"tidyverse\")\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"httr\")\ninstall.packages(\"shinydashboard\")\ninstall.packages(\"leaflet\")\nruntime.txt\nr-4.3.2-2024-01-10",
    "crumbs": [
      "JupyterHub Skills",
      "Other images"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html",
    "href": "topics-skills/03-earthdata.html",
    "title": "Earthdata Login",
    "section": "",
    "text": "NASA data are stored at one of several Distributed Active Archive Centers (DAACs). If you’re interested in available data for a given area and time of interest, the Earthdata Search portal provides a convenient web interface.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#why-do-i-need-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Why do I need an Earthdata login?",
    "text": "Why do I need an Earthdata login?\nTo programmatically access NASA data from within your Python or R scripts, you will need to enter your Earthdata username and password.",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "href": "topics-skills/03-earthdata.html#getting-an-earthdata-login",
    "title": "Earthdata Login",
    "section": "Getting an Earthdata login",
    "text": "Getting an Earthdata login\nIf you do not already have an Earthdata login, then navigate to the Earthdata Login page, a username and password, and then record this somewhere for use during the tutorials:",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "href": "topics-skills/03-earthdata.html#configure-programmatic-access-to-nasa-servers",
    "title": "Earthdata Login",
    "section": "Configure programmatic access to NASA servers",
    "text": "Configure programmatic access to NASA servers\nRun the following commands on the JupyterHub:\n\n\n\n\n\n\nImportant\n\n\n\nIn the below command, replace EARTHDATA_LOGIN with your personal username and EARTHDATA_PASSWORD with your password\n\n\necho 'machine urs.earthdata.nasa.gov login \"EARTHDATA_LOGIN\" password \"EARTHDATA_PASSWORD\"' &gt; ~/.netrc\nchmod 0600 ~/.netrc",
    "crumbs": [
      "JupyterHub Skills",
      "Earthdata login"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html",
    "href": "topics-skills/03-AWS_S3_bucket.html",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "",
    "text": "This set of instructions will walk through how to setup an AWS S3 bucket for a specific project and how to configure that bucket to allow all members of the project team to have access.\nThis notebook is from the CryoCloud documentation. THE CODE WILL NOT WORK SINCE YOU NEED TO AUTHENTICATE TO THE S3 BUCKET.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-an-aws-account-and-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create an AWS account and S3 bucket",
    "text": "Create an AWS account and S3 bucket\nThe first step is to create an AWS account that will be billed to your particular project. This can be done using these instructions.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-aws-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create AWS S3 bucket",
    "text": "Create AWS S3 bucket\nWithin your new AWS account, create an new S3 bucket:\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nChoose Create bucket\nName the bucket and select us-west-2 for the region\nLeave all other default options\nClick Create Bucket",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-a-user",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create a user",
    "text": "Create a user\nWithin the same AWS account, create a new IAM user:\n\nOn the AWS Console Home page, select the IAM service\nIn the navigation pane, select Users and then select Add users\nName the user and click Next\nAttach policies directly\nDo not select any policies\nClick Next\nCreate user\n\nOnce the user has been created, find the user’s ARN and copy it.\nNow, create access keys for this user:\n\nSelect Users and click the user that you created\nOpen the Security Credentials tab\nCreate access key\nSelect Command Line Interface (CLI)\nCheck the box to agree to the recommendation and click Next\nLeave the tag blank and click Create access key\nIMPORTANT: Copy the access key and the secret access key. This will be used later.",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "href": "topics-skills/03-AWS_S3_bucket.html#create-the-bucket-policy",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Create the bucket policy",
    "text": "Create the bucket policy\nConfigure a policy for this S3 bucket that will allow the newly created user to access it.\n\nOpen the AWS S3 console (https://console.aws.amazon.com/s3/)\nFrom the navigation pane, choose Buckets\nSelect the new S3 bucket that you created\nOpen the Permissions tab\nAdd the following bucket policy, replacing USER_ARN with the ARN that you copied above and BUCKET_ARN with the bucket ARN, found on the Edit bucket policy page on the AWS console:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ListBucket\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:ListBucket\",\n            \"Resource\": \"BUCKET_ARN\"\n        },\n        {\n            \"Sid\": \"AllObjectActions\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"USER_ARN\"\n            },\n            \"Action\": \"s3:*Object\",\n            \"Resource\": \"BUCKET_ARN/*\"\n        }\n    ]\n}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#reading-from-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Reading from the S3 bucket",
    "text": "Reading from the S3 bucket\n\nExample: ls bucket using s3fs\n\nimport s3fs\ns3 = s3fs.S3FileSystem(anon=False, profile='icesat2')\n\n\n\nExample: open HDF5 file using xarray\n\nimport s3fs\nimport xarray as xr\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nprint(ssh_ds)\n\n&lt;xarray.Dataset&gt;\nDimensions:      (Longitude: 2160, nv: 2, Latitude: 960, Time: 1)\nCoordinates:\n  * Longitude    (Longitude) float32 0.08333 0.25 0.4167 ... 359.6 359.8 359.9\n  * Latitude     (Latitude) float32 -79.92 -79.75 -79.58 ... 79.58 79.75 79.92\n  * Time         (Time) datetime64[ns] 1992-10-10T12:00:00\nDimensions without coordinates: nv\nData variables:\n    Lon_bounds   (Longitude, nv) float32 ...\n    Lat_bounds   (Latitude, nv) float32 ...\n    Time_bounds  (Time, nv) datetime64[ns] ...\n    SLA          (Time, Latitude, Longitude) float32 ...\n    SLA_ERR      (Time, Latitude, Longitude) float32 ...\nAttributes: (12/21)\n    Conventions:            CF-1.6\n    ncei_template_version:  NCEI_NetCDF_Grid_Template_v2.0\n    Institution:            Jet Propulsion Laboratory\n    geospatial_lat_min:     -79.916664\n    geospatial_lat_max:     79.916664\n    geospatial_lon_min:     0.083333336\n    ...                     ...\n    version_number:         2205\n    Data_Pnts_Each_Sat:     {\"16\": 661578, \"1001\": 636257}\n    source_version:         commit dc95db885c920084614a41849ce5a7d417198ef3\n    SLA_Global_MEAN:        -0.0015108844021796562\n    SLA_Global_STD:         0.09098986023297456\n    latency:                final\n\n\n\nimport s3fs\n\nimport xarray as xr\n\nimport hvplot.xarray\nimport holoviews as hv\n\nfs_s3 = s3fs.core.S3FileSystem(profile='icesat2')\n\ns3_url = 's3://gris-outlet-glacier-seasonality-icesat2/ssh_grids_v2205_1992101012.nc'\ns3_file_obj = fs_s3.open(s3_url, mode='rb')\nssh_ds = xr.open_dataset(s3_file_obj, engine='h5netcdf')\nssh_da = ssh_ds.SLA\n\nssh_da.hvplot.image(x='Longitude', y='Latitude', cmap='Spectral_r', geo=True, tiles='ESRI', global_extent=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nExample: read a geotiff using rasterio\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsession = rasterio.env.Env(profile_name='icesat2')\n\nurl = 's3://gris-outlet-glacier-seasonality-icesat2/out.tif'\n\nwith session:\n    with rasterio.open(url) as ds:\n        print(ds.profile)\n        band1 = ds.read(1)\n        \nband1[band1==-9999] = np.nan\nplt.imshow(band1)\nplt.colorbar()\n\n{'driver': 'GTiff', 'dtype': 'float32', 'nodata': -9999.0, 'width': 556, 'height': 2316, 'count': 1, 'crs': CRS.from_epsg(3413), 'transform': Affine(50.0, 0.0, -204376.0,\n       0.0, -50.0, -2065986.0), 'blockysize': 3, 'tiled': False, 'interleave': 'band'}",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "href": "topics-skills/03-AWS_S3_bucket.html#writing-to-the-s3-bucket",
    "title": "Instructions for setting up an AWS S3 bucket for your project",
    "section": "Writing to the S3 bucket",
    "text": "Writing to the S3 bucket\n\ns3 = s3fs.core.S3FileSystem(profile='icesat2')\n\nwith s3.open('gris-outlet-glacier-seasonality-icesat2/new-file', 'wb') as f:\n    f.write(2*2**20 * b'a')\n    f.write(2*2**20 * b'a') # data is flushed and file closed\n\ns3.du('gris-outlet-glacier-seasonality-icesat2/new-file')\n\n4194304",
    "crumbs": [
      "JupyterHub Skills",
      "AWS S3 Bucket"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#what-is-git-and-github",
    "href": "topics-skills/02-git.html#what-is-git-and-github",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "What is Git and GitHub?",
    "text": "What is Git and GitHub?\nGit A program to track your file changes and create a history of those changes. Creates a ‘container’ for a set of files called a repository.\nGitHub A website to host these repositories and allow you to sync local copies (on your computer) to the website. Lots of functionality built on top of this.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#some-basic-git-jargon",
    "href": "topics-skills/02-git.html#some-basic-git-jargon",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Some basic Git jargon",
    "text": "Some basic Git jargon\n\nRepo Repository. It is your code and the record of your changes. This record and also the status of your repo is a hidden folder called .git . You have a local repo and a remote repo. The remote repo is on GitHub (for in our case) is called origin. The local repo is on the JupyterHub.\nStage Tell Git which changes you want to commit (write to the repo history).\nCommit Write a note about what change the staged files and “commit” that note to the repository record. You are also tagging this state of the repo and you could go back to this state if you wanted.\nPush Push local changes (commits) up to the remote repository on GitHub (origin).\nPull Pull changes on GitHub into the local repository on the JupyterHub.\nGit GUIs A graphical interface for Git (which is command line). Today I will use jupyterlab-git which we have installed on JupyterHub.\nShell A terminal window where we can issue git commands.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#overview",
    "href": "topics-skills/02-git.html#overview",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "Overview",
    "text": "Overview\nToday I will cover the four basic Git/GitHub skills. The goal for today is to first get you comfortable with the basic skills and terminology. We will use what is called a “trunk-based workflow”.\n\nSimple Trunk-based Workflow:\n\nMake local (on your computer) changes to code.\nRecord what those changes were about and commit to the code change record (history).\nPush those changes to your remote repository (aka origin)\n\nWe’ll do this",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git.html#the-key-skills",
    "href": "topics-skills/02-git.html#the-key-skills",
    "title": "Intro to Version Control, Git and GitHub",
    "section": "The Key Skills",
    "text": "The Key Skills\nThese basic skills are all you need to learn to get started:\n\nSkill 1: Create a blank repo on GitHub (the remote or origin)\nSkill 2: Clone your GitHub repo to your local computer (in our case the JupyterHub)\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub (the remote or origin)\nSkill 1b: Create a new repo from some else’s GitHub repository\n\nIn the next tutorials, you will practice these in RStudio or JuptyerHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to Git"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#prerequisites",
    "href": "topics-skills/02-git-rstudio.html#prerequisites",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "href": "topics-skills/02-git-rstudio.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-rstudio.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "href": "topics-skills/02-git-rstudio.html#git-tab-in-rstudio",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Git tab in RStudio",
    "text": "Git tab in RStudio\nWhen the instructions say to use or open or click the Git tab, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#the-key-skills",
    "href": "topics-skills/02-git-rstudio.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo to RStudio\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "href": "topics-skills/02-git-rstudio.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\nShow me\n\n\nSkill 2: Clone your repo to the RStudio\nIn RStudio we do this by making a new project.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nFile &gt; New Project &gt; Version Control &gt; Git\nPaste in the URL of your repo from Step 1\nCheck that it is being created in your Home directory which will be denoted ~ in the JupyterHub.\nClick Create.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nMake some changes to the README.md file in the Test repo.\nClick the Git tab, and stage the change(s) by checking the checkboxes next to the files listed.\nClick the Commit button.\nAdd a commit comment, click commit.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git tab, click on the Green up arrow that says Push.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in RStudio)\nFrom Git tab, click on the down arrow that says Pull.\n\nShow me\n\n\nActivity 1\nIn RStudio,\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me in RStudio\n\n\nActivity 2\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nGo to RStudio and pull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to RStudio and create a new project",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-rstudio.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-rstudio.html#footnotes",
    "href": "topics-skills/02-git-rstudio.html#footnotes",
    "title": "Basic Git/GitHub Skills in RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in RStudio"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html",
    "href": "topics-skills/02-git-jupyter-old.html",
    "title": "Git in JupyterLab",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-jupyter-old.html#step-3",
    "href": "topics-skills/02-git-jupyter-old.html#step-3",
    "title": "Git in JupyterLab",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "href": "topics-skills/02-git-authentication.html#tell-git-who-you-are",
    "title": "GitHub Authentication",
    "section": "Tell Git who you are",
    "text": "Tell Git who you are\nFirst open a terminal and run these lines. Replace &lt;your email&gt; with your email and remove the angle brackets.\ngit config --global user.email \"&lt;your email&gt;\"\ngit config --global user.name \"&lt;your name&gt;\"\ngit config --global pull.rebase false",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#authentication",
    "href": "topics-skills/02-git-authentication.html#authentication",
    "title": "GitHub Authentication",
    "section": "Authentication",
    "text": "Authentication\nYou need to authenticate to GitHub so you can push your local changes up to GitHub. There are a few ways to do this. For the JupyterHub, we will mainly use gh-scroped-creds which is a secure app that temporarily stores your GitHub credentials on a JupyterHub. But we will also show you a way to store your credentials in a file that works on any computer, including a virtual computer like the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "href": "topics-skills/02-git-authentication.html#preferred-gh-scoped-creds",
    "title": "GitHub Authentication",
    "section": "Preferred: gh-scoped-creds",
    "text": "Preferred: gh-scoped-creds\nIf you get the error that it cannot find gh-scoped-creds, then type\npip install gh-scoped-creds\nin a termnal.\n\nOpen a terminal\nType gh-scoped-creds\nFollow the instructions\nFIRST TIME: Make sure to follow the second pop-up instructions and tell it what repos it is allowed to interact with. You have to go through a number of pop up windows.\n\nJump down to the “Test” section to test.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "href": "topics-skills/02-git-authentication.html#also-works-set-up-authentication-with-a-personal-token",
    "title": "GitHub Authentication",
    "section": "Also works: Set up authentication with a Personal Token",
    "text": "Also works: Set up authentication with a Personal Token\nThis will store your credentials in a file on the hub. This is not as secure since the file is unencrypted but sometimes gh-scoped-creds will not be an option.\n\nStep 1: Generate a Personal Access Token\nWe are going to generate a classic token.\n\nGo to https://github.com/settings/tokens\nClick Generate new token &gt; Generate new token (classic)\nWhen the pop-up shows up, fill in a description, click the “repo” checkbox, and then scroll to bottom to click “Generate”.\nSAVE the token. You need it for the next step.\n\n\n\nStep 2: Tell Git who your are\n\nOpen a terminal in JupyterLab or RStudio\nPaste these 3 lines of code into the terminal\n\ngit config --global credential.helper store",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-skills/02-git-authentication.html#test",
    "href": "topics-skills/02-git-authentication.html#test",
    "title": "GitHub Authentication",
    "section": "Test",
    "text": "Test\n\nGo to https://github.com/new\nCreate a PRIVATE repo called “test”\nMake sure to check the “Add a README file” box!\nOpen a terminal and type these lines. Make sure to replace &lt;username&gt;\n\ngit clone https://github.com/&lt;username&gt;/test\n\nIf you properly authenticated, git will ask for your username and password. At the password, paste in the TOKEN not your actual password.",
    "crumbs": [
      "JupyterHub Skills",
      "Git Authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/test.html",
    "href": "topics-2025/2025-opendap/test.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "Cases where opendap uses authentication. Some use .dodsrc file to tell it to use .netrc auth. https://tds.mercator-ocean.fr/userguide/user_guide.html\n\nimport xarray as xr\nurl=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\ndatapgn = xr.open_dataset(url)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:211, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    210 try:\n--&gt; 211     file = self._cache[self._key]\n    212 except KeyError:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56, in LRUCache.__getitem__(self, key)\n     55 with self._lock:\n---&gt; 56     value = self._cache[key]\n     57     self._cache.move_to_end(key)\n\nKeyError: [&lt;class 'netCDF4._netCDF4.Dataset'&gt;, ('https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '0229c544-0e21-4f3a-a8e0-5fa1693d0a2f']\n\nDuring handling of the above exception, another exception occurred:\n\nOSError                                   Traceback (most recent call last)\nCell In[3], line 3\n      1 import xarray as xr\n      2 url=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\n----&gt; 3 datapgn = xr.open_dataset(url)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:666, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    644 def open_dataset(\n    645     self,\n    646     filename_or_obj: str | os.PathLike[Any] | ReadBuffer | AbstractDataStore,\n   (...)\n    663     autoclose=False,\n    664 ) -&gt; Dataset:\n    665     filename_or_obj = _normalize_path(filename_or_obj)\n--&gt; 666     store = NetCDF4DataStore.open(\n    667         filename_or_obj,\n    668         mode=mode,\n    669         format=format,\n    670         group=group,\n    671         clobber=clobber,\n    672         diskless=diskless,\n    673         persist=persist,\n    674         auto_complex=auto_complex,\n    675         lock=lock,\n    676         autoclose=autoclose,\n    677     )\n    679     store_entrypoint = StoreBackendEntrypoint()\n    680     with close_on_error(store):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:452, in NetCDF4DataStore.open(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\n    448     kwargs[\"auto_complex\"] = auto_complex\n    449 manager = CachingFileManager(\n    450     netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n    451 )\n--&gt; 452 return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:393, in NetCDF4DataStore.__init__(self, manager, group, mode, lock, autoclose)\n    391 self._group = group\n    392 self._mode = mode\n--&gt; 393 self.format = self.ds.data_model\n    394 self._filename = self.ds.filepath()\n    395 self.is_remote = is_remote_uri(self._filename)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:461, in NetCDF4DataStore.ds(self)\n    459 @property\n    460 def ds(self):\n--&gt; 461     return self._acquire()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:455, in NetCDF4DataStore._acquire(self, needs_lock)\n    454 def _acquire(self, needs_lock=True):\n--&gt; 455     with self._manager.acquire_context(needs_lock) as root:\n    456         ds = _nc4_require_group(root, self._group, self._mode)\n    457     return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/contextlib.py:137, in _GeneratorContextManager.__enter__(self)\n    135 del self.args, self.kwds, self.func\n    136 try:\n--&gt; 137     return next(self.gen)\n    138 except StopIteration:\n    139     raise RuntimeError(\"generator didn't yield\") from None\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:199, in CachingFileManager.acquire_context(self, needs_lock)\n    196 @contextlib.contextmanager\n    197 def acquire_context(self, needs_lock=True):\n    198     \"\"\"Context manager for acquiring a file.\"\"\"\n--&gt; 199     file, cached = self._acquire_with_cache_info(needs_lock)\n    200     try:\n    201         yield file\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:217, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    215     kwargs = kwargs.copy()\n    216     kwargs[\"mode\"] = self._mode\n--&gt; 217 file = self._opener(*self._args, **kwargs)\n    218 if self._mode == \"w\":\n    219     # ensure file doesn't get overridden when opened again\n    220     self._mode = \"a\"\n\nFile src/netCDF4/_netCDF4.pyx:2521, in netCDF4._netCDF4.Dataset.__init__()\n\nFile src/netCDF4/_netCDF4.pyx:2158, in netCDF4._netCDF4._ensure_nc_success()\n\nOSError: [Errno -77] NetCDF: Access failure: 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0'\n\n\n\n\n#url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, decode_times=True)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile src/netCDF4/_netCDF4.pyx:5079, in netCDF4._netCDF4.Variable.__getitem__()\n\nFile src/netCDF4/_netCDF4.pyx:6051, in netCDF4._netCDF4.Variable._get()\n\nFile src/netCDF4/_netCDF4.pyx:2164, in netCDF4._netCDF4._ensure_nc_success()\n\nRuntimeError: NetCDF: Access failure\n\nDuring handling of the above exception, another exception occurred:\n\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[4], line 2\n      1 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\n----&gt; 2 ds = xr.open_dataset(url, decode_times=True)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:681, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    679 store_entrypoint = StoreBackendEntrypoint()\n    680 with close_on_error(store):\n--&gt; 681     ds = store_entrypoint.open_dataset(\n    682         store,\n    683         mask_and_scale=mask_and_scale,\n    684         decode_times=decode_times,\n    685         concat_characters=concat_characters,\n    686         decode_coords=decode_coords,\n    687         drop_variables=drop_variables,\n    688         use_cftime=use_cftime,\n    689         decode_timedelta=decode_timedelta,\n    690     )\n    691 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    394 stack_char_dim = (\n    395     _item_or_default(concat_characters, k, True)\n    396     and v.dtype == \"S1\"\n    397     and v.ndim &gt; 0\n    398     and stackable(v.dims[-1])\n    399 )\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n    412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    224         if use_cftime is not None:\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n    237     var = variables.EndianCoder().decode(var)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1326 units = pop_to(attrs, encoding, \"units\")\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n   1338 data = lazy_elemwise_func(data, transform, dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    295 def _decode_cf_datetime_dtype(\n    296     data,\n    297     units: str,\n   (...)\n    303     # successfully. Otherwise, tracebacks end up swallowed by\n    304     # Dataset.__repr__ when users try to view their lazily decoded array.\n    305     values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306     example_value = np.concatenate(\n--&gt; 307         [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308     )\n    310     try:\n    311         result = decode_cf_datetime(\n    312             example_value, units, calendar, use_cftime, time_unit\n    313         )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     95 if isinstance(array, Variable):\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    136     return data\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    569 def __array__(\n    570     self, dtype: np.typing.DTypeLike = None, /, *, copy: bool | None = None\n    571 ) -&gt; np.ndarray:\n    572     if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573         return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574     else:\n    575         return np.asarray(self.get_duck_array(), dtype=dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    648     array = apply_indexer(self.array, self.key)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n    658 if isinstance(array, ExplicitlyIndexed):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:103, in NetCDF4ArrayWrapper.__getitem__(self, key)\n    102 def __getitem__(self, key):\n--&gt; 103     return indexing.explicit_indexing_adapter(\n    104         key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n    105     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n    991 \"\"\"Support explicit indexing by delegating to a raw indexing method.\n    992 \n    993 Outer and/or vectorized indexers are supported by indexing a second time\n   (...)\n   1010 Indexing result, in the form of a duck numpy-array.\n   1011 \"\"\"\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n   1016     indexable = NumpyIndexingAdapter(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:116, in NetCDF4ArrayWrapper._getitem(self, key)\n    114     with self.datastore.lock:\n    115         original_array = self.get_array(needs_lock=False)\n--&gt; 116         array = getitem(original_array, key)\n    117 except IndexError as err:\n    118     # Catch IndexError in netCDF4 and return a more informative\n    119     # error message.  This is most often called when an unsorted\n    120     # indexer is used before the data is loaded from disk.\n    121     msg = (\n    122         \"The indexing operation you are attempting to perform \"\n    123         \"is not valid on netCDF4.Variable object. Try loading \"\n    124         \"your data into memory first by calling .load().\"\n    125     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:267, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    262 msg = (\n    263     f\"getitem failed, waiting {next_delay} ms before trying again \"\n    264     f\"({max_retries - n} tries remaining). Full traceback: {traceback.format_exc()}\"\n    265 )\n    266 logger.debug(msg)\n--&gt; 267 time.sleep(1e-3 * next_delay)\n\nKeyboardInterrupt: \n\n\n\n\n#url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, decode_times=True)\n\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\nsyntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\ncontext: HTTP^ Basic: Access denied.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile src/netCDF4/_netCDF4.pyx:5079, in netCDF4._netCDF4.Variable.__getitem__()\n\nFile src/netCDF4/_netCDF4.pyx:6051, in netCDF4._netCDF4.Variable._get()\n\nFile src/netCDF4/_netCDF4.pyx:2164, in netCDF4._netCDF4._ensure_nc_success()\n\nRuntimeError: NetCDF: Access failure\n\nDuring handling of the above exception, another exception occurred:\n\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[5], line 3\n      1 #url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/1980/01/MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\"\n      2 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n----&gt; 3 ds = xr.open_dataset(url, decode_times=True)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:681, in NetCDF4BackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\n    679 store_entrypoint = StoreBackendEntrypoint()\n    680 with close_on_error(store):\n--&gt; 681     ds = store_entrypoint.open_dataset(\n    682         store,\n    683         mask_and_scale=mask_and_scale,\n    684         decode_times=decode_times,\n    685         concat_characters=concat_characters,\n    686         decode_coords=decode_coords,\n    687         drop_variables=drop_variables,\n    688         use_cftime=use_cftime,\n    689         decode_timedelta=decode_timedelta,\n    690     )\n    691 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    394 stack_char_dim = (\n    395     _item_or_default(concat_characters, k, True)\n    396     and v.dtype == \"S1\"\n    397     and v.ndim &gt; 0\n    398     and stackable(v.dims[-1])\n    399 )\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n    412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    224         if use_cftime is not None:\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n    237     var = variables.EndianCoder().decode(var)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1326 units = pop_to(attrs, encoding, \"units\")\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n   1338 data = lazy_elemwise_func(data, transform, dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    295 def _decode_cf_datetime_dtype(\n    296     data,\n    297     units: str,\n   (...)\n    303     # successfully. Otherwise, tracebacks end up swallowed by\n    304     # Dataset.__repr__ when users try to view their lazily decoded array.\n    305     values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306     example_value = np.concatenate(\n--&gt; 307         [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308     )\n    310     try:\n    311         result = decode_cf_datetime(\n    312             example_value, units, calendar, use_cftime, time_unit\n    313         )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     95 if isinstance(array, Variable):\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    136     return data\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    569 def __array__(\n    570     self, dtype: np.typing.DTypeLike = None, /, *, copy: bool | None = None\n    571 ) -&gt; np.ndarray:\n    572     if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573         return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574     else:\n    575         return np.asarray(self.get_duck_array(), dtype=dtype)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    648     array = apply_indexer(self.array, self.key)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n    658 if isinstance(array, ExplicitlyIndexed):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:103, in NetCDF4ArrayWrapper.__getitem__(self, key)\n    102 def __getitem__(self, key):\n--&gt; 103     return indexing.explicit_indexing_adapter(\n    104         key, self.shape, indexing.IndexingSupport.OUTER, self._getitem\n    105     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n    991 \"\"\"Support explicit indexing by delegating to a raw indexing method.\n    992 \n    993 Outer and/or vectorized indexers are supported by indexing a second time\n   (...)\n   1010 Indexing result, in the form of a duck numpy-array.\n   1011 \"\"\"\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n   1016     indexable = NumpyIndexingAdapter(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:116, in NetCDF4ArrayWrapper._getitem(self, key)\n    114     with self.datastore.lock:\n    115         original_array = self.get_array(needs_lock=False)\n--&gt; 116         array = getitem(original_array, key)\n    117 except IndexError as err:\n    118     # Catch IndexError in netCDF4 and return a more informative\n    119     # error message.  This is most often called when an unsorted\n    120     # indexer is used before the data is loaded from disk.\n    121     msg = (\n    122         \"The indexing operation you are attempting to perform \"\n    123         \"is not valid on netCDF4.Variable object. Try loading \"\n    124         \"your data into memory first by calling .load().\"\n    125     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:267, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    262 msg = (\n    263     f\"getitem failed, waiting {next_delay} ms before trying again \"\n    264     f\"({max_retries - n} tries remaining). Full traceback: {traceback.format_exc()}\"\n    265 )\n    266 logger.debug(msg)\n--&gt; 267 time.sleep(1e-3 * next_delay)\n\nKeyboardInterrupt: \n\n\n\n\nimport pydap\nfrom pydap.util.urs import install_basic_client\ninstall_basic_client()\nfrom pydap.client import open_url\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[7], line 2\n      1 import pydap\n----&gt; 2 from pydap.util.urs import install_basic_client\n      3 install_basic_client()\n      4 from pydap.client import open_url\n\nModuleNotFoundError: No module named 'pydap.util'\n\n\n\n\n# this works\nfrom pydap.client import open_url\n\ndataset_url=\"https://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\npydap_dataset = open_url(dataset_url, protocol=\"dap4\")\n\n\nimport requests\nfrom pydap.client import open_url\n\n\n# this give redirect\nurl = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\nds = xr.open_dataset(url, engine=\"pydap\")\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:401, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    400 try:\n--&gt; 401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n    404         concat_characters=_item_or_default(concat_characters, k, True),\n    405         mask_and_scale=_item_or_default(mask_and_scale, k, True),\n    406         decode_times=_item_or_default(decode_times, k, True),\n    407         stack_char_dim=stack_char_dim,\n    408         use_cftime=_item_or_default(use_cftime, k, None),\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:234, in decode_cf_variable(name, var, concat_characters, mask_and_scale, decode_times, decode_endianness, stack_char_dim, use_cftime, decode_timedelta)\n    225             raise TypeError(\n    226                 \"Usage of 'use_cftime' as a kwarg is not allowed \"\n    227                 \"if a 'CFDatetimeCoder' instance is passed to \"\n   (...)\n    232                 \"    ds = xr.open_dataset(decode_times=time_coder)\\n\",\n    233             )\n--&gt; 234     var = decode_times.decode(var, name=name)\n    236 if decode_endianness and not var.dtype.isnative:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:1328, in CFDatetimeCoder.decode(self, variable, name)\n   1327 calendar = pop_to(attrs, encoding, \"calendar\")\n-&gt; 1328 dtype = _decode_cf_datetime_dtype(\n   1329     data, units, calendar, self.use_cftime, self.time_unit\n   1330 )\n   1331 transform = partial(\n   1332     decode_cf_datetime,\n   1333     units=units,\n   (...)\n   1336     time_unit=self.time_unit,\n   1337 )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/coding/times.py:307, in _decode_cf_datetime_dtype(data, units, calendar, use_cftime, time_unit)\n    305 values = indexing.ImplicitToExplicitIndexingAdapter(indexing.as_indexable(data))\n    306 example_value = np.concatenate(\n--&gt; 307     [first_n_items(values, 1) or [0], last_item(values) or [0]]\n    308 )\n    310 try:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/formatting.py:97, in first_n_items(array, n_desired)\n     96     array = array._data\n---&gt; 97 return np.ravel(to_duck_array(array))[:n_desired]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/namedarray/pycompat.py:138, in to_duck_array(data, **kwargs)\n    137 else:\n--&gt; 138     return np.asarray(data)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:573, in ImplicitToExplicitIndexingAdapter.__array__(self, dtype, copy)\n    572 if Version(np.__version__) &gt;= Version(\"2.0.0\"):\n--&gt; 573     return np.asarray(self.get_duck_array(), dtype=dtype, copy=copy)\n    574 else:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:578, in ImplicitToExplicitIndexingAdapter.get_duck_array(self)\n    577 def get_duck_array(self):\n--&gt; 578     return self.array.get_duck_array()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:652, in LazilyIndexedArray.get_duck_array(self)\n    649 else:\n    650     # If the array is not an ExplicitlyIndexedNDArrayMixin,\n    651     # it may wrap a BackendArray so use its __getitem__\n--&gt; 652     array = self.array[self.key]\n    654 # self.array[self.key] is now a numpy array when\n    655 # self.array is a BackendArray subclass\n    656 # and self.key is BasicIndexer((slice(None, None, None),))\n    657 # so we need the explicit check for ExplicitlyIndexed\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:47, in PydapArrayWrapper.__getitem__(self, key)\n     46 def __getitem__(self, key):\n---&gt; 47     return indexing.explicit_indexing_adapter(\n     48         key, self.shape, indexing.IndexingSupport.BASIC, self._getitem\n     49     )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/indexing.py:1013, in explicit_indexing_adapter(key, shape, indexing_support, raw_indexing_method)\n   1012 raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n-&gt; 1013 result = raw_indexing_method(raw_key.tuple)\n   1014 if numpy_indices.tuple:\n   1015     # index the loaded np.ndarray\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:55, in PydapArrayWrapper._getitem(self, key)\n     54 array = getattr(self.array, \"array\", self.array)\n---&gt; 55 result = robust_getitem(array, key, catch=ValueError)\n     56 result = np.asarray(result)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/common.py:256, in robust_getitem(array, key, catch, max_retries, initial_delay)\n    255 try:\n--&gt; 256     return array[key]\n    257 except catch:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:341, in BaseType.__getitem__(self, index)\n    340 out = copy.copy(self)\n--&gt; 341 out.data = self._get_data_index(index)\n    342 if type(self.data).__name__ == \"BaseProxyDap4\":\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:373, in BaseType._get_data_index(self, index)\n    372 else:\n--&gt; 373     return self._data[index]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:379, in BaseProxyDap2.__getitem__(self, index)\n    371 r = GET(\n    372     url,\n    373     self.application,\n   (...)\n    376     verify=self.verify,\n    377 )\n--&gt; 379 raise_for_status(r)\n    380 dds, data = safe_dds_and_data(r, self.user_charset)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/net.py:51, in raise_for_status(response)\n     50     text = \"\"\n---&gt; 51 raise HTTPError(\n     52     detail=(\n     53         response.status\n     54         + \"\\n\"\n     55         + text\n     56         + \"\\n\"\n     57         + \"This is redirect error. These should not usually raise \"\n     58         + \"an error in pydap beacuse redirects are handled \"\n     59         + \"implicitly. If it failed it is likely due to a \"\n     60         + \"circular redirect.\"\n     61     ),\n     62     headers=response.headers,\n     63     comment=response.body,\n     64 )\n\nHTTPError: 302 Found\n&lt;!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;title&gt;302 Found&lt;/title&gt;\n&lt;/head&gt;&lt;body&gt;\n&lt;h1&gt;Found&lt;/h1&gt;\n&lt;p&gt;The document has moved &lt;a href=\"https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&amp;app_type=401&amp;client_id=e2WVk8Pw6weeLUKZYOxvTQ&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fgoldsmr4.gesdisc.eosdis.nasa.gov%2Fdata-redirect&amp;state=aHR0cHM6Ly9nb2xkc21yNC5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9vcGVuZGFwL2h5cmF4L01FUlJBMi9NMkkxTlhBU00uNS4xMi40LzE5ODAvMDEvTUVSUkEyXzEwMC5pbnN0MV8yZF9hc21fTnguMTk4MDAxMDEubmM0LmRvZHM%2FdGltZSU1QjA6MTowJTVE\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\nThis is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.\n\nThe above exception was the direct cause of the following exception:\n\nHTTPError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 url = \"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n----&gt; 2 ds = xr.open_dataset(url, engine=\"pydap\")\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:203, in PydapBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, application, session, output_grid, timeout, verify, user_charset)\n    201 store_entrypoint = StoreBackendEntrypoint()\n    202 with close_on_error(store):\n--&gt; 203     ds = store_entrypoint.open_dataset(\n    204         store,\n    205         mask_and_scale=mask_and_scale,\n    206         decode_times=decode_times,\n    207         concat_characters=concat_characters,\n    208         decode_coords=decode_coords,\n    209         drop_variables=drop_variables,\n    210         use_cftime=use_cftime,\n    211         decode_timedelta=decode_timedelta,\n    212     )\n    213     return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/store.py:47, in StoreBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\n     44 vars, attrs = filename_or_obj.load()\n     45 encoding = filename_or_obj.get_encoding()\n---&gt; 47 vars, attrs, coord_names = conventions.decode_cf_variables(\n     48     vars,\n     49     attrs,\n     50     mask_and_scale=mask_and_scale,\n     51     decode_times=decode_times,\n     52     concat_characters=concat_characters,\n     53     decode_coords=decode_coords,\n     54     drop_variables=drop_variables,\n     55     use_cftime=use_cftime,\n     56     decode_timedelta=decode_timedelta,\n     57 )\n     59 ds = Dataset(vars, attrs=attrs)\n     60 ds = ds.set_coords(coord_names.intersection(vars))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:412, in decode_cf_variables(variables, attributes, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime, decode_timedelta)\n    401     new_vars[k] = decode_cf_variable(\n    402         k,\n    403         v,\n   (...)\n    409         decode_timedelta=_item_or_default(decode_timedelta, k, None),\n    410     )\n    411 except Exception as e:\n--&gt; 412     raise type(e)(f\"Failed to decode variable {k!r}: {e}\") from e\n    413 if decode_coords in [True, \"coordinates\", \"all\"]:\n    414     var_attrs = new_vars[k].attrs\n\nHTTPError: Failed to decode variable 'time': 302 Found\n&lt;!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;title&gt;302 Found&lt;/title&gt;\n&lt;/head&gt;&lt;body&gt;\n&lt;h1&gt;Found&lt;/h1&gt;\n&lt;p&gt;The document has moved &lt;a href=\"https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&amp;app_type=401&amp;client_id=e2WVk8Pw6weeLUKZYOxvTQ&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fgoldsmr4.gesdisc.eosdis.nasa.gov%2Fdata-redirect&amp;state=aHR0cHM6Ly9nb2xkc21yNC5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9vcGVuZGFwL2h5cmF4L01FUlJBMi9NMkkxTlhBU00uNS4xMi40LzE5ODAvMDEvTUVSUkEyXzEwMC5pbnN0MV8yZF9hc21fTnguMTk4MDAxMDEubmM0LmRvZHM%2FdGltZSU1QjA6MTowJTVE\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\nThis is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect.\n\n\n\n\nfrom pydap.client import open_url\n\ndataset_url=\"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA2/M2I1NXASM.5.12.4/1980/01/MERRA2_100.inst1_2d_asm_Nx.19800101.nc4\"\n\npydap_dataset = open_url(dataset_url, protocol=\"dap4\")\n\n\nfrom pydap.client import open_url\n\nurl=\"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\nds = xr.open_dataset(url, engine=\"pydap\")\n\n\nfrom pydap.client import open_url\n\nurls=[\"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\", \n      \"dap4://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"]\n\nds = xr.open_dataset(url, engine=\"pydap\")\n\n&lt;DatasetType with children 'U2M', 'TROPT', 'TROPPB', 'T2M', 'TQL', 'TOX', 'PS', 'V50M', 'DISPH', 'TO3', 'TS', 'T10M', 'TROPPT', 'TQI', 'SLP', 'TQV', 'V2M', 'TROPQ', 'V10M', 'U50M', 'U10M', 'QV2M', 'TROPPV', 'QV10M', 'lat', 'lon', 'time'&gt;\n\n\n\n%%time\nurl_DAP4 = \"dap4://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0310/PACE_OCI.20240310.L3m.DAY.CHL.V2_0.chlor_a.4km.NRT.nc\"\nds_full = open_url(url_DAP4)\n\nCPU times: user 23.4 ms, sys: 0 ns, total: 23.4 ms\nWall time: 1.16 s\n\n\n\n%time\nds = xr.open_dataset(url_DAP4, engine=\"pydap\")\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 5.01 µs\n\n\n\nprint('uncompressed dataset size [GBs]: ', ds['chlor_a'].nbytes / 1e9)\n\nuncompressed dataset size [GBs]:  0.1492992\n\n\n\nds['chlor_a']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'chlor_a' (/lat: 4320, /lon: 8640)&gt; Size: 149MB\n[37324800 values with dtype=float32]\nDimensions without coordinates: /lat, /lon\nAttributes:\n    long_name:      Chlorophyll Concentration, OCI Algorithm\n    units:          mg m^-3\n    standard_name:  mass_concentration_of_chlorophyll_in_sea_water\n    valid_min:      0.00100000005\n    valid_max:      100.0\n    reference:      Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a alg...\n    display_scale:  log\n    display_min:    0.00999999978\n    display_max:    20.0\n    Maps:           ('/lat', '/lon')xarray.DataArray'chlor_a'/lat: 4320/lon: 8640...[37324800 values with dtype=float32]Coordinates: (0)Indexes: (0)Attributes: (10)long_name :Chlorophyll Concentration, OCI Algorithmunits :mg m^-3standard_name :mass_concentration_of_chlorophyll_in_sea_watervalid_min :0.00100000005valid_max :100.0reference :Hu, C., Lee Z., and Franz, B.A. (2012). Chlorophyll-a algorithms for oligotrophic oceans: A novel approach based on three-band reflectance difference, J. Geophys. Res., 117, C01011, doi:10.1029/2011JC007395.display_scale :logdisplay_min :0.00999999978display_max :20.0Maps :('/lat', '/lon')\n\n\n\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 40MB\nDimensions:  (time: 24, lat: 361, lon: 576)\nDimensions without coordinates: time, lat, lon\nData variables:\n    T2M      (time, lat, lon) float64 40MB ...\nAttributes: (12/36)\n    History:                           Original file generated: Sat May 31 17...\n    Comment:                           GMAO filename: d5124_m2_jan79.tavg1_2d...\n    Filename:                          MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    created:                           2025-01-07T19:09:32Z\n    build_dmrpp:                       3.21.0-526\n    bes:                               3.21.0-526\n    libdap:                            libdap-3.21.0-120\n    configuration:                     \\n# TheBESKeys::get_as_config()\\nAllow...\n    invocation:                        build_dmrpp -c /tmp/bes_conf_t086 -f /...xarray.DatasetDimensions:time: 24lat: 361lon: 576Coordinates: (0)Data variables: (1)T2M(time, lat, lon)float64...long_name :2-meter_air_temperatureunits :Kfmissing_value :999999987000000.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]Maps :('/time', '/lat', '/lon')[4990464 values with dtype=float64]Indexes: (0)Attributes: (36)History :Original file generated: Sat May 31 17:43:10 2014 GMTComment :GMAO filename: d5124_m2_jan79.tavg1_2d_slv_Nx.19800101.nc4Filename :MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4ProductionDateTime :Original file generated: Sat May 31 17:43:10 2014 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :1980-01-01RangeBeginningTime :00:00:00.000000RangeEndingDate :1980-01-01RangeEndingTime :23:59:59.000000created :2025-01-07T19:09:32Zbuild_dmrpp :3.21.0-526bes :3.21.0-526libdap :libdap-3.21.0-120configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.RootDirectory=/tmp/tmp3z1dyeym/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Data.RootDirectory=/dev/null\nBES.LogName=./bes.log\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.EnableCF=false\nH5.EnableCheckNameClashing=true\ninvocation :build_dmrpp -c /tmp/bes_conf_t086 -f /tmp/tmp3z1dyeym//MERRA2_100.tavg1_2d_slv_Nx.19800101.nc4 -r /tmp/dmr__iclKFq -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\n\nds[\"T2M\"].isel(time=1).plot()\n\n\n\n\n\n\n\n\n\nurl = \"https://opendap.earthdata.nasa.gov/collections/C1276812863-GES_DISC/granules/M2T1NXSLV.5.12.4%3AMERRA2_100.tavg1_2d_slv_Nx.19800101.nc4?dap4.ce=/T2M\"\n\n\n%time\nds = xr.open_dataset(url, engine=\"pydap\", decode_cf=True)\n\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 4.53 µs\n\n\n\nprint(ds.dims)\n\nFrozenMappingWarningOnValuesAccess({'/time': 24, '/lat': 361, '/lon': 576})\n\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[32], line 1\n----&gt; 1 ds.isel(time=1).plot()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:3100, in Dataset.isel(self, indexers, drop, missing_dims, **indexers_kwargs)\n   3096     return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n   3098 # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n   3099 # lists, or zero or one-dimensional np.ndarray's\n-&gt; 3100 indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n   3102 variables = {}\n   3103 dims: dict[Hashable, int] = {}\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/utils.py:844, in drop_dims_from_indexers(indexers, dims, missing_dims)\n    842     invalid = indexers.keys() - set(dims)\n    843     if invalid:\n--&gt; 844         raise ValueError(\n    845             f\"Dimensions {invalid} do not exist. Expected one or more of {dims}\"\n    846         )\n    848     return indexers\n    850 elif missing_dims == \"warn\":\n    851     # don't modify input\n\nValueError: Dimensions {'time'} do not exist. Expected one or more of FrozenMappingWarningOnValuesAccess({'/time': 24, '/lat': 361, '/lon': 576})\n\n\n\n\n%time\nurl=\"https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXFLX.5.12.4/1980/01/MERRA2_100.tavg1_2d_flx_Nx.19800101.nc4\"\n#ds = xr.open_dataset(url, engine=\"pydap\")\npydap_dataset = open_url(url, protocol=\"dap4\")\n\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 4.53 µs\n\n\n\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 xr.open_dataset(pydap_dataset)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:667, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    664     kwargs.update(backend_kwargs)\n    666 if engine is None:\n--&gt; 667     engine = plugins.guess_engine(filename_or_obj)\n    669 if from_array_kwargs is None:\n    670     from_array_kwargs = {}\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/plugins.py:194, in guess_engine(store_spec)\n    186 else:\n    187     error_msg = (\n    188         \"found the following matches with the input file in xarray's IO \"\n    189         f\"backends: {compatible_engines}. But their dependencies may not be installed, see:\\n\"\n    190         \"https://docs.xarray.dev/en/stable/user-guide/io.html \\n\"\n    191         \"https://docs.xarray.dev/en/stable/getting-started-guide/installing.html\"\n    192     )\n--&gt; 194 raise ValueError(error_msg)\n\nValueError: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'h5netcdf', 'scipy', 'kerchunk', 'pydap', 'rasterio', 'zarr']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
  },
  {
    "objectID": "topics-2025/2025-opendap/ea.html",
    "href": "topics-2025/2025-opendap/ea.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "import earthaccess\nauth = earthaccess.login()\nconcept_id = 'C2036881712-POCLOUD'\nresults = earthaccess.search_data(\n    concept_id = concept_id\n)\nlen(results)\n\n3360\n\n\n\nprint(results[0])\n\nCollection: {'Version': '2.1', 'ShortName': 'AVHRR_OI-NCEI-L4-GLOB-v2.1'}\nSpatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'BoundingRectangles': [{'WestBoundingCoordinate': -179.875, 'SouthBoundingCoordinate': -89.875, 'EastBoundingCoordinate': 179.875, 'NorthBoundingCoordinate': 89.875}]}}}\nTemporal coverage: {'RangeDateTime': {'EndingDateTime': '2016-01-02T00:00:00.000Z', 'BeginningDateTime': '2016-01-01T00:00:00.000Z'}}\nSize(MB): 0.995025634765625\nData: ['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20160101120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc']\n\n\n\nimport requests\n#CMR Link to use\n#https://cmr.earthdata.nasa.gov/search/granules.umm_json?collection_concept_id=C1625128926-GHRC_CLOUD&temporal=2019-01-01T10:00:00Z,2019-12-31T23:59:59Z\nr = requests.get('https://cmr.earthdata.nasa.gov/search/granules.umm_json?collection_concept_id=C1996881146-POCLOUD&temporal=2019-01-01T10:00:00Z,2019-02-01T00:00:00Z&pageSize=365')\nresponse_body = r.json()\n\n\nod_files = []\nfor itm in response_body['items']:\n    for urls in itm['umm']['RelatedUrls']:\n        if 'OPeNDAP' in urls['Description']:\n            od_files.append(urls['URL'])\n\nod_files[1]+'.dap.nc4'\n\n'https://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/GHRSST%20Level%204%20MUR%20Global%20Foundation%20Sea%20Surface%20Temperature%20Analysis%20(v4.1)/granules/20190102090000-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.dap.nc4'\n\n\n\ndir(results[0])\n\n['__annotations__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__ior__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__or__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__ror__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_basic_meta_fields_',\n '_basic_umm_fields_',\n '_derive_s3_link',\n '_filter_fields_',\n '_filter_related_links',\n '_repr_html_',\n 'clear',\n 'cloud_hosted',\n 'copy',\n 'data_links',\n 'dataviz_links',\n 'fromkeys',\n 'get',\n 'get_s3_credentials_endpoint',\n 'items',\n 'keys',\n 'pop',\n 'popitem',\n 'render_dict',\n 'setdefault',\n 'size',\n 'update',\n 'uuid',\n 'values']\n\n\n\nresults[0].data_links\n\n&lt;bound method DataGranule.data_links of Collection: {'Version': '2.1', 'ShortName': 'AVHRR_OI-NCEI-L4-GLOB-v2.1'}\nSpatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'BoundingRectangles': [{'WestBoundingCoordinate': -179.875, 'SouthBoundingCoordinate': -89.875, 'EastBoundingCoordinate': 179.875, 'NorthBoundingCoordinate': 89.875}]}}}\nTemporal coverage: {'RangeDateTime': {'EndingDateTime': '2016-01-02T00:00:00.000Z', 'BeginningDateTime': '2016-01-01T00:00:00.000Z'}}\nSize(MB): 0.995025634765625\nData: ['https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/AVHRR_OI-NCEI-L4-GLOB-v2.1/20160101120000-NCEI-L4_GHRSST-SSTblend-AVHRR_OI-GLOB-v02.0-fv02.1.nc']&gt;\n\n\n\nimport earthaccess\nimport pydap\nearthaccess.login()\nsession = earthaccess.get_requests_https_session()\npydap_ds = pydap.client.open_url(od_files[1], protocol=\"dap4\", session=session)\n\n\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\npydap_ds = pydap.client.open_url(eula_url, protocol=\"dap4\", session=session)\n\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\n%%time\nimport xarray as xr\n# very fast and this is 1Tb of data\nds = xr.open_mfdataset(eula_urls, engine=\"pydap\", combine=\"nested\", concat_dim=\"/time\", decode_cf=False, session=session)\nds\n\nCPU times: user 171 ms, sys: 8.5 ms, total: 180 ms\nWall time: 9.54 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (/time: 48, /lat: 361, /lon: 576)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/50)\n    U2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    V250      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPT     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPPB    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    T2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TQL       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    ...        ...\n    T2MWET    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    U500      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    QV10M     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    lat       (/time, /lat) float64 139kB dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;\n    lon       (/time, /lon) float64 221kB dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;\n    time      (/time) int32 192B dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;\nAttributes: (12/31)\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000\n    Unlimited_Dimension:               timexarray.DatasetDimensions:/time: 48/lat: 361/lon: 576Coordinates: (0)Data variables: (50)U2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_temperature_using_blended_TROPP_estimateunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPB(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_blended_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPBMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_liquid_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_500_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTOX(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_odd_oxygenunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOXMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nOMEGA500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :omega_at_500_hPaunits :Pa s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_250_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_250_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MDEW(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :dew_point_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEWMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPBLTOP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :pbltop_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDPRS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_500_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nDISPH(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :zero_plane_displacement_heightunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPHMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH1000(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_1000_mbunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTO3(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_ozoneunits :Dobsons_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_skin_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_thermal_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQI(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_ice_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQIMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nSLP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :sea_level_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_850_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nZLCL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :lifting_condensation_levelunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_water_vaporunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_250_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPQ(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_850_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_850_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDTMP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_EPV_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_500_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MWET(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :wet_bulb_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWETMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nlat(/time, /lat)float64dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n135.38 kiB\n67.69 kiB\n\n\nShape\n(48, 361)\n(24, 361)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          361 48\n\n\n\n\nlon(/time, /lon)float64dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/lonMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n216.00 kiB\n108.00 kiB\n\n\nShape\n(48, 576)\n(24, 576)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          576 48\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;long_name :timeunits :minutes since 2016-06-01 00:30:00time_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n192 B\n96 B\n\n\nShape\n(48,)\n(24,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          48 1\n\n\n\n\nIndexes: (0)Attributes: (31)History :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000Unlimited_Dimension :time\n\n\n\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).load()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'T2M' (lat: 361, lon: 576)&gt; Size: 832kB\narray([[218.05759, 218.05759, 218.05759, ..., 218.05759, 218.05759,\n        218.05759],\n       [217.35446, 217.35446, 217.33884, ..., 217.38571, 217.38571,\n        217.37009],\n       [217.10446, 217.10446, 217.10446, ..., 217.10446, 217.10446,\n        217.10446],\n       ...,\n       [266.9951 , 266.98727, 266.97946, ..., 267.01462, 267.0068 ,\n        266.999  ],\n       [268.10446, 268.10446, 268.10446, ..., 268.10056, 268.10056,\n        268.10446],\n       [269.124  , 269.124  , 269.124  , ..., 269.124  , 269.124  ,\n        269.124  ]], dtype=float32)\nDimensions without coordinates: lat, lon\nAttributes: (12/14)\n    long_name:       2-meter_air_temperature\n    units:           K\n    _FillValue:      999999987000000.0\n    missing_value:   999999987000000.0\n    fmissing_value:  999999987000000.0\n    scale_factor:    1.0\n    ...              ...\n    vmax:            999999987000000.0\n    vmin:            -999999987000000.0\n    valid_range:     [-999999987000000.0, 999999987000000.0]\n    origname:        T2M\n    fullnamepath:    /T2M\n    Maps:            ()xarray.DataArray'T2M'lat: 361lon: 576218.1 218.1 218.1 218.1 218.1 218.1 ... 269.1 269.1 269.1 269.1 269.1array([[218.05759, 218.05759, 218.05759, ..., 218.05759, 218.05759,\n        218.05759],\n       [217.35446, 217.35446, 217.33884, ..., 217.38571, 217.38571,\n        217.37009],\n       [217.10446, 217.10446, 217.10446, ..., 217.10446, 217.10446,\n        217.10446],\n       ...,\n       [266.9951 , 266.98727, 266.97946, ..., 267.01462, 267.0068 ,\n        266.999  ],\n       [268.10446, 268.10446, 268.10446, ..., 268.10056, 268.10056,\n        268.10446],\n       [269.124  , 269.124  , 269.124  , ..., 269.124  , 269.124  ,\n        269.124  ]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (14)long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\n\nimport xarray as xr\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()\n\n# Try loading metadata for the whole collection\nurl = \"https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L1/GPM_1BPR.07/\"\nds = xr.open_dataset(url, engine=\"pydap\", session=edl_session)\n\nprint(ds)\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[4], line 8\n      6 # Try loading metadata for the whole collection\n      7 url = \"https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L1/GPM_1BPR.07/\"\n----&gt; 8 ds = xr.open_dataset(url, engine=\"pydap\", session=edl_session)\n     10 print(ds)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:686, in open_dataset(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\n    674 decoders = _resolve_decoders_kwargs(\n    675     decode_cf,\n    676     open_backend_dataset_parameters=backend.open_dataset_parameters,\n   (...)\n    682     decode_coords=decode_coords,\n    683 )\n    685 overwrite_encoded_chunks = kwargs.pop(\"overwrite_encoded_chunks\", None)\n--&gt; 686 backend_ds = backend.open_dataset(\n    687     filename_or_obj,\n    688     drop_variables=drop_variables,\n    689     **decoders,\n    690     **kwargs,\n    691 )\n    692 ds = _dataset_from_backend_dataset(\n    693     backend_ds,\n    694     filename_or_obj,\n   (...)\n    704     **kwargs,\n    705 )\n    706 return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:191, in PydapBackendEntrypoint.open_dataset(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, application, session, output_grid, timeout, verify, user_charset)\n    173 def open_dataset(\n    174     self,\n    175     filename_or_obj: str | os.PathLike[Any] | ReadBuffer | AbstractDataStore,\n   (...)\n    189     user_charset=None,\n    190 ) -&gt; Dataset:\n--&gt; 191     store = PydapDataStore.open(\n    192         url=filename_or_obj,\n    193         application=application,\n    194         session=session,\n    195         output_grid=output_grid,\n    196         timeout=timeout,\n    197         verify=verify,\n    198         user_charset=user_charset,\n    199     )\n    201     store_entrypoint = StoreBackendEntrypoint()\n    202     with close_on_error(store):\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/pydap_.py:129, in PydapDataStore.open(cls, url, application, session, output_grid, timeout, verify, user_charset)\n    127 if user_charset is not None:\n    128     kwargs.update({\"user_charset\": user_charset})\n--&gt; 129 ds = pydap.client.open_url(**kwargs)\n    130 return cls(ds)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/client.py:78, in open_url(url, application, session, output_grid, timeout, verify, user_charset, protocol)\n     62 def open_url(\n     63     url,\n     64     application=None,\n   (...)\n     70     protocol=None,\n     71 ):\n     72     \"\"\"\n     73     Open a remote URL, returning a dataset.\n     74 \n     75     set output_grid to `False` to retrieve only main arrays and\n     76     never retrieve coordinate axes.\n     77     \"\"\"\n---&gt; 78     handler = pydap.handlers.dap.DAPHandler(\n     79         url,\n     80         application,\n     81         session,\n     82         output_grid,\n     83         timeout=timeout,\n     84         verify=verify,\n     85         user_charset=user_charset,\n     86         protocol=protocol,\n     87     )\n     88     dataset = handler.dataset\n     90     # attach server-side functions\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:101, in DAPHandler.__init__(self, url, application, session, output_grid, timeout, verify, user_charset, protocol)\n     92 arg = (\n     93     self.scheme,\n     94     self.netloc,\n   (...)\n     98     self.fragment,\n     99 )\n    100 self.base_url = urlunparse(arg)\n--&gt; 101 self.make_dataset()\n    102 self.add_proxies()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:137, in DAPHandler.make_dataset(self)\n    135     self.dataset_from_dap4()\n    136 else:\n--&gt; 137     self.dataset_from_dap2()\n    138     self.attach_das()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:181, in DAPHandler.dataset_from_dap2(self)\n    164 dds_url = urlunparse(\n    165     (\n    166         self.scheme,\n   (...)\n    172     )\n    173 )\n    174 r = GET(\n    175     dds_url,\n    176     self.application,\n   (...)\n    179     verify=self.verify,\n    180 )\n--&gt; 181 raise_for_status(r)\n    182 dds = safe_charset_text(r, self.user_charset)\n    183 self.dataset = dds_to_dataset(dds)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/net.py:37, in raise_for_status(response)\n     34 def raise_for_status(response):\n     35     # Raise error if status is above 300:\n     36     if response.status_code &gt;= 400:\n---&gt; 37         raise HTTPError(\n     38             detail=response.status + \"\\n\" + response.text,\n     39             headers=response.headers,\n     40             comment=response.body,\n     41         )\n     42     elif response.status_code &gt;= 300:\n     43         try:\n\nHTTPError: 404 404\n\n\n\n\n\n\n\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n\n\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n&lt;head&gt;\n    &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/&gt;\n    &lt;link rel='stylesheet' href='/opendap/docs/css/contents.css' type='text/css'/&gt;\n    &lt;title&gt;Hyrax - Resource Not Found (404)&lt;/title&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n&lt;p align=\"left\"&gt;&nbsp;&lt;/p&gt;\n\n&lt;h1 align=\"center\"&gt;Hyrax - Resource Not Found (404)&lt;/h1&gt;\n&lt;hr align=\"left\" size=\"1\" noshade=\"noshade\"/&gt;\n&lt;table width=\"100%\" border=\"0\"&gt;\n    &lt;tr&gt;\n        &lt;td&gt;\n            &lt;a href=\"/opendap/docs/images/largeEarth.jpg\"&gt;\n            &lt;img src=\"/opendap/docs/images/smallEarth.jpg\"\n                 alt=\"I looked everywhere!\"\n                 title=\"I looked everywhere!\"\n                 border=\"0\"/&gt;\n            &lt;/a&gt;\n        &lt;/td&gt;\n\n        &lt;td&gt;\n            &lt;p align=\"left\"&gt;The URL requested does not describe a resource that can be found on this server.&lt;/p&gt;\n\n            &lt;p align=\"left\"&gt;If you would like to start at the top level of this server, go &lt;a\n                    href=\"/opendap/\"&gt;&lt;strong&gt;HERE&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;\n\n            \n            &lt;p align=\"left\"&gt;The specific error message associated with your request was:&lt;/p&gt;\n            &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Failed to locate resource: /TRMM_L1/GPM_1BPR.07/.dds &lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;\n            \n            \n            &lt;p align=\"left\"&gt; If you think that the server is broken (that the URL you submitted should have worked),\n                then please contact the OPeNDAP user support coordinator at:\n                &lt;a href=\"mailto:gsfc-dl-help-disc@mail.nasa.gov?subject=Hyrax Error 404&amp;body=%0A%0A%0A%0A%0A# -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --%0A# %0A# We&#39;re sorry you had a problem using the server.%0A# Please use the space above to describe what you%0A# were trying to do and we will try to assist you.%0A# Thanks,%0A# OPeNDAP Support.%0A# %0A# -- -- -- hyrax error info, please include -- -- --%0A# %0A# ReqInfo.getRequestUrl(): https://disc2.gesdisc.eosdis.nasa.gov/opendap/TRMM_L1/GPM_1BPR.07/.dds%0A# request_url: https://disc2.gesdisc.eosdis.nasa.gov/opendap/error/error404.jsp%0A# protocol: HTTP/1.1%0A# server: disc2.gesdisc.eosdis.nasa.gov%0A# port: 443%0A# javax.servlet.forward.request_uri: /opendap/TRMM_L1/GPM_1BPR.07/.dds%0A# query_string: n/a%0A# status: 404%0A# message: Failed to locate resource: /TRMM_L1/GPM_1BPR.07/.dds%0A# %0A# -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --%0A\"&gt;gsfc-dl-help-disc@mail.nasa.gov&lt;/a&gt;\n            &lt;/p&gt;\n            \n\n        &lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n&lt;hr align=\"left\" size=\"1\" noshade=\"noshade\"/&gt;\n&lt;h1 align=\"center\"&gt;Hyrax - Resource Not Found (404)&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\n\nimport requests\nimport pydap.client\n#import xarray\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\nmy_session=requests.Session()\npydap_ds = pydap.client.open_url(eula_url, session=my_session, protocol=\"dap4\")\ntest_ds = pydap_ds['lat'][:]\n\n\n# works when we try to get data\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 # works when we try to get data\n----&gt; 2 test_ds = pydap_ds['lat'][:]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:341, in BaseType.__getitem__(self, index)\n    339 def __getitem__(self, index):\n    340     out = copy.copy(self)\n--&gt; 341     out.data = self._get_data_index(index)\n    342     if type(self.data).__name__ == \"BaseProxyDap4\":\n    343         out.attributes[\"checksum\"] = self.data.checksum\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/model.py:373, in BaseType._get_data_index(self, index)\n    371     return np.vectorize(decode_np_strings)(self._data[index])\n    372 else:\n--&gt; 373     return self._data[index]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:462, in BaseProxyDap4.__getitem__(self, index)\n    452 logger.info(\"Fetching URL: %s\" % url)\n    454 r = GET(\n    455     url,\n    456     self.application,\n   (...)\n    459     verify=self.verify,\n    460 )\n--&gt; 462 raise_for_status(r)\n    463 dataset = UNPACKDAP4DATA(r, self.user_charset).dataset\n    464 self.checksum = dataset[self.id].attributes[\"checksum\"]\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/net.py:51, in raise_for_status(response)\n     45 except AttributeError:\n     46     # With this status_code, response.text could\n     47     # be ill-defined. If the redirect does not set\n     48     # an encoding (i.e. response.charset is None).\n     49     # Set the text to empty string:\n     50     text = \"\"\n---&gt; 51 raise HTTPError(\n     52     detail=(\n     53         response.status\n     54         + \"\\n\"\n     55         + text\n     56         + \"\\n\"\n     57         + \"This is redirect error. These should not usually raise \"\n     58         + \"an error in pydap beacuse redirects are handled \"\n     59         + \"implicitly. If it failed it is likely due to a \"\n     60         + \"circular redirect.\"\n     61     ),\n     62     headers=response.headers,\n     63     comment=response.body,\n     64 )\n\nHTTPError: 302 Found\n&lt;!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"&gt;\n&lt;html&gt;&lt;head&gt;\n&lt;title&gt;302 Found&lt;/title&gt;\n&lt;/head&gt;&lt;body&gt;\n&lt;h1&gt;Found&lt;/h1&gt;\n&lt;p&gt;The document has moved &lt;a href=\"https://urs.earthdata.nasa.gov/oauth/authorize/?scope=uid&amp;app_type=401&amp;client_id=e2WVk8Pw6weeLUKZYOxvTQ&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fgoldsmr4.gesdisc.eosdis.nasa.gov%2Fdata-redirect&amp;state=aHR0cHM6Ly9nb2xkc21yNC5nZXNkaXNjLmVvc2Rpcy5uYXNhLmdvdi9vcGVuZGFwL01FUlJBMi9NMlQxTlhTTFYuNS4xMi40LzIwMTYvMDYvTUVSUkEyXzQwMC50YXZnMV8yZF9zbHZfTnguMjAxNjA2MDEubmM0LmRhcD9kYXA0LmNlPWxhdCU1QjA6MTozNjAlNUQ\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\nThis is redirect error. These should not usually raise an error in pydap beacuse redirects are handled implicitly. If it failed it is likely due to a circular redirect."
  },
  {
    "objectID": "topics-2025/2025-opendap/Untitled.html",
    "href": "topics-2025/2025-opendap/Untitled.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "pip install git+https://github.com/pydap/pydap.git@main\n\nCollecting git+https://github.com/pydap/pydap.git@main\n  Cloning https://github.com/pydap/pydap.git (to revision main) to /tmp/pip-req-build-p2vtu_x0\n  Running command git clone --filter=blob:none --quiet https://github.com/pydap/pydap.git /tmp/pip-req-build-p2vtu_x0\n  Resolved https://github.com/pydap/pydap.git to commit 13e1b07040e0466e099eb8e7e2633195d08647d2\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (2.1.3)\nRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (2.32.3)\nCollecting requests-cache (from pydap==3.5.4.dev10+g13e1b07)\n  Using cached requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (1.15.2)\nRequirement already satisfied: Webob in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (1.8.9)\nRequirement already satisfied: beautifulsoup4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (4.13.3)\nRequirement already satisfied: lxml in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pydap==3.5.4.dev10+g13e1b07) (5.3.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from beautifulsoup4-&gt;pydap==3.5.4.dev10+g13e1b07) (2.5)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from beautifulsoup4-&gt;pydap==3.5.4.dev10+g13e1b07) (4.12.2)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-&gt;pydap==3.5.4.dev10+g13e1b07) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-&gt;pydap==3.5.4.dev10+g13e1b07) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-&gt;pydap==3.5.4.dev10+g13e1b07) (1.26.19)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-&gt;pydap==3.5.4.dev10+g13e1b07) (2025.1.31)\nRequirement already satisfied: attrs&gt;=21.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-cache-&gt;pydap==3.5.4.dev10+g13e1b07) (25.2.0)\nCollecting cattrs&gt;=22.2 (from requests-cache-&gt;pydap==3.5.4.dev10+g13e1b07)\n  Using cached cattrs-24.1.2-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: platformdirs&gt;=2.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from requests-cache-&gt;pydap==3.5.4.dev10+g13e1b07) (4.3.6)\nCollecting url-normalize&gt;=1.4 (from requests-cache-&gt;pydap==3.5.4.dev10+g13e1b07)\n  Using cached url_normalize-1.4.3-py2.py3-none-any.whl.metadata (3.1 kB)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.12/site-packages (from url-normalize&gt;=1.4-&gt;requests-cache-&gt;pydap==3.5.4.dev10+g13e1b07) (1.17.0)\nUsing cached requests_cache-1.2.1-py3-none-any.whl (61 kB)\nUsing cached cattrs-24.1.2-py3-none-any.whl (66 kB)\nUsing cached url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\nBuilding wheels for collected packages: pydap\n  Building wheel for pydap (pyproject.toml) ... done\n  Created wheel for pydap: filename=pydap-3.5.4.dev10+g13e1b07-py3-none-any.whl size=2396291 sha256=3813592ec729dbd3d51b5db271f1560e9596daf12b9a0059c16f42b77ed4c562\n  Stored in directory: /tmp/pip-ephem-wheel-cache-bbbd37h8/wheels/66/7b/1d/4148c4cadc026dfdaf05a6da1510038c2fe7ada0e5a4baf46b\nSuccessfully built pydap\nInstalling collected packages: url-normalize, cattrs, requests-cache, pydap\n  Attempting uninstall: pydap\n    Found existing installation: pydap 3.5.3\n    Uninstalling pydap-3.5.3:\n      Successfully uninstalled pydap-3.5.3\nSuccessfully installed cattrs-24.1.2 pydap-3.5.4.dev10+g13e1b07 requests-cache-1.2.1 url-normalize-1.4.3\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pydap\nprint(pydap.__version__)\n\n3.5.4.dev10+g13e1b07\n\n\n\nimport requests\nimport pydap.client\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\nmy_session=requests.Session()\n\n\npydap_ds = pydap.client.open_url(eula_url, session=my_session, protocol=\"dap4\")\ntest_ds = pydap_ds['lat'][:]\n\n\nprint(pydap.__version__)\n\n3.5.4.dev10+g13e1b07\n\n\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\npip install xarray\n\nCollecting xarray\n  Using cached xarray-2025.1.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy&gt;=1.24 in /opt/conda/lib/python3.12/site-packages (from xarray) (2.2.4)\nRequirement already satisfied: packaging&gt;=23.2 in /opt/conda/lib/python3.12/site-packages (from xarray) (24.2)\nCollecting pandas&gt;=2.1 (from xarray)\n  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas&gt;=2.1-&gt;xarray) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas&gt;=2.1-&gt;xarray) (2025.1)\nCollecting tzdata&gt;=2022.7 (from pandas&gt;=2.1-&gt;xarray)\n  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=2.1-&gt;xarray) (1.17.0)\nUsing cached xarray-2025.1.2-py3-none-any.whl (1.2 MB)\nUsing cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\nUsing cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\nInstalling collected packages: tzdata, pandas, xarray\nSuccessfully installed pandas-2.2.3 tzdata-2025.1 xarray-2025.1.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nprint(pydap.__version__)\n\n3.5.4.dev10+g13e1b07\n\n\n\nimport xarray as xr\n# very fast and this is 1Tb of data\nds = xr.open_mfdataset(eula_urls, engine=\"pydap\", \n                       combine=\"nested\", concat_dim=\"/time\", \n                       decode_cf=False, session=my_session)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (/time: 48, /lat: 361, /lon: 576)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/50)\n    U2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    V250      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPT     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPPB    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    T2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TQL       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    ...        ...\n    T2MWET    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    U500      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    QV10M     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    lat       (/time, /lat) float64 139kB dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;\n    lon       (/time, /lon) float64 221kB dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;\n    time      (/time) int32 192B dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;\nAttributes: (12/31)\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000\n    Unlimited_Dimension:               timexarray.DatasetDimensions:/time: 48/lat: 361/lon: 576Coordinates: (0)Data variables: (50)U2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_temperature_using_blended_TROPP_estimateunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPTdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPB(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_blended_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPBdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_liquid_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQLdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_500_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTOX(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_odd_oxygenunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOXdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PSdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nOMEGA500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :omega_at_500_hPaunits :Pa s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_250_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_250_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MDEW(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :dew_point_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEWdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPBLTOP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :pbltop_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOPdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDPRS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRSdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_500_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nDISPH(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :zero_plane_displacement_heightunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPHdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH1000(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_1000_mbunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTO3(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_ozoneunits :Dobsons_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_skin_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TSdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_thermal_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPTdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQI(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_ice_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQIdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nSLP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :sea_level_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLPdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_850_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nZLCL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :lifting_condensation_levelunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCLdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_water_vaporunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQVdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_250_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPQ(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_850_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_850_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDTMP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMPdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_EPV_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPVdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_500_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MWET(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :wet_bulb_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWETdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500dims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10Mdims :['time', 'lat', 'lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nlat(/time, /lat)float64dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latdims :['lat']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n135.38 kiB\n67.69 kiB\n\n\nShape\n(48, 361)\n(24, 361)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          361 48\n\n\n\n\nlon(/time, /lon)float64dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/londims :['lon']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n216.00 kiB\n108.00 kiB\n\n\nShape\n(48, 576)\n(24, 576)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          576 48\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;long_name :timeunits :minutes since 2016-06-01 00:30:00time_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timedims :['time']Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n192 B\n96 B\n\n\nShape\n(48,)\n(24,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          48 1\n\n\n\n\nIndexes: (0)Attributes: (31)History :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000Unlimited_Dimension :time\n\n\n\n#ds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel({\"/time\": 1}).load();\n\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\npip install dask\n\nCollecting dask\n  Downloading dask-2025.2.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting click&gt;=8.1 (from dask)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting cloudpickle&gt;=3.0.0 (from dask)\n  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting fsspec&gt;=2021.09.0 (from dask)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.12/site-packages (from dask) (24.2)\nCollecting partd&gt;=1.4.0 (from dask)\n  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pyyaml&gt;=5.3.1 in /opt/conda/lib/python3.12/site-packages (from dask) (6.0.2)\nCollecting toolz&gt;=0.10.0 (from dask)\n  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting locket (from partd&gt;=1.4.0-&gt;dask)\n  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\nDownloading dask-2025.2.0-py3-none-any.whl (1.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 30.6 MB/s eta 0:00:00\nDownloading click-8.1.8-py3-none-any.whl (98 kB)\nDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\nDownloading partd-1.4.2-py3-none-any.whl (18 kB)\nDownloading toolz-1.0.0-py3-none-any.whl (56 kB)\nDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nInstalling collected packages: toolz, locket, fsspec, cloudpickle, click, partd, dask\nSuccessfully installed click-8.1.8 cloudpickle-3.1.1 dask-2025.2.0 fsspec-2025.3.0 locket-1.0.0 partd-1.4.2 toolz-1.0.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport dask\n\n\n# Check if dask is installed\nprint(\"Dask version:\", dask.__version__)\n\n# Check if xarray detects dask\nprint(\"Available backends:\", xr.backends.list_engines())\n\n\npip install netcdf4\n\nCollecting netcdf4\n  Downloading netCDF4-1.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting cftime (from netcdf4)\n  Downloading cftime-1.6.4.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from netcdf4) (2025.1.31)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from netcdf4) (2.2.4)\nDownloading netCDF4-1.7.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 86.1 MB/s eta 0:00:00\nDownloading cftime-1.6.4.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 68.7 MB/s eta 0:00:00\nInstalling collected packages: cftime, netcdf4\nSuccessfully installed cftime-1.6.4.post1 netcdf4-1.7.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nprint(my_session)\n\n&lt;requests.sessions.Session object at 0x7f45e4175b50&gt;\n\n\n\nimport xarray as xr\nimport requests\nmy_session=requests.Session()\nurl=\"dap4://opendap.cr.usgs.gov/opendap/hyrax/MOD13Q1.061/h09v06.ncml\"\nds = xr.open_dataset(url, engine=\"pydap\", session=my_session)\nds[\"_250m_16_days_VI_Quality\"].isel({\"/time\": 1, \"/YDim\": 1}).load()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray '_250m_16_days_VI_Quality' (/XDim: 4800)&gt; Size: 19kB\narray([2116., 2116., 2116., ..., 2116., 2116., 2116.], dtype=float32)\nCoordinates:\n    Latitude   (/XDim) float64 38kB 30.0 30.0 30.0 30.0 ... 30.0 30.0 30.0 30.0\n    Longitude  (/XDim) float64 38kB -103.9 -103.9 -103.9 ... -92.38 -92.37\nDimensions without coordinates: /XDim\nAttributes:\n    long_name:     250m 16 days VI Quality\n    units:         bit field\n    valid_range:   [0, 65534]\n    Legend:        \\n\\t Bit Fields Description (Right to Left): \\n\\t[0-1] : M...\n    grid_mapping:  MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projection\n    dims:          ['time', 'YDim', 'XDim']\n    Maps:          ()xarray.DataArray'_250m_16_days_VI_Quality'/XDim: 48002.116e+03 2.116e+03 2.116e+03 ... 2.116e+03 2.116e+03 2.116e+03array([2116., 2116., 2116., ..., 2116., 2116., 2116.], dtype=float32)Coordinates: (2)Latitude(/XDim)float6430.0 30.0 30.0 ... 30.0 30.0 30.0long_name :Latitudeunits :degrees_northdims :['YDim', 'XDim']Maps :()array([29.996875, 29.996875, 29.996875, ..., 29.996875, 29.996875,\n       29.996875])Longitude(/XDim)float64-103.9 -103.9 ... -92.38 -92.37long_name :Longitudeunits :degrees_eastdims :['YDim', 'XDim']Maps :()array([-103.91857343, -103.91616788, -103.91376233, ...,  -92.37914828,\n        -92.37674273,  -92.37433718])Indexes: (0)Attributes: (7)long_name :250m 16 days VI Qualityunits :bit fieldvalid_range :[0, 65534]Legend :\n     Bit Fields Description (Right to Left): \n    [0-1] : MODLAND_QA [2 bit range] \n         00: VI produced, good quality \n         01: VI produced, but check other QA \n         10: Pixel produced, but most probably cloudy \n         11: Pixel not produced due to other reasons than clouds \n    [2-5] : VI usefulness [4 bit range] \n         0000: Highest quality \n         0001: Lower quality  \n         0010..1010: Decreasing quality \n         1100: Lowest quality \n         1101: Quality so low that it is not useful \n         1110: L1B data faulty \n         1111: Not useful for any other reason/not processed \n    [6-7] : Aerosol quantity [2 bit range] \n         00: Climatology \n         01: Low \n         10: Average \n         11: High (11) \n    [8] : Adjacent cloud detected; [1 bit range] \n         1: Yes \n         0: No \n    [9] : Atmosphere BRDF correction performed [1 bit range] \n         1: Yes \n         0: No \n    [10] : Mixed clouds  [1 bit range] \n         1: Yes \n         0: No \n    [11-13] : Land/Water Flag [3 bit range] \n         000: Shallow ocean \n         001: Land (Nothing else but land) \n         010: Ocean coastlines and lake shorelines \n         011: Shallow inland water \n         100: Ephemeral water \n         101: Deep inland water \n         110: Moderate or continental ocean \n         111: Deep ocean \n    [14] : Possible snow/ice [1 bit range] \n         1: Yes \n         0: No \n    [15] : Possible shadow [1 bit range] \n         1: Yes \n         0: No \ngrid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectiondims :['time', 'YDim', 'XDim']Maps :()"
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#overview",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#overview",
    "title": "PyDAP Authentication",
    "section": "Overview",
    "text": "Overview\nSee the 3-nasa.ipynb notebook for the recommended way to authenticate until pydap &gt; 3.5.3 (Jan 2025) is released. This notebook is included to document PyDAP authentication. When the new version is released, this authentication works similar to token-based (meaning works for all cases tested).\nimport requests\nmy_session=requests.Session()\nPyDAP will automatically look for ~/.netrc file and use that so it may seem that you are not authenticating (if you have that file). But you are. The “silent” method fails if the data require a User Agreement (EULA) that you have to accept on your Earthdata profile because it cannot handle redirects. You can set up a session with username/password which will work for single files with xarray.open_dataset but fails with xarray.open_mfdataset. Setting up a session with a token works in all cases, but is more of a hassle. earthaccess will authenticate from your ~/.netrc file and handle the token for you. So I recommend that approach.\nI am going to first show how to set up a token and create a “session” that is passed to xarray. For what I have found (in March 2025), this works to get past “re-direct” errors that arise when the data require a EULA. Tokens also get past the redirect errors from other non-NASA OPeNDAP servers that use NASA EDL (USDA and NSIDC). At the end, I will show how xarray+pydap can automatically use your .netrc file to authenticate, but this won’t work for data that need a EULA so it might be best to use the token method as that will work for data that need a EULA too.\n\nReferences\n\nhttps://podaac.jpl.nasa.gov/OPeNDAP-in-the-Cloud\n\n\n\nPrerequisites\nSet up a NASA EDL token. Go to https://urs.earthdata.nasa.gov/profile and click on Generate Token. Copy your token (it is exceedingly long) and paste into this code. Run this in a terminal windown (not notebook). Alternatively, open ~/.env and paste in NASA_EDL_TOKEN='your_edl_token_here'. Note the token only last a month, so you will need to refresh your tokens regularly.\necho \"NASA_EDL_TOKEN='your_edl_token_here'\" &gt;&gt; ~/.env\nNow we can load the token into any Jupyter notebook with\nfrom dotenv import load_dotenv\nload_dotenv()\nedl_token = os.getenv(\"NASA_EDL_TOKEN\")\nFor the section, where I cover authentication with username and password, I assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password. Create that file if needed.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages\n\nimport xarray as xr\nimport pydap.client",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#three-ways-to-authenticate-with-pydap",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#three-ways-to-authenticate-with-pydap",
    "title": "PyDAP Authentication",
    "section": "Three ways to authenticate with pydap",
    "text": "Three ways to authenticate with pydap\n\nLet pydap find your ~/.netrc file\nSet up a session with your Earthdata username and password\nSet up a session with a Earthdata token\n\nI suggest you only use the latter since that works for all cases: data w or wo a EULA, singles files and multiple files, xarray open_dataset and open_mfdataset.\n\nLet pydap use your .netrc file\nIf the data need Earthdata authentication but don’t need a EULA, then you can let pydap use your .netrc file silently. But this method is the most likely to produce redirect errors and access errors.\nThe following code works because pydap uses the .netrc file that is in ~/.netrc. It does this silently for you. But this only works because this particular dataset does not require a User Agreement to be accepted on your Earthdata profile.\n\n# doesn't need a eula\nurl=\"dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\nds = xr.open_dataset(url, engine=\"pydap\", decode_timedelta=False)\nds[\"wind_speed\"].load()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'wind_speed' (/time: 1, /lat: 2400, /lon: 2400)&gt; Size: 46MB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ...,  8.,  8.,  8.],\n        [nan, nan, nan, ...,  8.,  8., nan]]])\nDimensions without coordinates: /time, /lat, /lon\nAttributes:\n    long_name:      10m wind speed\n    standard_name:  wind_speed\n    units:          m s-1\n    height:         10 m\n    valid_min:      0\n    valid_max:      127\n    time_offset:    0.0\n    source:         WSP-ECMWF-Forecast\n    comment:        These wind speeds were created by the ECMWF and represent...\n    Maps:           ()xarray.DataArray'wind_speed'/time: 1/lat: 2400/lon: 2400nan nan nan nan nan nan nan nan ... 8.0 8.0 8.0 8.0 8.0 8.0 8.0 nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ...,  8.,  8.,  8.],\n        [nan, nan, nan, ...,  8.,  8., nan]]])Coordinates: (0)Indexes: (0)Attributes: (10)long_name :10m wind speedstandard_name :wind_speedunits :m s-1height :10 mvalid_min :0valid_max :127time_offset :0.0source :WSP-ECMWF-Forecastcomment :These wind speeds were created by the ECMWF and represent winds at 10 metres above the sea surface. Maps :()\n\n\n\n\nSet up a session with username and password\nUsing the session argument with pydap is more robust because it will open data that require a EULA. You can set up a session like this with your username and password in your .netrc file. This mostly works but it fails when using xarray.open_mfdataset() with data that require a EULA. So using a token is better though more of a hassle.\n\nimport netrc\n# Get credentials from .netrc\nauth_host = \"urs.earthdata.nasa.gov\"\ntry:\n    credentials = netrc.netrc().authenticators(auth_host)\n    if credentials:\n        username, _, password = credentials\n    else:\n        raise ValueError(\"No credentials found in .netrc!\")\nexcept FileNotFoundError:\n    raise FileNotFoundError(\"Could not find ~/.netrc. Ensure it exists and is configured correctly.\")\n\n\nimport pydap\nfrom pydap.client import open_url\nfrom pydap.cas.urs import setup_session\npw_session = setup_session(username, password)\n\n\n\nSet up a session with a EDL token (pydap = 3.5.3)\nThis is the robust way to set up an authenticated session. Note see the notebook where I authenticate with earthaccess, which makes this easier and you don’t need to save anything to .env.\nLoad the token. Make sure you have your token set up in your ~/.env file. See prerequisites.\n\nfrom dotenv import load_dotenv\nimport os\nload_dotenv()\nedl_token = os.getenv(\"NASA_EDL_TOKEN\")\n\nNext we set up a NASA EDL session with this token.\n\nimport requests\nauth_hdr=\"Bearer \" + edl_token\ntoken_session = requests.Session()\ntoken_session.headers={\"Authorization\": auth_hdr}\n\n\n\nIf pydap version &gt; 3.5.3\nIf you have pydap version &gt; 3.5.3, then you do not need to generate your own token. You can use your .netrc file (with EDL username and password) and use this:\n\n# uncomment to use\n#import requests\n#token_session=requests.Session()\n\nFor the rest of the notebook, the token_session will be used.",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#load-non-eula-data",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#load-non-eula-data",
    "title": "PyDAP Authentication",
    "section": "Load non-EULA data",
    "text": "Load non-EULA data\n\n# doesn't point to specific file but we will spec protocol\nurl=\"https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\n\n# this is another test url that works for this example\n# url=\"https://opendap.earthdata.nasa.gov/hyrax/data/nc/fnoc1.nc\"\n\n\n%%time\n#from pydap.client import open_url\npydap_ds = pydap.client.open_url(url, protocol=\"dap4\", session=token_session)\n\nCPU times: user 13 ms, sys: 11.9 ms, total: 24.9 ms\nWall time: 744 ms\n\n\n\n%%time\n# getting the data does work\ntest_ds = pydap_ds['lat'][:]\n\nCPU times: user 18 ms, sys: 628 µs, total: 18.6 ms\nWall time: 2.67 s\n\n\n\nds = xr.open_dataset(\n    url.replace(\"https\", \"dap4\", 1), engine=\"pydap\", \n    decode_cf=False, session=token_session)\n\n\nds[\"wind_speed\"].plot();\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/utils.py:260: RuntimeWarning: overflow encountered in scalar absolute\n  vlim = max(abs(vmin - center), abs(vmax - center))\n\n\n\n\n\n\n\n\n\n\n%%time\n# This is doing the same thing as engine=\"pydap\" I think but I don't have to know the dap4 replacement\n# ds created is the same though\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store, decode_cf=False)\n\nCPU times: user 2.11 ms, sys: 0 ns, total: 2.11 ms\nWall time: 2.11 ms",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#data-with-a-eula",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#data-with-a-eula",
    "title": "PyDAP Authentication",
    "section": "Data with a EULA",
    "text": "Data with a EULA\nLetting pydap silently use your .netrc fails (redirect error) when there is a EULA and you have accepted the EULA. You need to set up a session with username and password or with a token.\n\nPrerequisites\nMake sure you have the GESDISC EULA accepted.\n\nLog into https://urs.earthdata.nasa.gov\nThen go here https://urs.earthdata.nasa.gov/profile\nThen click EULAs\nGo to unaccepted EULAs and make sure that GESDISC is accepted\n\n\n# the GESDISC data requires a EULA\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\n\n\n%%time\nimport pydap\nfrom pydap.client import open_url\npydap_ds = open_url(eula_url, session=token_session, protocol=\"dap4\")\n\nCPU times: user 41.3 ms, sys: 3.79 ms, total: 45.1 ms\nWall time: 1.58 s\n\n\n\n# works when we try to get data\ntest_ds = pydap_ds['lat'][:]\n\n\n%%time\n# this works but is a lot of syntax to remember\nds = xr.open_dataset(eula_url.replace(\"https\", \"dap4\", 1), \n                     engine=\"pydap\", decode_cf=False, \n                     session=token_session)\n\nCPU times: user 42.4 ms, sys: 3.77 ms, total: 46.2 ms\nWall time: 1.08 s\n\n\n\n# yeah! I can get the data\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).plot();\n\n\n\n\n\n\n\n\n\n%%time\nimport xarray as xr\n# This works too but is so much slower\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)\n\nCPU times: user 35.4 ms, sys: 11.5 ms, total: 46.9 ms\nWall time: 2.47 s\n\n\n\n# I have to rename variables\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#creating-data-cubes",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#creating-data-cubes",
    "title": "PyDAP Authentication",
    "section": "Creating data cubes",
    "text": "Creating data cubes\nOur goal is not individual files rather data cubes from multiple files. We will use xarray.open_mfdataset but we need to make some tweaks.\n\nWe need to use dap4 instead of https. I don’t know what to do if that doesn’t work.\nWe need to concatenate using \\time not time since the dim name has that slash in it.\n\n\nExample 1 Non-EULA data\n\n# doesn't point to specific file but we will spec protocol\nurl1=\"dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0\"\nurl2=\"dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220813010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220813_010000-v02.0-fv01.0\"\nurls = [url1, url2]\nurls\n\n['dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0',\n 'dap4://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220813010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220813_010000-v02.0-fv01.0']\n\n\n\nds = xr.open_mfdataset(urls, engine=\"pydap\", \n                       combine=\"nested\", \n                       concat_dim=\"/time\", decode_cf=False,\n                      session=token_session)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 288MB\nDimensions:                    (/time: 2, /lat: 2400, /lon: 2400)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/19)\n    wind_speed                 (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    lon                        (/time, /lon) float32 19kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    or_longitude               (/time, /lat, /lon) int16 23MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    time                       (/time) int32 8B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    sses_standard_deviation    (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sst_dtime                  (/time, /lat, /lon) int32 46MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    ...                         ...\n    dt_analysis                (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    satellite_zenith_angle     (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    lat                        (/time, /lat) float32 19kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    adi_dtime_from_sst         (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    or_latitude                (/time, /lat, /lon) int16 23MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sses_bias                  (/time, /lat, /lon) int8 12MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\nAttributes: (12/53)\n    Conventions:                CF-1.4\n    title:                      Sea Surface Temperature\n    summary:                    The L3C product derived from GOES16/ABI brigh...\n    references:                 Geostationary Sea Surface Temperature Product...\n    institution:                OSISAF\n    comment:                    None\n    ...                         ...\n    netcdf_version_id:          4.6.3\n    build_dmrpp:                3.20.9-91\n    bes:                        3.20.9-91\n    libdap:                     libdap-3.20.8-41\n    configuration:              \\n# TheBESKeys::get_as_config()\\nAllowedHosts...\n    invocation:                 build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648...xarray.DatasetDimensions:/time: 2/lat: 2400/lon: 2400Coordinates: (0)Data variables: (19)wind_speed(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :10m wind speedstandard_name :wind_speedunits :m s-1height :10 madd_offset :0.0scale_factor :1.0valid_min :0valid_max :127time_offset :0.0source :WSP-ECMWF-Forecastcomment :These wind speeds were created by the ECMWF and represent winds at 10 metres above the sea surface. Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nlon(/time, /lon)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.75 kiB\n9.38 kiB\n\n\nShape\n(2, 2400)\n(1, 2400)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n          2400 2\n\n\n\n\nor_longitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original longitude of the SST valuestandard_name :longitudeunits :degrees_eastadd_offset :0.0scale_factor :0.01valid_min :-18000valid_max :18000comment :Original longitude of the SST valueMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :reference time of sst filestandard_name :timeaxis :Tunits :seconds since 1981-01-01 00:00:00Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n8 B\n4 B\n\n\nShape\n(2,)\n(1,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          2 1\n\n\n\n\nsses_standard_deviation(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES standard deviationunits :kelvinadd_offset :1.0scale_factor :0.01valid_min :-127valid_max :127comment :Standard deviation estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsst_dtime(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-2147483648long_name :time difference from reference timeunits :secondsadd_offset :0.0scale_factor :1.0valid_min :-2147483647valid_max :2147483647comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n43.95 MiB\n21.97 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsolar_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :solar zenith angleunits :angular_degreeadd_offset :90.0scale_factor :1.0valid_min :-90valid_max :90comment :The solar zenith angle at the time of the SST observations.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsea_ice_fraction(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sea ice fractionstandard_name :sea_ice_area_fractionunits :Noneadd_offset :0.0scale_factor :0.01valid_min :0valid_max :100time_offset :0.0source :ICE-OSISAFcomment :Fractional sea ice cover from OSISAF ice productMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nl2p_flags(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;long_name :L2P flagsvalid_min :0valid_max :15flag_meanings :microwave land ice lakeflag_masks :[1, 2, 4, 8]comment :These flags are important to properly use the data.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n43.95 MiB\n21.97 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsources_of_adi(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sources of aerosol dynamic indicatorvalid_min :0valid_max :2flag_meanings :no_data AOD-NAAPS-ADI SDI-OSISAF-ADIflag_values :[0, 1, 2]comment :This variable provides a pixel by pixel description of where aerosol optical depth were derived from.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\naerosol_dynamic_indicator(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :aerosol dynamic indicatorunits :Noneadd_offset :0.0scale_factor :0.1valid_min :0valid_max :127source :sources_of_adicomment :NoneMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsea_surface_temperature(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :sea surface subskin temperaturestandard_name :sea_surface_subskin_temperatureunits :kelvinadd_offset :273.15scale_factor :0.01valid_min :-300valid_max :4500depth :1 millimetersource :GOES_Imagercomment :Temperature of the subskin of the oceanMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nquality_level(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :quality level of SST pixelvalid_min :0valid_max :5flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualityflag_values :[0, 1, 2, 3, 4, 5]comment :These are the overall quality indicators and are used for all GHRSST SSTsMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\ndt_analysis(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :deviation from SST analysis or reference climatologyunits :kelvinadd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127reference :OSTIAcomment :The difference between this SST and the previous day's SST analysisMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsatellite_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :satellite zenith angleunits :angular_degreeadd_offset :0.0scale_factor :1.0valid_min :-90valid_max :90comment :The satellite zenith angle at the time of the SST observations.Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nlat(/time, /lat)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.75 kiB\n9.38 kiB\n\n\nShape\n(2, 2400)\n(1, 2400)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n          2400 2\n\n\n\n\nadi_dtime_from_sst(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :time difference of ADI data from sst measurementunits :houradd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127comment :Difference in hours between the ADI and SST dataMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nor_latitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original latitude of the SST valuestandard_name :latitudeunits :degrees_northadd_offset :0.0scale_factor :0.01valid_min :-9000valid_max :9000comment :Original latitude of the SST valueMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.97 MiB\n10.99 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nsses_bias(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES bias estimateunits :kelvinadd_offset :0.0scale_factor :0.01valid_min :-127valid_max :127comment :Bias estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n10.99 MiB\n5.49 MiB\n\n\nShape\n(2, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                           2400 2400 2\n\n\n\n\nIndexes: (0)Attributes: (53)Conventions :CF-1.4title :Sea Surface Temperaturesummary :The L3C product derived from GOES16/ABI brightness temperatures.references :Geostationary Sea Surface Temperature Product User Manual, http://www.osi-saf.orginstitution :OSISAFcomment :Nonelicense :All intellectual property rights of the Ocean & Sea Ice SAF products belong to EUMETSAT. The use of these products is granted to every user, free of charge. If users wish to use these products, EUMETSAT's copyright credit must be shown by displaying the words 'Copyright EUMETSAT' under each of the products shown. EUMETSAT offers no warranty and accepts no liability in respect of the Ocean & Sea Ice SAF products. EUMETSAT neither commits to nor guarantees the continuity, availability, or quality or suitability for any purpose of, the Ocean & Sea Ice SAF products.id :GOES16-OSISAF-L3C-v1.0product_id :OSI-207-bnaming_authority :org.ghrsstproduct_version :1.0gds_version_id :2.0file_quality_level :3spatial_resolution :0.05 degreenorthernmost_latitude :60.0southernmost_latitude :-60.0easternmost_longitude :-15.0westernmost_longitude :-135.0source :GOES_ABIplatform :GOES16sensor :GOES_ABIMetadata_Conventions :Unidata Dataset Discovery v1.0metadata_link :N/Akeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature keywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.05geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.05acknowledgment :In case SAF data (pre-operational or operational) has been used for the study described in a paper the following sentence would be an appropriate reference to the funding coming from EUMETSAT: The data from the EUMETSAT Satellite Application Facility on Ocean & Sea Ice  used in this study are accessible through the SAF's homepage http://www.osi-saf.orgcreator_name :O&SI SAFcreator_email :osi-saf.helpdesk@meteo.frcreator_url :http://www.osi-saf.orgproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L3Ccdm_data_type :gridhistory :METEO-FRANCE GEOSAFO v1.1.8uuid :DF556788-19E1-11ED-A08A-48DF370DAD10date_created :20220812T015542Zstart_time :20220812T004042Ztime_coverage_start :20220812T004042Zstop_time :20220812T011929Ztime_coverage_end :20220812T011929Znetcdf_version_id :4.6.3build_dmrpp :3.20.9-91bes :3.20.9-91libdap :libdap-3.20.8-41configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.Exclude=^\\..*;\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.Include=;\nBES.Catalog.catalog.RootDirectory=/tmp/tmph648lz17/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Container.Persistence=strict\nBES.Data.RootDirectory=/dev/null\nBES.DefaultResponseMethod=POST\nBES.FollowSymLinks=Yes\nBES.Group=group_name\nBES.Info.Buffered=no\nBES.Info.Type=xml\nBES.LogName=./bes.log\nBES.LogVerbose=no\nBES.Memory.GlobalArea.ControlHeap=no\nBES.Memory.GlobalArea.EmergencyPoolSize=1\nBES.Memory.GlobalArea.MaximumHeapSize=20\nBES.Memory.GlobalArea.Verbose=no\nBES.ProcessManagerMethod=multiple\nBES.ServerAdministrator=admin.email.address@your.domain.name\nBES.Uncompress.NumTries=10\nBES.Uncompress.Retry=2000\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.User=user_name\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.Cache.latlon.path=/tmp/latlon\nH5.Cache.latlon.prefix=l\nH5.Cache.latlon.size=20000\nH5.CheckIgnoreObj=false\nH5.DefaultHandleDimension=true\nH5.DisableStructMetaAttr=true\nH5.DiskCacheComp=true\nH5.DiskCacheCompThreshold=2.0\nH5.DiskCacheCompVarSize=10000\nH5.DiskCacheDataPath=/tmp\nH5.DiskCacheFilePrefix=c\nH5.DiskCacheFloatOnlyComp=true\nH5.DiskCacheSize=10000\nH5.DiskMetaDataCachePath=/tmp\nH5.EnableAddPathAttrs=true\nH5.EnableCF=false\nH5.EnableCFDMR=true\nH5.EnableCheckNameClashing=true\nH5.EnableDiskDDSCache=false\nH5.EnableDiskDataCache=false\nH5.EnableDiskMetaDataCache=false\nH5.EnableDropLongString=true\nH5.EnableEOSGeoCacheFile=false\nH5.EnableFillValueCheck=true\nH5.KeepVarLeadingUnderscore=false\nH5.LargeDataMemCacheEntries=0\nH5.MetaDataMemCacheEntries=300\nH5.SmallDataMemCacheEntries=0\ninvocation :build_dmrpp -c /tmp/conf_GGue -f /tmp/tmph648lz17//20220812010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220812_010000-v02.0-fv01.0.nc -r /tmp/dmr_HJF1 -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\nLet’s do some slicing and see how fast we can get the data.\n\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\n\n\n%%time\ntest = ds[\"wind_speed\"].isel(time=1).load()\n\nCPU times: user 32.1 ms, sys: 36 ms, total: 68.1 ms\nWall time: 6.08 s\n\n\n\n\nExample 2 EULA data\ntoken_session is critical so that things don’t go south with re-directs.\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\n%%time\nimport xarray as xr\n# very fast and this is 1Tb of data\nds = xr.open_mfdataset(eula_urls, engine=\"pydap\", \n                       combine=\"nested\", concat_dim=\"/time\", \n                       decode_cf=False, session=token_session)\nds\n\nCPU times: user 141 ms, sys: 12 ms, total: 153 ms\nWall time: 2.24 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (/time: 48, /lat: 361, /lon: 576)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/50)\n    U2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    V250      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPT     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPPB    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    T2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TQL       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    ...        ...\n    T2MWET    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    U500      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    QV10M     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    lat       (/time, /lat) float64 139kB dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;\n    lon       (/time, /lon) float64 221kB dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;\n    time      (/time) int32 192B dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;\nAttributes: (12/31)\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000\n    Unlimited_Dimension:               timexarray.DatasetDimensions:/time: 48/lat: 361/lon: 576Coordinates: (0)Data variables: (50)U2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_temperature_using_blended_TROPP_estimateunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPB(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_blended_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPBMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_liquid_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_500_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTOX(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_odd_oxygenunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOXMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nOMEGA500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :omega_at_500_hPaunits :Pa s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_250_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_250_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MDEW(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :dew_point_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEWMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPBLTOP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :pbltop_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDPRS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_500_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nDISPH(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :zero_plane_displacement_heightunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPHMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH1000(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_1000_mbunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTO3(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_ozoneunits :Dobsons_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_skin_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_thermal_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQI(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_ice_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQIMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nSLP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :sea_level_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_850_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nZLCL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :lifting_condensation_levelunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_water_vaporunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_250_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPQ(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_850_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_850_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDTMP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_EPV_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_500_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MWET(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :wet_bulb_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWETMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nlat(/time, /lat)float64dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n135.38 kiB\n67.69 kiB\n\n\nShape\n(48, 361)\n(24, 361)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          361 48\n\n\n\n\nlon(/time, /lon)float64dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/lonMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n216.00 kiB\n108.00 kiB\n\n\nShape\n(48, 576)\n(24, 576)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          576 48\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;long_name :timeunits :minutes since 2016-06-01 00:30:00time_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n192 B\n96 B\n\n\nShape\n(48,)\n(24,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          48 1\n\n\n\n\nIndexes: (0)Attributes: (31)History :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000Unlimited_Dimension :time\n\n\n\nprint(f\"Dataset size: {ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 1876.77 MB\n\n\n\n# We can load the data\nds = ds.rename({\"/time\": \"time\", \"/lat\": \"lat\", \"/lon\": \"lon\"})\nds[\"T2M\"].isel(time=1).load()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'T2M' (lat: 361, lon: 576)&gt; Size: 832kB\narray([[218.05759, 218.05759, 218.05759, ..., 218.05759, 218.05759,\n        218.05759],\n       [217.35446, 217.35446, 217.33884, ..., 217.38571, 217.38571,\n        217.37009],\n       [217.10446, 217.10446, 217.10446, ..., 217.10446, 217.10446,\n        217.10446],\n       ...,\n       [266.9951 , 266.98727, 266.97946, ..., 267.01462, 267.0068 ,\n        266.999  ],\n       [268.10446, 268.10446, 268.10446, ..., 268.10056, 268.10056,\n        268.10446],\n       [269.124  , 269.124  , 269.124  , ..., 269.124  , 269.124  ,\n        269.124  ]], dtype=float32)\nDimensions without coordinates: lat, lon\nAttributes: (12/14)\n    long_name:       2-meter_air_temperature\n    units:           K\n    _FillValue:      999999987000000.0\n    missing_value:   999999987000000.0\n    fmissing_value:  999999987000000.0\n    scale_factor:    1.0\n    ...              ...\n    vmax:            999999987000000.0\n    vmin:            -999999987000000.0\n    valid_range:     [-999999987000000.0, 999999987000000.0]\n    origname:        T2M\n    fullnamepath:    /T2M\n    Maps:            ()xarray.DataArray'T2M'lat: 361lon: 576218.1 218.1 218.1 218.1 218.1 218.1 ... 269.1 269.1 269.1 269.1 269.1array([[218.05759, 218.05759, 218.05759, ..., 218.05759, 218.05759,\n        218.05759],\n       [217.35446, 217.35446, 217.33884, ..., 217.38571, 217.38571,\n        217.37009],\n       [217.10446, 217.10446, 217.10446, ..., 217.10446, 217.10446,\n        217.10446],\n       ...,\n       [266.9951 , 266.98727, 266.97946, ..., 267.01462, 267.0068 ,\n        266.999  ],\n       [268.10446, 268.10446, 268.10446, ..., 268.10056, 268.10056,\n        268.10446],\n       [269.124  , 269.124  , 269.124  , ..., 269.124  , 269.124  ,\n        269.124  ]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (14)long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\nNote, if you tried to use the username/password for creating the session instead of a token, you would get the redirect error.\n\n\nExample 3 from PyDap documentation\nIn this example, constraint expression is used just to get certain variables. See full notebook here.\n\nbaseURL = 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/'\nTemp_Salt = \"ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_\"\nyear = '2017-'\nmonth = '01'\nend_ = '_ECCO_V4r4_native_llc0090'\nCE = '?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time'\n\nTemp_2017 = [baseURL + Temp_Salt + year + f'{i:02}' + end_ + CE for i in range(1, 4)]\nTemp_2017\n\n['dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-02_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-03_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time']\n\n\nCreate data cube with open_mfdataset but not concat dim is /time not time. This takes a really long time, but if we didn’t do the constraint expression part, it would take much longer. So it is good to do that step.\nNote token_session is not required since these data don’t need a EULA, but good practice to use.\n\n%%time\n# 13 seconds to assemble the data cube for a 126Mb dataset...slow\ntheta_salt_ds = xr.open_mfdataset(\n    Temp_2017, \n    engine='pydap',\n    parallel=True, \n    combine='nested', \n    concat_dim='/time',\n    session=token_session\n)\ntheta_salt_ds\n\nCPU times: user 184 ms, sys: 12.7 ms, total: 196 ms\nWall time: 13 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 126MB\nDimensions:  (/time: 3, /k: 50, /tile: 13, /j: 90, /i: 90)\nCoordinates:\n    time     (/time) datetime64[ns] 24B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nDimensions without coordinates: /time, /k, /tile, /j, /i\nData variables:\n    SALT     (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    THETA    (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    i        (/time, /i) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    j        (/time, /j) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    k        (/time, /k) int32 600B dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;\n    tile     (/time, /tile) int32 156B dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;\nAttributes: (12/62)\n    acknowledgement:                 This research was carried out by the Jet...\n    author:                          Ian Fenty and Ou Wang\n    cdm_data_type:                   Grid\n    comment:                         Fields provided on the curvilinear lat-l...\n    Conventions:                     CF-1.8, ACDD-1.3\n    coordinates_comment:             Note: the global 'coordinates' attribute...\n    ...                              ...\n    time_coverage_duration:          P1M\n    time_coverage_end:               2017-02-01T00:00:00\n    time_coverage_resolution:        P1M\n    time_coverage_start:             2017-01-01T00:00:00\n    title:                           ECCO Ocean Temperature and Salinity - Mo...\n    uuid:                            f5b7028c-4181-11eb-b7e6-0cc47a3f47b1xarray.DatasetDimensions:/time: 3/k: 50/tile: 13/j: 90/i: 90Coordinates: (1)time(/time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :center time of averaging periodaxis :Tbounds :time_bndscoverage_content_type :coordinatestandard_name :timeorigname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n24 B\n8 B\n\n\nShape\n(3,)\n(1,)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n           3 1\n\n\n\n\nData variables: (6)SALT(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Salinityunits :1e-3coverage_content_type :modelResultstandard_name :sea_water_salinitycomment :Defined using CF convention 'Sea water salinity is the salt content of sea water, often on the Practical Salinity Scale of 1978. However, the unqualified term 'salinity' is generic and does not necessarily imply any particular method of calculation. The units of salinity are dimensionless and the units attribute should normally be given as 1e-3 or 0.001 i.e. parts per thousand.' see https://cfconventions.org/Data/cf-standard-names/73/build/cf-standard-name-table.htmlvalid_min :17.106637954711914valid_max :41.26802444458008origname :SALTfullnamepath :/SALTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\nTHETA(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Potential temperature units :degree_Ccoverage_content_type :modelResultstandard_name :sea_water_potential_temperaturecomment :Sea water potential temperature is the temperature a parcel of sea water would have if moved adiabatically to sea level pressure. Note: the equation of state is a modified UNESCO formula by Jackett and McDougall (1995), which uses the model variable potential temperature as input assuming a horizontally and temporally constant pressure of $p_0=-g ho_{0} z$.valid_min :-2.2909388542175293valid_max :36.032955169677734origname :THETAfullnamepath :/THETAMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\ni(/time, /i)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Xlong_name :grid index in x for variables at tracer and 'v' locationsswap_dim :XCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'v' variables (e.g., VVEL) have the same x coordinate on the model grid.coverage_content_type :coordinateorigname :ifullnamepath :/iMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nj(/time, /j)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Ylong_name :grid index in y for variables at tracer and 'u' locationsswap_dim :YCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'u' variables (e.g., UVEL) have the same y coordinate on the model grid.coverage_content_type :coordinateorigname :jfullnamepath :/jMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nk(/time, /k)int32dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;axis :Zlong_name :grid index in z for tracer variablesswap_dim :Zcoverage_content_type :coordinateorigname :kfullnamepath :/kMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n600 B\n200 B\n\n\nShape\n(3, 50)\n(1, 50)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           50 3\n\n\n\n\ntile(/time, /tile)int32dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;long_name :lat-lon-cap tile indexcomment :The ECCO V4 horizontal model grid is divided into 13 tiles of 90x90 cells for convenience.coverage_content_type :coordinateorigname :tilefullnamepath :/tileMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n156 B\n52 B\n\n\nShape\n(3, 13)\n(1, 13)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           13 3\n\n\n\n\nIndexes: (0)Attributes: (62)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on the curvilinear lat-lon-cap 90 (llc90) native grid used in the ECCO model.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T14:39:59date_issued :2020-12-18T14:39:59date_metadata_modified :2021-03-15T21:56:21date_modified :2021-03-15T21:56:21geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :variablegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :variablegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :-6134.5geospatial_vertical_positive :upgeospatial_vertical_resolution :variablegeospatial_vertical_units :meterhistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECL5M-OTS44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE &gt; OCEANS &gt; SALINITY/DENSITY &gt; SALINITY, EARTH SCIENCE SERVICES &gt; MODELS &gt; EARTH SCIENCE REANALYSES/ASSIMILATION MODELS, EARTH SCIENCE &gt; OCEANS &gt; OCEAN TEMPERATURE &gt; POTENTIAL TEMPERATUREkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_TEMP_SALINITY_LLC0090GRID_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged ocean potential temperature and salinity on the lat-lon-cap 90 (llc90) native model grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2017-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2017-01-01T00:00:00title :ECCO Ocean Temperature and Salinity - Monthly Mean llc90 Grid (Version 4 Release 4)uuid :f5b7028c-4181-11eb-b7e6-0cc47a3f47b1\n\n\n\nprint(f\"Dataset size: {theta_salt_ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 126.36 MB\n\n\n\ntheta_salt_ds = theta_salt_ds.rename({\"/time\": \"time\", \"/j\": \"j\", \"/i\": \"i\", \"/tile\": \"tile\", \"/k\": \"k\"})\n\n/tmp/ipykernel_1511/704975482.py:1: UserWarning: rename '/time' to 'time' does not create an index anymore. Try using swap_dims instead or use set_index after rename to create an indexed coordinate.\n  theta_salt_ds = theta_salt_ds.rename({\"/time\": \"time\", \"/j\": \"j\", \"/i\": \"i\", \"/tile\": \"tile\", \"/k\": \"k\"})\n\n\n\ntheta_salt_ds[\"THETA\"].isel(time=1, tile=10, k=1).plot()",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#conclusion",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#conclusion",
    "title": "PyDAP Authentication",
    "section": "Conclusion",
    "text": "Conclusion\nUse token-based sessions with pydap to ensure no redirect gotchas.",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/6-nasa-pydap.html#references-1",
    "href": "topics-2025/2025-opendap/6-nasa-pydap.html#references-1",
    "title": "PyDAP Authentication",
    "section": "References",
    "text": "References\n\nhttps://pydap.github.io/pydap/intro.html\nhttps://opendap.github.io/documentation/tutorials/ClientAuthentication.html#_pydap\nhttps://github.com/OPENDAP/NASA-tutorials/tree/main\nhttps://pydap.github.io/pydap/notebooks/Authentication.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Pydap authentication"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#overview",
    "href": "topics-2025/2025-opendap/4-nsidc.html#overview",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nThe National Snow and Ice Data Center servers require NASA Earthdata login authentication, but don’t require a EULA (as far as I know). However they have similar redirect issues as data that does require a EULA. The solution used for datasets that require a EULA seems to work for access.\nI suggest going throuhg 3-nasa.ipynb before this tutorial.\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages and setup\n\nimport xarray as xr\nimport pydap.client\n\n\n# create an authenticated session\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#national-snow-and-ice-data-center",
    "href": "topics-2025/2025-opendap/4-nsidc.html#national-snow-and-ice-data-center",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "National Snow and Ice Data Center",
    "text": "National Snow and Ice Data Center\nTheir OPeNDAP server also uses NASA Earthdata login authentication. I struggled to find the right format for url that would open.\n\n#url = \"https://n5eil02u.ecs.nsidc.org/opendap/OTHR/NISE.004/2012.10.02/NISE_SSMISF17_20121002.HDFEOS\"\nurl = \"https://n5eil02u.ecs.nsidc.org/opendap/MOST/MOD10A1.006/2000.03.22/MOD10A1.A2000082.h00v08.006.2016061212345.hdf\"\n\n\nimport pydap\nfrom pydap.client import open_url\npydap_ds = open_url(url, session=edl_session, protocol=\"dap2\")\n\n\n# I don't know how to tell it to use dap2\nds = xr.open_dataset(url, session=edl_session, engine=\"pydap\")\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'NDSI_Snow_Cover' has multiple fill values {np.int64(200), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'Snow_Albedo_Daily_Tile' has multiple fill values {np.int64(250), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n\n\n\npydap_ds\n\n&lt;DatasetType with children 'NDSI_Snow_Cover', 'NDSI_Snow_Cover_Basic_QA', 'NDSI_Snow_Cover_Algorithm_Flags_QA', 'NDSI', 'Snow_Albedo_Daily_Tile', 'orbit_pnt', 'granule_pnt', 'Latitude', 'Longitude', 'YDim', 'XDim', 'MOD_Grid_Snow_500m_eos_cf_projection'&gt;\n\n\n\n%%time\n# this works but odd errors\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'NDSI_Snow_Cover' has multiple fill values {np.int64(200), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'Snow_Albedo_Daily_Tile' has multiple fill values {np.int64(250), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n\n\nCPU times: user 31.2 ms, sys: 3.93 ms, total: 35.1 ms\nWall time: 4.09 s\n\n\n\n%%time\nurl=\"https://n5eil02u.ecs.nsidc.org/opendap/MOST/MOD10A1.006/2000.03.22/MOD10A1.A2000082.h00v08.006.2016061212345.hdf\"\nds = xr.open_dataset(url, engine=\"pydap\", session=session)\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/pydap/handlers/dap.py:123: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2 which is consider legacy and may result in slower responses. For more, see go to https://www.opendap.org/faq-page.\n  _warnings.warn(\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'NDSI_Snow_Cover' has multiple fill values {np.int64(200), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/conventions.py:200: SerializationWarning: variable 'Snow_Albedo_Daily_Tile' has multiple fill values {np.int64(250), np.int64(255)} defined, decoding all values to NaN.\n  var = coder.decode(var, name=name)\n\n\nCPU times: user 55 ms, sys: 15.3 ms, total: 70.2 ms\nWall time: 1.09 s",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#conclusion",
    "href": "topics-2025/2025-opendap/4-nsidc.html#conclusion",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nUse token-based sessions to fix redirect errors.",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/4-nsidc.html#references",
    "href": "topics-2025/2025-opendap/4-nsidc.html#references",
    "title": "Accessing data on NSIDC servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://github.com/pydap/pydap/issues/188\nhttps://nsidc.org/data/user-resources/help-center/how-do-i-access-data-using-opendap#anchor-using-a-command-line-interface",
    "crumbs": [
      "Python - OPeNDAP",
      "NSIDC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#overview",
    "href": "topics-2025/2025-opendap/2-dbofs.html#overview",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Overview",
    "text": "Overview\nThis example is very similar to the first tutorial using NCEP-NCAR Reanalysis 1, but the netcdfs are slightly different and you will get more practice. This tutorial uses an example where the server doesn’t require authentication (username and password).",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#deleware-bay-forecast",
    "href": "topics-2025/2025-opendap/2-dbofs.html#deleware-bay-forecast",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Deleware Bay Forecast",
    "text": "Deleware Bay Forecast\nWe will create a data cube for data from the Delaware Bay Operational Forecast System (DBOFS). The approach is the same. We go to the THREDDS server for NOS and navigate through until we find the OPeNDAP page for a file. Then we need to get the url format for each file. Here is an example for one day. Note they only keep recent data so this url will break after March 2025.\nhttps://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/14/dbofs.t18z.20250314.regulargrid.n001.nc\nFirst step is to create some file urls. I am going to get the files for yesterday since they only keep the gridded data for couple days.\n\nimport xarray as xr\n\n\nfrom datetime import datetime, timedelta, timezone\n\n# Get yesterday's date in UTC\nyesterday = datetime.now(timezone.utc) - timedelta(days=1)\nyear, month, day = yesterday.strftime('%Y'), yesterday.strftime('%m'), yesterday.strftime('%d')\n\n# Base URL with placeholders\nbase = f'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/{year}/{month}/{day}/dbofs.t%2.2dz.{year}{month}{day}.regulargrid.n001.nc'\n\n# Generate URLs for different hours\nurls = [base % d for d in range(0, 24, 6)]\nurls\n\n['https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/17/dbofs.t00z.20250317.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/17/dbofs.t06z.20250317.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/17/dbofs.t12z.20250317.regulargrid.n001.nc',\n 'https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS/2025/03/17/dbofs.t18z.20250317.regulargrid.n001.nc']\n\n\nThen we can open these as usual with xarray.\n\n%%time\nds = xr.open_mfdataset(urls, parallel=True)\nds\n\nCPU times: user 1.48 s, sys: 358 ms, total: 1.84 s\nWall time: 5.97 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 392MB\nDimensions:      (time: 4, ny: 487, nx: 529, Depth: 22)\nCoordinates:\n  * Depth        (Depth) float64 176B 0.0 1.0 2.0 4.0 ... 80.0 90.0 100.0 125.0\n    Latitude     (ny, nx) float64 2MB dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;\n    Longitude    (ny, nx) float64 2MB dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;\n  * time         (time) datetime64[ns] 32B 2025-03-16T19:00:00 ... 2025-03-17...\nDimensions without coordinates: ny, nx\nData variables:\n    h            (time, ny, nx) float64 8MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    mask         (time, ny, nx) float64 8MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    zeta         (time, ny, nx) float32 4MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    zetatomllw   (time, ny, nx) float32 4MB dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;\n    u_eastward   (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    v_northward  (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    temp         (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\n    salt         (time, Depth, ny, nx) float32 91MB dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;\nAttributes: (12/36)\n    file:                            dbofs.t00z.20250317.fields.nowcast_0002.nc\n    format:                          netCDF-4/HDF5 file\n    Conventions:                     CF-1.4, SGRID-0.3\n    type:                            ROMS/TOMS history file\n    title:                           dbofs nowcast RUN in operational mode\n    var_info:                        varinfo.yaml\n    ...                              ...\n    compiler_flags:                  -fp-model precise -ip -O3\n    tiling:                          008x016\n    history:                         ROMS/TOMS, Version 4.2, Monday - March 1...\n    ana_file:                        ROMS/Functionals/ana_btflux.h, ROMS/Func...\n    CPP_options:                     mode, ADD_FSOBC, ADD_M2OBC, ANA_BSFLUX, ...\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:time: 4ny: 487nx: 529Depth: 22Coordinates: (4)Depth(Depth)float640.0 1.0 2.0 ... 90.0 100.0 125.0long_name :Depths of Standard Layerstandard_name :depthunits :mpositive :downaxis :Zarray([  0.,   1.,   2.,   4.,   6.,   8.,  10.,  12.,  15.,  20.,  25.,  30.,\n        35.,  40.,  45.,  50.,  60.,  70.,  80.,  90., 100., 125.])Latitude(ny, nx)float64dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;long_name :Latitude in common gridstandard_name :latitudeunits :degrees_north\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.97 MiB\n1.97 MiB\n\n\nShape\n(487, 529)\n(487, 529)\n\n\nDask graph\n1 chunks in 15 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         529 487\n\n\n\n\nLongitude(ny, nx)float64dask.array&lt;chunksize=(487, 529), meta=np.ndarray&gt;long_name :Longitude in common gridstandard_name :longitudeunits :degrees_east\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.97 MiB\n1.97 MiB\n\n\nShape\n(487, 529)\n(487, 529)\n\n\nDask graph\n1 chunks in 15 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n         529 487\n\n\n\n\ntime(time)datetime64[ns]2025-03-16T19:00:00 ... 2025-03-...long_name :time since initializationfield :time, scalar, series_ChunkSizes :512array(['2025-03-16T19:00:00.000000000', '2025-03-17T01:00:00.000000000',\n       '2025-03-17T07:00:00.000000000', '2025-03-17T13:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (8)h(time, ny, nx)float64dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :bathymetrystandard_name :sea_floor_depth_below_mean_sea_levelunits :meterpositive :downgrid :gridlocation :facefield :bath, scaler\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n7.86 MiB\n1.97 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 13 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nmask(time, ny, nx)float64dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :maskstandard_name :sea_binary_maskflag_values :[0. 1.]flag_meanings :land, watergrid :gridlocation :face\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n7.86 MiB\n1.97 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 13 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nzeta(time, ny, nx)float32dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :free-surface referenced to mean sea levelstandard_name :sea_surface_elevationunits :meterstime :timegrid :gridlocation :facefield :free-surface, scalar, series_ChunkSizes :[  1 487 529]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.93 MiB\n0.98 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nzetatomllw(time, ny, nx)float32dask.array&lt;chunksize=(1, 487, 529), meta=np.ndarray&gt;long_name :free-surface referenced to mean lower low waterunits :meterstime :timegrid :gridlocation :facefield :free-surface, scalar, series_ChunkSizes :[  1 487 529]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n3.93 MiB\n0.98 MiB\n\n\nShape\n(4, 487, 529)\n(1, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                               529 487 4\n\n\n\n\nu_eastward(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :eastward momentum componentstandard_name :eastward_sea_water_velocityunits :meters second-1time :timegrid :gridlocation :facefield :u_eastward, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nv_northward(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :northward momentum componentstandard_name :northward_sea_water_velocityunits :meters second-1time :timegrid :gridlocation :facefield :v_northward, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\ntemp(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :potential temperaturestandard_name :sea_water_temperatureunits :Celsiustime :timegrid :gridlocation :facefield :temperature, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nsalt(time, Depth, ny, nx)float32dask.array&lt;chunksize=(1, 22, 487, 529), meta=np.ndarray&gt;long_name :salinitystandard_name :sea_water_salinityunits :PSUtime :timegrid :gridlocation :facefield :salinity, scalar, series_ChunkSizes :[  1  11 244 265]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n86.48 MiB\n21.62 MiB\n\n\nShape\n(4, 22, 487, 529)\n(1, 22, 487, 529)\n\n\nDask graph\n4 chunks in 9 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n            4 1                          529 487 22\n\n\n\n\nIndexes: (2)DepthPandasIndexPandasIndex(Index([  0.0,   1.0,   2.0,   4.0,   6.0,   8.0,  10.0,  12.0,  15.0,  20.0,\n        25.0,  30.0,  35.0,  40.0,  45.0,  50.0,  60.0,  70.0,  80.0,  90.0,\n       100.0, 125.0],\n      dtype='float64', name='Depth'))timePandasIndexPandasIndex(DatetimeIndex(['2025-03-16 19:00:00', '2025-03-17 01:00:00',\n               '2025-03-17 07:00:00', '2025-03-17 13:00:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (36)file :dbofs.t00z.20250317.fields.nowcast_0002.ncformat :netCDF-4/HDF5 fileConventions :CF-1.4, SGRID-0.3type :ROMS/TOMS history filetitle :dbofs nowcast RUN in operational modevar_info :varinfo.yamlrst_file :dbofs.t00z.20250317.rst.nowcast.nchis_base :dbofs.t00z.20250317.fields.nowcaststa_file :dbofs.t00z.20250317.stations.nowcast.ncgrd_file :dbofs.romsgrid.ncini_file :dbofs.t00z.20250317.init.nowcast.ncriver_file :dbofs.t00z.20250317.river.nctide_file :dbofs.roms.tides.ncfrc_file_01 :dbofs.t00z.20250317.met.nowcast.ncbry_file_01 :dbofs.t00z.20250317.obc.ncscript_file :dbofs_ROMS_nowcast.inspos_file :dbofs.stations.inNLM_TADV :\nADVECTION:   HORIZONTAL   VERTICAL     \ntemp:        HSIMT        HSIMT        \nsalt:        HSIMT        HSIMTNLM_LBC :\nEDGE:  WEST   SOUTH  EAST   NORTH  \nzeta:  Cha    Cha    Cha    Clo    \nubar:  Fla    Fla    Fla    Clo    \nvbar:  Fla    Fla    Fla    Clo    \nu:     Rad    Rad    Rad    Clo    \nv:     Rad    Rad    Rad    Clo    \ntemp:  RadNud RadNud RadNud Clo    \nsalt:  RadNud RadNud RadNud Clo    \ntke:   Gra    Gra    Gra    Clogit_url :https://github.com/NOAA-CO-OPS/2024-NOS-Code-Package_v3.6.0git_rev :c1e184cd03c2208a7b6924f2ef264d1c9feeb838svn_url :https://www.myroms.org/svn/src/trunksvn_rev :1209code_dir :/lfs/h1/ops/para/packages/nosofs.v3.6.1/sorc/ROMS.fdheader_dir :/lfs/h1/ops/para/packages/nosofs.v3.6.1/includeheader_file :dbofs.hos :Linuxcpu :x86_64compiler_system :ftn-intelcompiler_command :/opt/cray/pe/craype/2.7.17/bin/ftncompiler_flags :-fp-model precise -ip -O3tiling :008x016history :ROMS/TOMS, Version 4.2, Monday - March 17, 2025 - 12:48:22 AMana_file :ROMS/Functionals/ana_btflux.h, ROMS/Functionals/ana_rain.h, ROMS/Functionals/ana_stflux.hCPP_options :mode, ADD_FSOBC, ADD_M2OBC, ANA_BSFLUX, ANA_BTFLUX, ANA_RAIN, ANA_SSFLUX, ASSUMED_SHAPE, ATM_PRESS, BOUNDARY_ALLREDUCE, BULK_FLUXES, COLLECT_ALLGATHER, CURVGRID, DIFF_GRID, DJ_GRADPS, DOUBLE_PRECISION, EMINUSP, HDF5, LIMIT_STFLX_COOLING, KANTHA_CLAYSON, LONGWAVE_OUT, MASKING, MIX_GEO_TS, MIX_S_UV, MPI, MY25_MIXING, NONLINEAR, NONLIN_EOS, NO_LBC_ATT, N2S2_HORAVG, PERFECT_RESTART, POWER_LAW, PROFILE, K_GSCHEME, RADIATION_2D, REDUCE_ALLREDUCE, !RST_SINGLE, SALINITY, STEP2D_LF_AM3, SOLAR_SOURCE, SOLVE3D, SSH_TIDES, STATIONS, TS_DIF2, UV_ADV, UV_COR, UV_U3HADVECTION, UV_C4VADVECTION, UV_DRAG_GRID, UV_QDRAG, UV_TIDES, UV_VIS2, VAR_RHO_2D, VISC_GRIDDODS_EXTRA.Unlimited_Dimension :time\n\n\n\n# each file is about 100Mb\nprint(f\"{ds.isel(time=1).nbytes / 1e6} Mb\")\n\n100.9884 Mb\n\n\nNext we can plot a map of the temperature at one time point.\n\nds.temp.isel(Depth=1, time=1).plot(x='Longitude', y='Latitude');\n\n\n\n\n\n\n\n\nAnd we can get the mean temperature for the 4 time points.\n\n%%time\nds_mean = ds[\"temp\"].isel(Depth=1).mean(dim=['ny', 'nx'])\nds_mean.plot();\n\nCPU times: user 58.2 ms, sys: 3.28 ms, total: 61.5 ms\nWall time: 1.19 s\n\n\n\n\n\n\n\n\n\n\nA plot of temperature by depth\nHere I will make a plot of temperature by depth in the middle of the bay.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Plot the full dataset\nfig, ax = plt.subplots(figsize=(8, 6))\nds2.temp.isel(Depth=1, time=1).plot(x=\"Longitude\", y=\"Latitude\", ax=ax)\n\n# Define the slice box coordinates\nlon_min, lon_max = -74.5, -74\nlat_min, lat_max = 38, 38.5\n\n# Create a rectangular patch (bounding box)\nbox = patches.Rectangle(\n    (lon_min, lat_min),  # Bottom-left corner (lon, lat)\n    lon_max - lon_min,   # Width (longitude range)\n    lat_max - lat_min,   # Height (latitude range)\n    linewidth=2, edgecolor='red', facecolor='none', linestyle=\"--\"\n)\n\n# Add the box to the plot\nax.add_patch(box)\n\n# Customize the plot\nax.set_title(\"Temperature with Slice Region Highlighted\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFirst, I am going to fix the indices to use lat/lon.\n\n# because I want to slice with actual lat lon\nlat_values = ds.isel(time=1, Depth=1, nx=1).Latitude.values\nlon_values = ds.isel(time=1, Depth=1, ny=1).Longitude.values\nds = ds.assign_coords({\"ny\": lat_values, \"nx\": lon_values})\nds = ds.rename({\"ny\": \"lat\", \"nx\": \"lon\"})\n\nCreate a mean by depth for each time period.\n\ndepth_slice = ds[\"temp\"].sel(lon=slice(-74.5,-74), lat=slice(38, 38.5))\n\n\ndepth_slice.mean(dim=['lat', 'lon']).plot.line(x=\"Depth\");",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#data-on-aws",
    "href": "topics-2025/2025-opendap/2-dbofs.html#data-on-aws",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Data on AWS",
    "text": "Data on AWS\nThe data are also available on AWS in a S3 bucket. Let’s compare data access to that. https://noaa-nos-ofs-pds.s3.amazonaws.com/index.html The data are here but you need to know how to make s3 urls.\nhttps://noaa-nos-ofs-pds.s3.amazonaws.com/dbofs/netcdf/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc\nbecomes this\ns3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/14/dbofs.t00z.20250314.regulargrid.n001.nc\nTo open netcdf on s3, we need to create a “fileset”; we cannot just us the list of urls like we can for the OPeNDAP links.\n\n# create the file urls to s3 bucket by processing our original files list\ns3_urls = [\n    url.replace(\n        \"https://opendap.co-ops.nos.noaa.gov/thredds/dodsC/NOAA/DBOFS/MODELS\",\n        \"s3://noaa-nos-ofs-pds/dbofs/netcdf\"\n    ) for url in urls\n]\ns3_urls\n\n['s3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/16/dbofs.t00z.20250316.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/16/dbofs.t06z.20250316.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/16/dbofs.t12z.20250316.regulargrid.n001.nc',\n 's3://noaa-nos-ofs-pds/dbofs/netcdf/2025/03/16/dbofs.t18z.20250316.regulargrid.n001.nc']\n\n\n\n# Run this code once to set up s3 access\nimport s3fs \nfs = s3fs.S3FileSystem(anon=True)\n\n# Create a fileset\nfileset = [fs.open(url) for url in s3_urls]\n\n\n# each file is about 100Mb\nfs.size(s3_urls[1])/1e6  # MB#\n\n101.415959\n\n\nWe open up the fileset.\n\n%%time\nds2 = xr.open_mfdataset(fileset)\n\nCPU times: user 2.18 s, sys: 1.2 s, total: 3.38 s\nWall time: 20.5 s\n\n\nWe now have a data cube that we can work with same as with our data cube from the OPeNDAP server. The data are only loaded when we need them (to plot or compute). Data access is considerably slower than for the OPeNDAP server. I don’t know why that is.\n\n%%time\nds2_mean = ds2[\"temp\"].isel(Depth=1).mean(dim=['ny', 'nx'])\nds2_mean.plot();\n\nCPU times: user 1.89 s, sys: 877 ms, total: 2.77 s\nWall time: 22.7 s",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#conclusion",
    "href": "topics-2025/2025-opendap/2-dbofs.html#conclusion",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "Conclusion",
    "text": "Conclusion\nWe worked through another example of getting data off an OPeNDAP server and compared to getting the data off an S3 bucket.",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/2-dbofs.html#references",
    "href": "topics-2025/2025-opendap/2-dbofs.html#references",
    "title": "Delaware Bay Operational Forecast System (DBOFS)",
    "section": "References",
    "text": "References\n\nOpen files in S3 bucket https://nbviewer.org/gist/rsignell-usgs/111222351c4fee9e99827844351ab952\nhttps://www.jamstec.go.jp/ridinfo/xarray-and-opendap/\nMore opendap + xarray debugging https://github.com/stuckyb/gcdl/issues/24\nAnother opendap example https://github.com/ornldaac/daymet-python-opendap-xarray/blob/master/1_daymetv4_discovery_access_subsetting.ipynb\nNice example https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html\n\n\nds.to_zarr(\"metadata.zarr\", consolidated=True, compute=False)\n\nDelayed('_finalize_store-304e974e-7cd0-41d4-b610-fe79bd608544')\n\n\n\nds_lazy = xr.open_zarr(\"metadata.zarr\", consolidated=True)\n\n\nds_lazy.temp.isel(Depth=1, time=1).plot(x='Longitude', y='Latitude');\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 ds_lazy.temp.isel(Depth=1, time=1).plot(x='Longitude', y='Latitude');\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/accessor.py:48, in DataArrayPlotAccessor.__call__(self, **kwargs)\n     46 @functools.wraps(dataarray_plot.plot, assigned=(\"__doc__\", \"__annotations__\"))\n     47 def __call__(self, **kwargs) -&gt; Any:\n---&gt; 48     return dataarray_plot.plot(self._da, **kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/dataarray_plot.py:310, in plot(darray, row, col, col_wrap, ax, hue, subplot_kws, **kwargs)\n    306     plotfunc = hist\n    308 kwargs[\"ax\"] = ax\n--&gt; 310 return plotfunc(darray, **kwargs)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/dataarray_plot.py:1607, in _plot2d.&lt;locals&gt;.newplotfunc(***failed resolving arguments***)\n   1603     raise ValueError(\"plt.imshow's `aspect` kwarg is not available in xarray\")\n   1605 ax = get_axis(figsize, size, aspect, ax, **subplot_kws)\n-&gt; 1607 primitive = plotfunc(\n   1608     xplt,\n   1609     yplt,\n   1610     zval,\n   1611     ax=ax,\n   1612     cmap=cmap_params[\"cmap\"],\n   1613     vmin=cmap_params[\"vmin\"],\n   1614     vmax=cmap_params[\"vmax\"],\n   1615     norm=cmap_params[\"norm\"],\n   1616     **kwargs,\n   1617 )\n   1619 # Label the plot with metadata\n   1620 if add_labels:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/dataarray_plot.py:2316, in pcolormesh(x, y, z, ax, xscale, yscale, infer_intervals, **kwargs)\n   2313         y = _infer_interval_breaks(y, axis=0, scale=yscale)\n   2315 ax.grid(False)\n-&gt; 2316 primitive = ax.pcolormesh(x, y, z, **kwargs)\n   2318 # by default, pcolormesh picks \"round\" values for bounds\n   2319 # this results in ugly looking plots with lots of surrounding whitespace\n   2320 if not hasattr(ax, \"projection\") and x.ndim == 1 and y.ndim == 1:\n   2321     # not a cartopy geoaxis\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/matplotlib/__init__.py:1521, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1518 @functools.wraps(func)\n   1519 def inner(ax, *args, data=None, **kwargs):\n   1520     if data is None:\n-&gt; 1521         return func(\n   1522             ax,\n   1523             *map(cbook.sanitize_sequence, args),\n   1524             **{k: cbook.sanitize_sequence(v) for k, v in kwargs.items()})\n   1526     bound = new_sig.bind(ax, *args, **kwargs)\n   1527     auto_label = (bound.arguments.get(label_namer)\n   1528                   or bound.kwargs.get(label_namer))\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/matplotlib/axes/_axes.py:6522, in Axes.pcolormesh(self, alpha, norm, cmap, vmin, vmax, colorizer, shading, antialiased, *args, **kwargs)\n   6519 shading = shading.lower()\n   6520 kwargs.setdefault('edgecolors', 'none')\n-&gt; 6522 X, Y, C, shading = self._pcolorargs('pcolormesh', *args,\n   6523                                     shading=shading, kwargs=kwargs)\n   6524 coords = np.stack([X, Y], axis=-1)\n   6526 kwargs.setdefault('snap', mpl.rcParams['pcolormesh.snap'])\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/matplotlib/axes/_axes.py:6026, in Axes._pcolorargs(self, funcname, shading, *args, **kwargs)\n   6024     if funcname == 'pcolormesh':\n   6025         if np.ma.is_masked(X) or np.ma.is_masked(Y):\n-&gt; 6026             raise ValueError(\n   6027                 'x and y arguments to pcolormesh cannot have '\n   6028                 'non-finite values or be of type '\n   6029                 'numpy.ma.MaskedArray with masked values')\n   6030     nrows, ncols = C.shape[:2]\n   6031 else:\n\nValueError: x and y arguments to pcolormesh cannot have non-finite values or be of type numpy.ma.MaskedArray with masked values\n\n\n\n\n\n\n\n\n\n\n\nds.to_netcdf(\"metadata.nc\", compute=False)\n\nHDF5-DIAG: Error detected in HDF5 (1.14.3) thread 1:\n  #000: H5F.c line 660 in H5Fcreate(): unable to synchronously create file\n    major: File accessibility\n    minor: Unable to create file\n  #001: H5F.c line 614 in H5F__create_api_common(): unable to create file\n    major: File accessibility\n    minor: Unable to open file\n  #002: H5VLcallback.c line 3605 in H5VL_file_create(): file create failed\n    major: Virtual Object Layer\n    minor: Unable to create file\n  #003: H5VLcallback.c line 3571 in H5VL__file_create(): file create failed\n    major: Virtual Object Layer\n    minor: Unable to create file\n  #004: H5VLnative_file.c line 94 in H5VL__native_file_create(): unable to create file\n    major: File accessibility\n    minor: Unable to open file\n  #005: H5Fint.c line 1870 in H5F_open(): unable to truncate a file which is already open\n    major: File accessibility\n    minor: Unable to open file\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:211, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    210 try:\n--&gt; 211     file = self._cache[self._key]\n    212 except KeyError:\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56, in LRUCache.__getitem__(self, key)\n     55 with self._lock:\n---&gt; 56     value = self._cache[key]\n     57     self._cache.move_to_end(key)\n\nKeyError: [&lt;class 'netCDF4._netCDF4.Dataset'&gt;, ('/home/jovyan/nmfshackdays-2025/topics-2025/2025-opendap/metadata.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '5982d31b-d457-4ea0-a129-4860759a8ef3']\n\nDuring handling of the above exception, another exception occurred:\n\nPermissionError                           Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 ds.to_netcdf(\"metadata.nc\", compute=False)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/core/dataset.py:2380, in Dataset.to_netcdf(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\n   2377     encoding = {}\n   2378 from xarray.backends.api import to_netcdf\n-&gt; 2380 return to_netcdf(  # type: ignore[return-value]  # mypy cannot resolve the overloads:(\n   2381     self,\n   2382     path,\n   2383     mode=mode,\n   2384     format=format,\n   2385     group=group,\n   2386     engine=engine,\n   2387     encoding=encoding,\n   2388     unlimited_dims=unlimited_dims,\n   2389     compute=compute,\n   2390     multifile=False,\n   2391     invalid_netcdf=invalid_netcdf,\n   2392     auto_complex=auto_complex,\n   2393 )\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/api.py:1911, in to_netcdf(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\n   1908 if auto_complex is not None:\n   1909     kwargs[\"auto_complex\"] = auto_complex\n-&gt; 1911 store = store_open(target, mode, format, group, **kwargs)\n   1913 if unlimited_dims is None:\n   1914     unlimited_dims = dataset.encoding.get(\"unlimited_dims\", None)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:452, in NetCDF4DataStore.open(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\n    448     kwargs[\"auto_complex\"] = auto_complex\n    449 manager = CachingFileManager(\n    450     netCDF4.Dataset, filename, mode=mode, kwargs=kwargs\n    451 )\n--&gt; 452 return cls(manager, group=group, mode=mode, lock=lock, autoclose=autoclose)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:393, in NetCDF4DataStore.__init__(self, manager, group, mode, lock, autoclose)\n    391 self._group = group\n    392 self._mode = mode\n--&gt; 393 self.format = self.ds.data_model\n    394 self._filename = self.ds.filepath()\n    395 self.is_remote = is_remote_uri(self._filename)\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:461, in NetCDF4DataStore.ds(self)\n    459 @property\n    460 def ds(self):\n--&gt; 461     return self._acquire()\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:455, in NetCDF4DataStore._acquire(self, needs_lock)\n    454 def _acquire(self, needs_lock=True):\n--&gt; 455     with self._manager.acquire_context(needs_lock) as root:\n    456         ds = _nc4_require_group(root, self._group, self._mode)\n    457     return ds\n\nFile /srv/conda/envs/notebook/lib/python3.12/contextlib.py:137, in _GeneratorContextManager.__enter__(self)\n    135 del self.args, self.kwds, self.func\n    136 try:\n--&gt; 137     return next(self.gen)\n    138 except StopIteration:\n    139     raise RuntimeError(\"generator didn't yield\") from None\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:199, in CachingFileManager.acquire_context(self, needs_lock)\n    196 @contextlib.contextmanager\n    197 def acquire_context(self, needs_lock=True):\n    198     \"\"\"Context manager for acquiring a file.\"\"\"\n--&gt; 199     file, cached = self._acquire_with_cache_info(needs_lock)\n    200     try:\n    201         yield file\n\nFile /srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/backends/file_manager.py:217, in CachingFileManager._acquire_with_cache_info(self, needs_lock)\n    215     kwargs = kwargs.copy()\n    216     kwargs[\"mode\"] = self._mode\n--&gt; 217 file = self._opener(*self._args, **kwargs)\n    218 if self._mode == \"w\":\n    219     # ensure file doesn't get overridden when opened again\n    220     self._mode = \"a\"\n\nFile src/netCDF4/_netCDF4.pyx:2521, in netCDF4._netCDF4.Dataset.__init__()\n\nFile src/netCDF4/_netCDF4.pyx:2158, in netCDF4._netCDF4._ensure_nc_success()\n\nPermissionError: [Errno 13] Permission denied: '/home/jovyan/nmfshackdays-2025/topics-2025/2025-opendap/metadata.nc'\n\n\n\n\nds_lazy = xr.open_dataset(\"metadata.nc\")",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - ROMS Models"
    ]
  },
  {
    "objectID": "topics-2025/2025-03-21-Signell/index.html#how-to-download-and-open-the-tutorials",
    "href": "topics-2025/2025-03-21-Signell/index.html#how-to-download-and-open-the-tutorials",
    "title": "Using Virtualizarr, Dask and Holoviz to explore NODD data",
    "section": "How to download and open the tutorials",
    "text": "How to download and open the tutorials\n\nJupyter Hub\n\nStart the Jupyter Hub server &lt;nmfs-openscapes.2i2c.cloud&gt;\nClick the orange Open in Jupyter Hub button\n\n\n\nColab\n\nClick the Open in Colab button\n\n\n\nDownload\n\nDownload to your local computer\nYou will need to have Python and Jupyter Lab installed\nInstall any needed packages\n\n\n\nHow to clone the git repository\nAfter cloning, you will need to navigate to the tutorials in the topics-2025 directory.\nNever cloned the NMFSHackDays-2025 repo?\ncd ~\ngit clone https://github.com/nmfs-opensci/NMFSHackDays-2025\nHave cloned it but need to update? This is going to destroy any changes that you made to the repo to make it match the current state of the repo on GitHub.\ncd ~/NMFSHackDays-2025\ngit fetch origin\ngit reset --hard origin/main"
  },
  {
    "objectID": "topics-2025/2025-03-21-Signell/Parallel_compute_with_dask.html",
    "href": "topics-2025/2025-03-21-Signell/Parallel_compute_with_dask.html",
    "title": "Simple dask example",
    "section": "",
    "text": "# import needed modules\nimport time, random\n\n# define our functions\ndef inc(x):\n    time.sleep(random.random())\n    return x + 1\n\ndef dec(x):\n    time.sleep(random.random())\n    return x - 1\n\ndef add(x, y):\n    time.sleep(random.random())\n    return x + y\n\n\n%%time\n\n# a sequential example with no parallelization\nresults = []\nfor x in range(20):\n    result = inc(x)\n    result = dec(result)\n    results.append(result)\n\nprint(results)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nCPU times: user 1.21 s, sys: 78.5 ms, total: 1.29 s\nWall time: 18.3 s\n\n\n\n# import dask for parallel work\nfrom dask.distributed import Client, LocalCluster\n\n\n# Set up our cluster with default workers and threads\ncluster = LocalCluster(processes=False)\ncluster\n\n/srv/conda/envs/notebook/lib/python3.11/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 37437 instead\n  warnings.warn(\n\n\n\n\n\n\n%%time\n# Set up a client for work\nclient = cluster.get_client()\n\nresults = []\nfor x in range(20):\n    result = client.submit(inc, x)\n    result = client.submit(dec, result)\n    results.append(result)\n\nresults = client.gather(results)\nprint(results)\nclient.close()\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nCPU times: user 1.14 s, sys: 50.1 ms, total: 1.19 s\nWall time: 6.52 s\n\n\n\n# When we are done we can close our dask cluster\ncluster.close()\n\n\n# Set up a new cluster with default 4 workers and 1 thread per worker\ncluster = LocalCluster(n_workers=4, processes=False, threads_per_worker=1)\ncluster\n\n\n\n\n\n%%time\n# Set up a client for work\nclient = cluster.get_client()\n\nresults = []\nfor x in range(20):\n    result = client.submit(inc, x)\n    result = client.submit(dec, result)\n    results.append(result)\n\nresults = client.gather(results)\nprint(results)\nclient.close()\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nCPU times: user 831 ms, sys: 39.9 ms, total: 871 ms\nWall time: 5.58 s\n\n\n\n# When we are done we can close our dask cluster\ncluster.close()\n\n\ncluster"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes Jupyter Hub and you will not need to install anything.\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)"
  },
  {
    "objectID": "index.html#schedule-links-to-content-on-left",
    "href": "index.html#schedule-links-to-content-on-left",
    "title": "HackHours 2025",
    "section": "Schedule (links to content on left)",
    "text": "Schedule (links to content on left)\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Using LLMs in R to improve data dashboards (Carl Boettiger, UC Berkeley) \nMar 21 - VirtualiZarr, Dask and Holoviz to explore NODD data (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs \nTBD - Introduction to the Nautilus HyperCluster for running containerized Big Data Applications, UC Berkeley)"
  },
  {
    "objectID": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "href": "content/why-cloud.html#why-would-i-want-to-work-in-the-cloud",
    "title": "NMFS HackHours 2025",
    "section": "Why would I want to work in the cloud?",
    "text": "Why would I want to work in the cloud?\nWatch this video on “Enabling Analysis in the Cloud Using NASA Earth Science Data” by Michele Thorton, a NASA Openscapes mentor from the Oak Ridge National Laboratory Distributed Active Archive Center.\n\n\nMore earth data tutorials to explore!\nThe content and tutorials is a mix of content from workshops by the NASA Openscapes mentors (for example 2023 Cloud AGU Workshop), content developed by Carl Boettiger for NASA TOPS-T Cloud Native Geospatial in R & Python, content by NMFS CoastWatch, and other internal and external tutorials.\nHow do I get these tutorials into the JupyterHub? clone this https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/ and then look in the examples folder.\nHow do I clone? Since these are Jupyter/Python notebooks, easiest is cloning via JupyterLab.\n\nGo to JupyterLab. How? I closed the tab. Open the JupyterHub url again.\nClick on the little file icon on left until you are at the home directory.\nClick on the little Git icon on left. Which is it? Click on all the icons until you find it.\nWhen you see the ‘Clone repository’ button, click that. Paste in the url of the GitHub repo. I don’t see ‘Clone repository’. Go back to step 1. You are not in the home directory yet.\n\n\n\nFAQ\n\nCan I bring my own content/code to the event and JupyterHub? Absolutely, please do!! You can clone a GitHub repo or just upload files into the hub."
  },
  {
    "objectID": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "href": "content/signup.html#noaa-fisheries-friday-hackhours-12-1pm-pt3-4pm-et",
    "title": "HackHours in R and Python",
    "section": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET",
    "text": "NOAA Fisheries Friday Hackhours 12-1pm PT/3-4pm ET\nUse this form to sign-up to be alerted for future hackdays and Intro to JupyterHubs sessions: SIGN-UP FORM\nContact or questions: Eli Holmes (NOAA) - Type my name in your NOAA email, and my contact will pop up. Note, it uses “Eli” not “Elizabeth”.\nDuring these 1 hour hackhours, we will learn to do cloud computing with a JupyterHub set-up with geospatial packages and data. These sessions will get you more familiar with cloud-computing, JupyterHubs, Jupyter notebooks, and Python for geospatial analysis.\nAdd event to your calendar\nClick “HackHour 2024” for list of events and dates"
  },
  {
    "objectID": "content/reuse.html",
    "href": "content/reuse.html",
    "title": "Reuse Statement",
    "section": "",
    "text": "This content is released under CC0 Creative Commons.\nPermissive Re-Mix and No Attribution Needed: You may reuse the NMFS Open Science content in this repository—excluding the NMFS Open Science logo and any NOAA logos—in any way you like. You do not need permission. You do not need to give attribution, but if you use large parts of tutorials or content it is polite to give acknowledgement of the source. Please check each repository for its reuse statement. Some of the content is from outside of NMFS Open Science. This will be noted and you should check the original content for its reuse statement.",
    "crumbs": [
      "Reuse Statement"
    ]
  },
  {
    "objectID": "content/hackhours.html",
    "href": "content/hackhours.html",
    "title": "Hackhours",
    "section": "",
    "text": "These sessions are for NOAA staff to gain more familiarity with Jupyter Hubs and working with spatial data, esp big data hosted in the cloud in databases, via code and via geospatial packages in R and Python.\nFor more data science trainings and resource, see: * https://sites.google.com/noaa.gov/nmfs-hq-st-open-science/trainings * https://coastwatch.noaa.gov/cwn/training-courses.html * https://ioos.github.io/ioos_code_lab/content/intro.html",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/hackhours.html#about-nmfs-open-science",
    "href": "content/hackhours.html#about-nmfs-open-science",
    "title": "Hackhours",
    "section": "About NMFS Open Science",
    "text": "About NMFS Open Science\nWe provide technical and infrastructure support for any groups within NOAA Fisheries who would like computing support for their workshops or trainings; See our training page (NOAA internal). In September 2024, we launched a JupyterHub with a variety of specialized computing environments tailored to needs in fisheries and ocean modeling. We also support the development of a Docker stack tailored to R and Python workflows. We run regular workshops and trainings in reproducible science. See NMFS Openscapes and NMFS Open Science. If at NOAA see our internal site and the tabs for News and Training.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "coc.html",
    "href": "coc.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to providing a harassment-free learning experience for everyone. We do not tolerate harassment of participants in any form. Sexual language and imagery is not appropriate either in-person or virtual form, including the Discussion boards and Chats. Participants (including event volunteers and organizers) violating these rules may be sanctioned or expelled from the event at the discretion of the organizers.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#definition-of-harassment",
    "href": "coc.html#definition-of-harassment",
    "title": "Code of Conduct",
    "section": "Definition of Harassment",
    "text": "Definition of Harassment\nHarassment includes, but is not limited to:\n\nVerbal comments that harass based on sexual orientation, disability, physical appearance, body size, race, age, religion.\nSexual images in public spaces\nDeliberate intimidation, stalking, or following\nHarassing photography or recording\nSustained disruption of talks or other events\nInappropriate physical contact\nUnwelcome sexual attention\nAdvocating for, or encouraging, any of the above behavior",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#expectations",
    "href": "coc.html#expectations",
    "title": "Code of Conduct",
    "section": "Expectations",
    "text": "Expectations\nParticipants asked to stop any harassing behavior are expected to comply immediately. If a participant engages in harassing behavior, the organizers retain the right to take any actions to keep the event a welcoming environment for all participants. This includes warning the offender or expulsion from the event.\nThe organizers may take action to redress anything designed to, or with the clear impact of, disrupting the event or making the environment hostile for any participants. We expect participants to follow these rules at all the event venues and event-related social activities.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "coc.html#reporting-a-violation",
    "href": "coc.html#reporting-a-violation",
    "title": "Code of Conduct",
    "section": "Reporting a violation",
    "text": "Reporting a violation\nHarassment and other code of conduct violations reduce the value of the event for everyone. If someone makes you or anyone else feel unsafe or unwelcome, please report it as soon as possible.\nIf you feel comfortable contacting someone associated with our event, you may speak with one of the event organizers in person or contact an organizer on a private channel.",
    "crumbs": [
      "JupyterHub",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "content/jhub.html",
    "href": "content/jhub.html",
    "title": "NMFS HackHours 2025",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub is managed by Openscapes and developed in partnership with the International Interactive Computing Collaboration 2i2c. Launched in September 2024, the NMFS Openscapes JupyterHub joins the NASA Openscapes JupyterHub in providing a curated interactive computing platform to support training in earth and life science visualization, computing and analysis. The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries.",
    "crumbs": [
      "JupyterHub Skills",
      "About the Hub"
    ]
  },
  {
    "objectID": "content/setup.html",
    "href": "content/setup.html",
    "title": "Quick Start",
    "section": "",
    "text": "For those already familiar with JupyterLab and unix.\n\nGitHub Account\nA GitHub account is required to gain access to the JupyterHub and to clone the tutorials used in the Hackhours.\nHow do I get the tutorials into the JupyterHub?\n\nYou can upload files.\nEasiest is probably cloning a repo into the hub. See the JupyterHub Skills section if you do not know how to do this.\n\n\n\nAuthenticating to GitHub\nThis is a little different on the JupyterHub. See git authentication.\nFor content that uses the NASA Earthdata repository, you will need an Earthdata Login account. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create. Please jot down your username and password, as you need to enter it in the tutorials.\n\n\nWhen done, please stop the JupyterHub\nIf you are in JupyterLab in the browser:\n\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you still have the JupyterLab tab open in your browser:\n\nGo to the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server\n\nIf you are in RStudio and you do not have the JupyterLab tab open in your browser because you closed that tab:\n\nGo to the url https://&lt;jupyterhub url&gt;/user/&lt;your username in the hub&gt;/lab/ That will open the JupyterLab tab\nFile &gt; Hub Control Panel &gt; Stop my server",
    "crumbs": [
      "JupyterHub Skills",
      "Quick Start"
    ]
  },
  {
    "objectID": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "href": "content/slides.html#enabling-analysis-in-the-cloud-using-nasa-earth-science-data",
    "title": "NASA AGU 2023 Workshop Slides",
    "section": "Enabling Analysis in the Cloud Using NASA Earth Science Data",
    "text": "Enabling Analysis in the Cloud Using NASA Earth Science Data"
  },
  {
    "objectID": "content/workshops.html",
    "href": "content/workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "The NMFS Openscapes JupyterHub supports workshops and trainings run by NOAA Fisheries. See the CoastWatch Training for all their events.\n\nOct-Dec 2024 Quarto Workshop\nOctober 25, 2024 Oceanographic Satellite and Animal Telemetry Data Training Course\nMay 17, 2024 Introduction to using earth data in the cloud for scientific workflows"
  },
  {
    "objectID": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "href": "topics-2025/2025-03-14-Boettiger/index.html#tutorials",
    "title": "Topic",
    "section": "Tutorials",
    "text": "Tutorials\nCarl will be talking about integrating LLMs into data dashboards specifically this dashboard (source code).\nUse Carl’s image\nWhen you start up the Jupyter Hub\n\nSelect Other for image\nIn the box, put rocker/binder:latest\n\nClone Carl’s demo repo\ncd ~\ngit clone https://github.com/boettiger-lab/geo-llm-r\nCarl’s tutorial\nHe went mainly through this example:\n\nhttps://github.com/boettiger-lab/geo-llm-r/blob/main/test.R\n\nchat &lt;- ellmer::chat_vllm(\n  base_url = \"https://llm.nrp-nautilus.io/\",\n  model = \"llama3\",\n  api_key = \"&lt;token&gt;\"\n)",
    "crumbs": [
      "Guest Speakers",
      "3/14 Carl Boettiger - Integrating LLMs into your data dashboards"
    ]
  },
  {
    "objectID": "topics-2025/2025-03-21-Signell/hackhours_demo.html",
    "href": "topics-2025/2025-03-21-Signell/hackhours_demo.html",
    "title": "Explore NOAA NODD CDR SST",
    "section": "",
    "text": "use Virtualizarr to create a cloud-optimized virtual dataset from multiple remote NetCDF files\nexplore the virtual dataset using Holoviz tools\ncompute in parallel using Dask\n\n\n\nimport fsspec\n\n\nfs = fsspec.filesystem(\"s3\", anon=True)\n\n\noisst_files = fs.glob(\n    \"s3://noaa-cdr-sea-surface-temp-optimum-interpolation-pds/data/v2.1/avhrr/202503/oisst-avhrr-v02r01.*.nc\"\n)\n\noisst_files = sorted([\"s3://\" + f for f in oisst_files])\n\n\nprint(len(oisst_files))\n\n\nfrom virtualizarr import open_virtual_dataset\n\n\nso = dict(anon=True)\n\n\n%%time\nvirtual_datasets = [\n    open_virtual_dataset(url, indexes={}, reader_options={\"storage_options\": so}, )\n    for url in oisst_files\n]\n\n\nimport xarray as xr\n\n\n# this Dataset wraps a bunch of virtual ManifestArray objects directly\nvirtual_ds = xr.concat(\n    virtual_datasets,\n    dim=\"time\",\n    coords=\"minimal\",\n    compat=\"override\",\n    combine_attrs=\"override\",\n)\n# cache the combined dataset pattern to disk, in this case using the existing kerchunk specification for reference files\nvirtual_ds.virtualize.to_kerchunk('combined.json', format='json')\n\n\nds = xr.open_dataset('combined.json', engine='kerchunk', backend_kwargs={'storage_options':dict(remote_options=so)}, chunks={})  # normal xarray.Dataset object, wrapping dask/numpy arrays etc.\n\n\nds\n\n\nimport hvplot.xarray\n\n\nds['sst'].nbytes/1e9\n\n\nds['sst']\n\n\nds = ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))  #.sortby('lon')\n\n\nds['sst'][0,0,:,:].hvplot()\n\n\nimport dask\n\n\ncluster_type = 'Coiled'\n\n\nif cluster_type == 'Gateway':\n    from dask_gateway import Gateway\n    gateway = Gateway()  # instantiate Dask gateway \n    options = gateway.cluster_options()\n    cluster = gateway.new_cluster(options)\n    client = cluster.get_client()\n    cluster.adapt(minimum=4, maximum=30)\n\n\nif cluster_type == 'Coiled':\n    import coiled\n    cluster = coiled.Cluster(\n        region=\"us-west-2\",\n        arm=True,   # run on ARM to save energy & cost\n        worker_vm_types=[\"t4g.small\"],  # cheap, small ARM instances, 2cpus, 2GB RAM\n        worker_options={'nthreads':2},\n        n_workers=4,\n        wait_for_workers=False,\n        compute_purchase_option=\"spot_with_fallback\",\n        name='hackhours_rps',   # Dask cluster name\n        software='hackhours-arm',  # Conda environment name\n        workspace='esip-lab',\n        timeout=180   # leave cluster running for 3 min in case we want to use it again\n    )\n\n    client = cluster.get_client()\n\n\n%%time\nso = dict(anon=True)\n\n\nvirtual_datasets = dask.compute(*[\n    dask.delayed(open_virtual_dataset)(url, indexes={}, reader_options={\"storage_options\": so}, )\n    for url in oisst_files\n])\n\n\n# this Dataset wraps a bunch of virtual ManifestArray objects directly\nvirtual_ds = xr.concat(\n    virtual_datasets,\n    dim=\"time\",\n    coords=\"minimal\",\n    compat=\"override\",\n    combine_attrs=\"override\",\n)\n# cache the combined dataset pattern to disk, in this case using the existing kerchunk specification for reference files\nvirtual_ds.virtualize.to_kerchunk('combined.json', format='json')\n\n\nurl = 'https://gist.githubusercontent.com/rsignell/dd6a8d6fafcea40dfd23dd3e887fcc1e/raw/ba366821704a648ff71aa669fe07ccf503cbfd1d/sst_combined_refs.json'\nds = xr.open_dataset(url, engine='kerchunk', backend_kwargs={'storage_options':dict(remote_options=so)}, chunks={})  # normal xarray.Dataset object, wrapping dask/numpy arrays etc.\n\n\n%%time\nda = ds['sst'][:3,0,:,:].load()\n\n\nda = ds['sst'][:,0,:,:].load()\n\n\nds"
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#overview",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#overview",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nTHREDDS is a common type of data server that usually includes multiple ways to access the data. One of those ways is via the OPeNDAP protocol which allows you to subset the data, instead of downloading the whole data file. Here you will learn to use xarray’s open_mfdataset to create data cubes on THREDDS servers using the OPeNDAP protocol. This tutorial uses an example where the server doesn’t require authentication (username and password).\nAcknowledgments: This example was adapted from Ryan Abernathy https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#ncep-ncar-reanalysis-1",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#ncep-ncar-reanalysis-1",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "NCEP-NCAR Reanalysis 1",
    "text": "NCEP-NCAR Reanalysis 1\nFor our first example, we will use 4xDaily Air temperature at sigma level 995 data (air.sig995) from the NCEP-NCAR Reanalysis 1. First look at the THREDDS catalog to orient yourself to the file naming convention. We can click on one of the files an see our [access options]. We are looking for the OPeNDAP information as we need to get the url for that. Clicking the OPeNDAP link reveals the url format of the files:\nhttps://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995.1948.nc\nNow we can create the file urls.\n\nimport xarray as xr\n\n\nbase_url = 'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995'\n\nfiles = [f'{base_url}.{year}.nc' for year in range(1948, 2025)]\nlen(files)\n\n77\n\n\nOpen a single file.\n\nfiles[0]\n\n'http://psl.noaa.gov/thredds/dodsC/Datasets/ncep.reanalysis/surface/air.sig995.1948.nc'\n\n\n\n%%time\nds = xr.open_dataset(files[0]);\nds\n\nCPU times: user 768 ms, sys: 55.9 ms, total: 824 ms\nWall time: 2.58 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 62MB\nDimensions:  (lon: 144, time: 1464, lat: 73)\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 12kB 1948-01-01 ... 1948-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nData variables:\n    air      (time, lat, lon) float32 62MB ...\nAttributes:\n    Conventions:                     COARDS\n    title:                           4x daily NMC reanalysis (1948)\n    description:                     Data is from NMC initialized reanalysis\\...\n    platform:                        Model\n    history:                         created 99/05/11 by Hoop (netCDF2.3)\\nCo...\n    dataset_title:                   NCEP-NCAR Reanalysis 1\n    References:                      http://www.psl.noaa.gov/data/gridded/dat...\n    _NCProperties:                   version=2,netcdf=4.6.3,hdf5=1.10.5\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:lon: 144time: 1464lat: 73Coordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 1948-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1948-12-31T06:00:00.000000000',\n       '1948-12-31T12:00:00.000000000', '1948-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Data variables: (1)air(time, lat, lon)float32...long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144][15389568 values with dtype=float32]Indexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1948-12-29 12:00:00', '1948-12-29 18:00:00',\n               '1948-12-30 00:00:00', '1948-12-30 06:00:00',\n               '1948-12-30 12:00:00', '1948-12-30 18:00:00',\n               '1948-12-31 00:00:00', '1948-12-31 06:00:00',\n               '1948-12-31 12:00:00', '1948-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=1464, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (9)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelhistory :created 99/05/11 by Hoop (netCDF2.3)\nConverted to chunked, deflated non-packed NetCDF4 2014/09dataset_title :NCEP-NCAR Reanalysis 1References :http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis.html_NCProperties :version=2,netcdf=4.6.3,hdf5=1.10.5DODS_EXTRA.Unlimited_Dimension :time\n\n\nWe will load the file metadata with open_mfdataset and create our virtual data cube. It will take 10 seconds or so but it doesn’t use much memory as we are only reading and loading the file metadata.\n\n%%time\nds = xr.open_mfdataset(files, parallel=True);\n\nCPU times: user 2.09 s, sys: 275 ms, total: 2.37 s\nWall time: 28.4 s\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 5GB\nDimensions:  (time: 112500, lat: 73, lon: 144)\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 900kB 1948-01-01 ... 2024-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nData variables:\n    air      (time, lat, lon) float32 5GB dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;\nAttributes:\n    Conventions:                     COARDS\n    title:                           4x daily NMC reanalysis (1948)\n    description:                     Data is from NMC initialized reanalysis\\...\n    platform:                        Model\n    history:                         created 99/05/11 by Hoop (netCDF2.3)\\nCo...\n    dataset_title:                   NCEP-NCAR Reanalysis 1\n    References:                      http://www.psl.noaa.gov/data/gridded/dat...\n    _NCProperties:                   version=2,netcdf=4.6.3,hdf5=1.10.5\n    DODS_EXTRA.Unlimited_Dimension:  timexarray.DatasetDimensions:time: 112500lat: 73lon: 144Coordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 2024-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '2024-12-31T06:00:00.000000000',\n       '2024-12-31T12:00:00.000000000', '2024-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Data variables: (1)air(time, lat, lon)float32dask.array&lt;chunksize=(1464, 73, 144), meta=np.ndarray&gt;long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n4.41 GiB\n58.71 MiB\n\n\nShape\n(112500, 73, 144)\n(1464, 73, 144)\n\n\nDask graph\n77 chunks in 155 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             144 73 112500\n\n\n\n\nIndexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '2024-12-29 12:00:00', '2024-12-29 18:00:00',\n               '2024-12-30 00:00:00', '2024-12-30 06:00:00',\n               '2024-12-30 12:00:00', '2024-12-30 18:00:00',\n               '2024-12-31 00:00:00', '2024-12-31 06:00:00',\n               '2024-12-31 12:00:00', '2024-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=112500, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (9)Conventions :COARDStitle :4x daily NMC reanalysis (1948)description :Data is from NMC initialized reanalysis\n(4x/day).  These are the 0.9950 sigma level values.platform :Modelhistory :created 99/05/11 by Hoop (netCDF2.3)\nConverted to chunked, deflated non-packed NetCDF4 2014/09dataset_title :NCEP-NCAR Reanalysis 1References :http://www.psl.noaa.gov/data/gridded/data.ncep.reanalysis.html_NCProperties :version=2,netcdf=4.6.3,hdf5=1.10.5DODS_EXTRA.Unlimited_Dimension :time\n\n\n\nds['air'].isel(time=1).plot();\n\n\n\n\n\n\n\n\nThe data set is not that huge, but it is bigger than the 2Gb RAM in our minimal Jupyter Hub.\n\nprint(f\"{ds.nbytes / 1e9} Gb\")\n\n4.731300868 Gb\n\n\nBut even a small dataset would crash our 2Gb RAM.\n\nds_sub = ds.sel(time=slice(\"1948\",\"1958\"))\nprint(f\"{ds_sub.nbytes / 1e9} Gb\")\n# ds_sub.load() # this would crash a 2Gb RAM\n\n0.6759249 Gb",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#creating-daily-means",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#creating-daily-means",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Creating daily means",
    "text": "Creating daily means\nFor one year, we can create a daily mean since 1 year is not that much data and we can fit that into memory.\n\nds_mean = ds[\"air\"].sel(time=\"1949\").mean(dim=['lat', 'lon'])\nds_mean.plot();\n\n\n\n\n\n\n\n\n\n# resample to daily\nds_mean.resample(time='D').mean().plot();\n\n\n\n\n\n\n\n\nBut if we try to do all years at once, we will run out of memory.\n\nChunking the data\nFortunately, xarray will process our data in chunks rather than loading the whole data into memory. The data were chunked automatically into yearly chunks by xarray since we created using open_mfdataset. The chunks are 1460 = 4 x 365 days since the files are one year.\n\nprint(ds[\"air\"].chunks)\n\n((1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464, 1460, 1460, 1460, 1464), (73,), (144,))\n\n\nI am going to rechunk smaller to get to about 1 Mb sized chunks. This should make it go a little faster and use a little less memory.\n\nimport dask\nds_chunk = ds[\"air\"].sel(time=slice(\"1948\", \"1958\")).chunk({'time': 24, 'lat': -1, 'lon': -1})\nds_chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'air' (time: 16072, lat: 73, lon: 144)&gt; Size: 676MB\ndask.array&lt;rechunk-merge, shape=(16072, 73, 144), dtype=float32, chunksize=(24, 73, 144), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lon      (lon) float32 576B 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n  * time     (time) datetime64[ns] 129kB 1948-01-01 ... 1958-12-31T18:00:00\n  * lat      (lat) float32 292B 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0\nAttributes: (12/13)\n    long_name:     4xDaily Air temperature at sigma level 995\n    units:         degK\n    precision:     2\n    GRIB_id:       11\n    GRIB_name:     TMP\n    var_desc:      Air temperature\n    ...            ...\n    parent_stat:   Other\n    actual_range:  [185.16 322.1 ]\n    valid_range:   [185.16 331.16]\n    dataset:       NCEP Reanalysis\n    level_desc:    0.995 sigma\n    _ChunkSizes:   [  1  73 144]xarray.DataArray'air'time: 16072lat: 73lon: 144dask.array&lt;chunksize=(24, 73, 144), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n644.49 MiB\n0.96 MiB\n\n\nShape\n(16072, 73, 144)\n(24, 73, 144)\n\n\nDask graph\n670 chunks in 157 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                             144 73 16072\n\n\n\n\nCoordinates: (3)lon(lon)float320.0 2.5 5.0 ... 352.5 355.0 357.5units :degrees_eastlong_name :Longitudeactual_range :[  0.  357.5]standard_name :longitudeaxis :Xarray([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n       350. , 352.5, 355. , 357.5], dtype=float32)time(time)datetime64[ns]1948-01-01 ... 1958-12-31T18:00:00long_name :Timedelta_t :0000-00-00 06:00:00standard_name :timeaxis :Tactual_range :[1297320. 1306098.]_ChunkSizes :1array(['1948-01-01T00:00:00.000000000', '1948-01-01T06:00:00.000000000',\n       '1948-01-01T12:00:00.000000000', ..., '1958-12-31T06:00:00.000000000',\n       '1958-12-31T12:00:00.000000000', '1958-12-31T18:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3290.0 87.5 85.0 ... -87.5 -90.0units :degrees_northactual_range :[ 90. -90.]long_name :Latitudestandard_name :latitudeaxis :Yarray([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n       -85. , -87.5, -90. ], dtype=float32)Indexes: (3)lonPandasIndexPandasIndex(Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,  22.5,\n       ...\n       335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0, 357.5],\n      dtype='float32', name='lon', length=144))timePandasIndexPandasIndex(DatetimeIndex(['1948-01-01 00:00:00', '1948-01-01 06:00:00',\n               '1948-01-01 12:00:00', '1948-01-01 18:00:00',\n               '1948-01-02 00:00:00', '1948-01-02 06:00:00',\n               '1948-01-02 12:00:00', '1948-01-02 18:00:00',\n               '1948-01-03 00:00:00', '1948-01-03 06:00:00',\n               ...\n               '1958-12-29 12:00:00', '1958-12-29 18:00:00',\n               '1958-12-30 00:00:00', '1958-12-30 06:00:00',\n               '1958-12-30 12:00:00', '1958-12-30 18:00:00',\n               '1958-12-31 00:00:00', '1958-12-31 06:00:00',\n               '1958-12-31 12:00:00', '1958-12-31 18:00:00'],\n              dtype='datetime64[ns]', name='time', length=16072, freq=None))latPandasIndexPandasIndex(Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,  67.5,\n        65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,  45.0,  42.5,\n        40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,  22.5,  20.0,  17.5,\n        15.0,  12.5,  10.0,   7.5,   5.0,   2.5,   0.0,  -2.5,  -5.0,  -7.5,\n       -10.0, -12.5, -15.0, -17.5, -20.0, -22.5, -25.0, -27.5, -30.0, -32.5,\n       -35.0, -37.5, -40.0, -42.5, -45.0, -47.5, -50.0, -52.5, -55.0, -57.5,\n       -60.0, -62.5, -65.0, -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5,\n       -85.0, -87.5, -90.0],\n      dtype='float32', name='lat'))Attributes: (13)long_name :4xDaily Air temperature at sigma level 995units :degKprecision :2GRIB_id :11GRIB_name :TMPvar_desc :Air temperaturestatistic :Individual Obsparent_stat :Otheractual_range :[185.16 322.1 ]valid_range :[185.16 331.16]dataset :NCEP Reanalysislevel_desc :0.995 sigma_ChunkSizes :[  1  73 144]\n\n\n\n# This is takes about 4 minutes; 1.4 Gb\nfrom dask.diagnostics import ProgressBar\n\nwith ProgressBar():\n    mean_all_years = ds_chunk.mean(dim=['lat', 'lon']).compute()\n\n[########################################] | 100% Completed | 218.60 s\n\n\n\nmean_all_years.plot();\n\n\n\n\n\n\n\n\n\n# resample to monthly\nmean_all_years.resample(time='ME').mean().plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#conclusion",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#conclusion",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWe practiced creating data cubes with urls to files on a THREDDS server via the OPeNDAP protocol.",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/1-ncep-ncar.html#references",
    "href": "topics-2025/2025-opendap/1-ncep-ncar.html#references",
    "title": "Creating data cubes on THREDDS servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://www.jamstec.go.jp/ridinfo/xarray-and-opendap/\nMore opendap + xarray debugging https://github.com/stuckyb/gcdl/issues/24\nAnother opendap example https://github.com/ornldaac/daymet-python-opendap-xarray/blob/master/1_daymetv4_discovery_access_subsetting.ipynb\nNice example https://rabernat.github.io/research_computing_2018/xarray-tips-and-tricks.html",
    "crumbs": [
      "Python - OPeNDAP",
      "Simple example 1 - NCEP-NCAR"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#overview",
    "href": "topics-2025/2025-opendap/3-nasa.html#overview",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nNASA OPeNDAP servers (have nasa.gov in url) require Earthdata Login (EDL) authentication and some require an End User Licence Agreement (EULA). Here is the list of NASA OPeNDAP servers. There are Hyrax, GraDS and THREDDS servers. They behave slightly differently so it is good to know which one you are using.\nIn my experience, the NASA OPeNDAP is hard to work with due to the authentication and redirect issues that arise for data that require EULAs and for other unknown reasons. To deal with this, I will show a workflow using engine=\"pydap\" and setting up an authenticated session using earthaccess. Although you can set up authentication with PyDAP, I find that tends to be fragile and only works consistently if you use an Earthdata token and do not use username/password. earthaccess automates the token part for you so you don’t have to do that manually. To see how to set up authentication with PyDAP see the extra notebook for that.\nBTW, I would try first to find the data on https://search.earthdata.nasa.gov/search and try to use earthaccess results which would point you to the non-opendap cloud links. The earthaccess workflow for creating data cubes with xarray.open_mfdataset is quite a bit easier with fewer gotchas.\n\nReferences\n\nhttps://podaac.jpl.nasa.gov/OPeNDAP-in-the-Cloud\n\n\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password. Create that file if needed.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages\n\nimport xarray as xr\nimport pydap.client",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#create-a-nasa-edl-authenticated-session",
    "href": "topics-2025/2025-opendap/3-nasa.html#create-a-nasa-edl-authenticated-session",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Create a NASA EDL authenticated session",
    "text": "Create a NASA EDL authenticated session\n\nAuthenticate with earthaccess.login() then create the session which will create the right headers when we (or pydap) does requests.\n\n\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#example-1-hyrax-server---no-eula",
    "href": "topics-2025/2025-opendap/3-nasa.html#example-1-hyrax-server---no-eula",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Example 1: Hyrax server - NO EULA",
    "text": "Example 1: Hyrax server - NO EULA\nLet’s go to this oceancolor OPeNDAP server https://oceandata.sci.gsfc.nasa.gov/opendap/. It is Hyrax, which we can see in the footer. These have both DAP4 and DAP2 protocols. With pydap, you can specify protocol=\"dap4\" so it knows which one to use. But xarray.open_dataset() doesn’t accept the protocol argument so we need to use a work around.\nWe dig into the directory to find the PACE-OCI data and then look at the L3SMI directory: https://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0301/contents.html\nWe will navigate to the PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc file in 2024/0301. Here is the url which you will see when you click on the file and get to the “OPeNDAP DAP4 Data Request Form”. This a monthly average.\nurl=\"http://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0301/PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc\"\nBut to open with xarray, we need to replace the http (or https) with dap4.\n\nLoading a single file\n\nurl=\"http://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0301/PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc\"\n\ndap4_url = url.replace(\"http\", \"dap4\", 1)\nds = xr.open_dataset(dap4_url, engine=\"pydap\", session=edl_session)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 26MB\nDimensions:  (/lat: 1800, /lon: 3600, /rgb: 3, /eightbitcolor: 256)\nDimensions without coordinates: /lat, /lon, /rgb, /eightbitcolor\nData variables:\n    lat      (/lat) float32 7kB ...\n    lon      (/lon) float32 14kB ...\n    avw      (/lat, /lon) float32 26MB ...\n    palette  (/rgb, /eightbitcolor) uint8 768B ...\nAttributes: (12/62)\n    product_name:                      PACE_OCI.20240301_20240331.L3m.MO.AVW....\n    instrument:                        OCI\n    title:                             OCI Level-3 Standard Mapped Image\n    project:                           Ocean Biology Processing Group (NASA/G...\n    platform:                          PACE\n    source:                            satellite observations from OCI-PACE\n    ...                                ...\n    cdm_data_type:                     grid\n    identifier_product_doi_authority:  http://dx.doi.org\n    identifier_product_doi:            10.5067/PACE/OCI/L3M/AVW/3.0\n    data_bins:                         Attribute elided: Unsupported attribut...\n    data_minimum:                      399.999969\n    data_maximum:                      700.000061xarray.DatasetDimensions:/lat: 1800/lon: 3600/rgb: 3/eightbitcolor: 256Coordinates: (0)Data variables: (4)lat(/lat)float32...long_name :Latitudeunits :degrees_northstandard_name :latitudevalid_min :-90.0valid_max :90.0Maps :()[1800 values with dtype=float32]lon(/lon)float32...long_name :Longitudeunits :degrees_eaststandard_name :longitudevalid_min :-180.0valid_max :180.0Maps :()[3600 values with dtype=float32]avw(/lat, /lon)float32...long_name :Apparent Visible Wavelengthunits :nmvalid_min :400.0valid_max :700.0reference :Vandermeulen, R. A., Mannino, A., Craig, S.E., Werdell, P.J., 2020: 150 shades of green: Using the full spectrum of remote sensing reflectance to elucidate color shifts in the ocean, Remote Sensing of Environment, 247, 111900, https://doi.org/10.1016/j.rse.2020.111900, https://doi.org/10.5067/KAROCHG01RYJdisplay_scale :lineardisplay_min :450.0display_max :575.0Maps :('/lat', '/lon')[6480000 values with dtype=float32]palette(/rgb, /eightbitcolor)uint8...Maps :()[768 values with dtype=uint8]Indexes: (0)Attributes: (62)product_name :PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.ncinstrument :OCItitle :OCI Level-3 Standard Mapped Imageproject :Ocean Biology Processing Group (NASA/GSFC/OBPG)platform :PACEsource :satellite observations from OCI-PACEtemporal_range :27-dayprocessing_version :3.0date_created :2025-03-06T17:06:41.000Zhistory :l3mapgen par=PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc.param l2_flag_names :ATMFAIL,LAND,HILT,HISATZEN,STRAYLIGHT,CLDICE,COCCOLITH,LOWLW,CHLWARN,CHLFAIL,NAVWARN,MAXAERITER,HISOLZEN,NAVFAIL,FILTER,HIGLINTtime_coverage_start :2024-03-05T00:08:58.000Ztime_coverage_end :2024-04-01T02:24:44.000Zstart_orbit_number :0end_orbit_number :0map_projection :Equidistant Cylindricallatitude_units :degrees_northlongitude_units :degrees_eastnorthernmost_latitude :90.0southernmost_latitude :-90.0westernmost_longitude :-180.0easternmost_longitude :180.0geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lon_max :180.0geospatial_lon_min :-180.0latitude_step :0.100000001longitude_step :0.100000001sw_point_latitude :-89.9499969sw_point_longitude :-179.949997spatialResolution :11.131949 kmgeospatial_lon_resolution :11.131949 kmgeospatial_lat_resolution :11.131949 kmgeospatial_lat_units :degrees_northgeospatial_lon_units :degrees_eastnumber_of_lines :1800number_of_columns :3600measure :Meansuggested_image_scaling_minimum :450.0suggested_image_scaling_maximum :575.0suggested_image_scaling_type :LINEARsuggested_image_scaling_applied :No_lastModified :2025-03-06T17:06:41.000ZConventions :CF-1.6 ACDD-1.3institution :NASA Goddard Space Flight Center, Ocean Ecology Laboratory, Ocean Biology Processing Groupstandard_name_vocabulary :CF Standard Name Table v36naming_authority :gov.nasa.gsfc.sci.oceandataid :3.0/L3/PACE_OCI.20240301_20240331.L3b.MO.AVW.V3_0.nclicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/creator_name :NASA/GSFC/OBPGpublisher_name :NASA/GSFC/OBPGcreator_email :data@oceancolor.gsfc.nasa.govpublisher_email :data@oceancolor.gsfc.nasa.govcreator_url :https://oceandata.sci.gsfc.nasa.govpublisher_url :https://oceandata.sci.gsfc.nasa.govprocessing_level :L3 Mappedcdm_data_type :grididentifier_product_doi_authority :http://dx.doi.orgidentifier_product_doi :10.5067/PACE/OCI/L3M/AVW/3.0data_bins :Attribute elided: Unsupported attribute type (NC_INT64)data_minimum :399.999969data_maximum :700.000061\n\n\nWe can plot. Note the indices have slashes which we will want to fix and the lat/lon need to be associated with the values in the lat/lon variable, but we are able to create our xarray dataset so we can now work with it with standard syntax and clean it up.\nHere a do a plot. Note I did not rename the indices to get rid of the slashes, so I have to use dict format for my .sel command.\n\nds[\"avw\"].sel({\"/lat\": slice(400, 1600), \"/lon\": slice(500, 2500)}).plot();\n\n\n\n\n\n\n\n\n\n\nOpening multiple files\nLet’s combine 3 months.\n\nmar = \"http://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0301/PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc\"\napr = \"http://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0401/PACE_OCI.20240401_20240430.L3m.MO.AVW.V3_0.avw.0p1deg.nc\"\nmay = \"http://oceandata.sci.gsfc.nasa.gov/opendap/PACE_OCI/L3SMI/2024/0501/PACE_OCI.20240501_20240531.L3m.MO.AVW.V3_0.avw.0p1deg.nc\"\nurls = [mar, apr, may]\n\n\ndap4_urls = [url.replace(\"http\", \"dap4\", 1) for url in urls]\nds = xr.open_mfdataset(\n    dap4_urls, engine=\"pydap\",\n    combine='nested', concat_dim=\"/time\", \n    session=edl_session)\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 78MB\nDimensions:  (/time: 3, /lat: 1800, /lon: 3600, /rgb: 3, /eightbitcolor: 256)\nDimensions without coordinates: /time, /lat, /lon, /rgb, /eightbitcolor\nData variables:\n    lat      (/time, /lat) float32 22kB dask.array&lt;chunksize=(1, 1800), meta=np.ndarray&gt;\n    lon      (/time, /lon) float32 43kB dask.array&lt;chunksize=(1, 3600), meta=np.ndarray&gt;\n    avw      (/time, /lat, /lon) float32 78MB dask.array&lt;chunksize=(1, 1800, 3600), meta=np.ndarray&gt;\n    palette  (/time, /rgb, /eightbitcolor) uint8 2kB dask.array&lt;chunksize=(1, 3, 256), meta=np.ndarray&gt;\nAttributes: (12/62)\n    product_name:                      PACE_OCI.20240301_20240331.L3m.MO.AVW....\n    instrument:                        OCI\n    title:                             OCI Level-3 Standard Mapped Image\n    project:                           Ocean Biology Processing Group (NASA/G...\n    platform:                          PACE\n    source:                            satellite observations from OCI-PACE\n    ...                                ...\n    cdm_data_type:                     grid\n    identifier_product_doi_authority:  http://dx.doi.org\n    identifier_product_doi:            10.5067/PACE/OCI/L3M/AVW/3.0\n    data_bins:                         Attribute elided: Unsupported attribut...\n    data_minimum:                      399.999969\n    data_maximum:                      700.000061xarray.DatasetDimensions:/time: 3/lat: 1800/lon: 3600/rgb: 3/eightbitcolor: 256Coordinates: (0)Data variables: (4)lat(/time, /lat)float32dask.array&lt;chunksize=(1, 1800), meta=np.ndarray&gt;long_name :Latitudeunits :degrees_northstandard_name :latitudevalid_min :-90.0valid_max :90.0Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n21.09 kiB\n7.03 kiB\n\n\nShape\n(3, 1800)\n(1, 1800)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           1800 3\n\n\n\n\nlon(/time, /lon)float32dask.array&lt;chunksize=(1, 3600), meta=np.ndarray&gt;long_name :Longitudeunits :degrees_eaststandard_name :longitudevalid_min :-180.0valid_max :180.0Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n42.19 kiB\n14.06 kiB\n\n\nShape\n(3, 3600)\n(1, 3600)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           3600 3\n\n\n\n\navw(/time, /lat, /lon)float32dask.array&lt;chunksize=(1, 1800, 3600), meta=np.ndarray&gt;long_name :Apparent Visible Wavelengthunits :nmvalid_min :400.0valid_max :700.0reference :Vandermeulen, R. A., Mannino, A., Craig, S.E., Werdell, P.J., 2020: 150 shades of green: Using the full spectrum of remote sensing reflectance to elucidate color shifts in the ocean, Remote Sensing of Environment, 247, 111900, https://doi.org/10.1016/j.rse.2020.111900, https://doi.org/10.5067/KAROCHG01RYJdisplay_scale :lineardisplay_min :450.0display_max :575.0Maps :('/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n74.16 MiB\n24.72 MiB\n\n\nShape\n(3, 1800, 3600)\n(1, 1800, 3600)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                             3600 1800 3\n\n\n\n\npalette(/time, /rgb, /eightbitcolor)uint8dask.array&lt;chunksize=(1, 3, 256), meta=np.ndarray&gt;Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n2.25 kiB\n768 B\n\n\nShape\n(3, 3, 256)\n(1, 3, 256)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nuint8 numpy.ndarray\n\n\n\n\n                             256 3 3\n\n\n\n\nIndexes: (0)Attributes: (62)product_name :PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.ncinstrument :OCItitle :OCI Level-3 Standard Mapped Imageproject :Ocean Biology Processing Group (NASA/GSFC/OBPG)platform :PACEsource :satellite observations from OCI-PACEtemporal_range :27-dayprocessing_version :3.0date_created :2025-03-06T17:06:41.000Zhistory :l3mapgen par=PACE_OCI.20240301_20240331.L3m.MO.AVW.V3_0.avw.0p1deg.nc.param l2_flag_names :ATMFAIL,LAND,HILT,HISATZEN,STRAYLIGHT,CLDICE,COCCOLITH,LOWLW,CHLWARN,CHLFAIL,NAVWARN,MAXAERITER,HISOLZEN,NAVFAIL,FILTER,HIGLINTtime_coverage_start :2024-03-05T00:08:58.000Ztime_coverage_end :2024-04-01T02:24:44.000Zstart_orbit_number :0end_orbit_number :0map_projection :Equidistant Cylindricallatitude_units :degrees_northlongitude_units :degrees_eastnorthernmost_latitude :90.0southernmost_latitude :-90.0westernmost_longitude :-180.0easternmost_longitude :180.0geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lon_max :180.0geospatial_lon_min :-180.0latitude_step :0.100000001longitude_step :0.100000001sw_point_latitude :-89.9499969sw_point_longitude :-179.949997spatialResolution :11.131949 kmgeospatial_lon_resolution :11.131949 kmgeospatial_lat_resolution :11.131949 kmgeospatial_lat_units :degrees_northgeospatial_lon_units :degrees_eastnumber_of_lines :1800number_of_columns :3600measure :Meansuggested_image_scaling_minimum :450.0suggested_image_scaling_maximum :575.0suggested_image_scaling_type :LINEARsuggested_image_scaling_applied :No_lastModified :2025-03-06T17:06:41.000ZConventions :CF-1.6 ACDD-1.3institution :NASA Goddard Space Flight Center, Ocean Ecology Laboratory, Ocean Biology Processing Groupstandard_name_vocabulary :CF Standard Name Table v36naming_authority :gov.nasa.gsfc.sci.oceandataid :3.0/L3/PACE_OCI.20240301_20240331.L3b.MO.AVW.V3_0.nclicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/creator_name :NASA/GSFC/OBPGpublisher_name :NASA/GSFC/OBPGcreator_email :data@oceancolor.gsfc.nasa.govpublisher_email :data@oceancolor.gsfc.nasa.govcreator_url :https://oceandata.sci.gsfc.nasa.govpublisher_url :https://oceandata.sci.gsfc.nasa.govprocessing_level :L3 Mappedcdm_data_type :grididentifier_product_doi_authority :http://dx.doi.orgidentifier_product_doi :10.5067/PACE/OCI/L3M/AVW/3.0data_bins :Attribute elided: Unsupported attribute type (NC_INT64)data_minimum :399.999969data_maximum :700.000061\n\n\n\n\nPlot mean AVW over all longitudes (global) by latitude and month\n\nlat_mean = ds[\"avw\"].sel({\"/lat\": slice(400, 1600), \"/lon\": slice(500, 2500)}).mean(dim=[\"/lon\"])\n\n\n# Define custom labels for each time step\ncustom_time_labels = [\"Mar\", \"Apr\", \"May\"]  # Adjust based on actual time steps\n\n# Assign new labels to the time coordinate\nlat_mean = lat_mean.assign_coords({\"/time\": custom_time_labels})\n\n# Plot\nlat_mean.plot.line(x=\"/lat\");",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#example-2-get-url-via-cmr-search",
    "href": "topics-2025/2025-opendap/3-nasa.html#example-2-get-url-via-cmr-search",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Example 2: Get URL via CMR search",
    "text": "Example 2: Get URL via CMR search\nWe are getting 6 hours of data.\n\nimport requests\n\n# Define variables\n# Get the concept_id from https://search.earthdata.nasa.gov/\nconcept_id = \"C2036877806-POCLOUD\"\nstart_time = \"2022-01-01T00:00:00Z\"\nend_time = \"2022-01-01T06:00:00Z\"\n\n# Format the CMR search URL with variables\nurl = f\"https://cmr.earthdata.nasa.gov/search/granules.umm_json?collection_concept_id={concept_id}&temporal={start_time},{end_time}&pageSize=365\"\n\n# Make the request\nr = requests.get(url)\nresponse_body = r.json()\n\n# Print response status\nprint(r.status_code)  # Should be 200 if successful\n\n200\n\n\nNow find the urls with opendap in them. Those are the ones we want.\n\nod_files = []\nfor itm in response_body['items']:\n    for urls in itm['umm']['RelatedUrls']:\n        if 'OPeNDAP' in urls['Description']:\n            od_files.append(urls['URL'])\n\nod_files\n\n['https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101000000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_000000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101010000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_010000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101020000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_020000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101030000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_030000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101040000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_040000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101050000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_050000-v02.0-fv01.0',\n 'https://opendap.earthdata.nasa.gov/collections/C2036877806-POCLOUD/granules/20220101060000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_060000-v02.0-fv01.0']\n\n\n\n# make sure we have an authenticated session\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()\n\nRead in the files and combine using the “dap4” replacement again. If you go to &lt;opendap.earthdata.nasa.gov&gt;, you will see that it is Hyrax and we need “dap4” for that. The file complains about decode_cf so I set that.\n\n%%time\ndap4_urls = [url.replace(\"https\", \"dap4\", 1) for url in od_files]\nds = xr.open_mfdataset(\n    dap4_urls, engine=\"pydap\",\n    combine='nested', concat_dim=\"/time\",\n    decode_cf=False,\n    session=edl_session)\n\nCPU times: user 750 ms, sys: 61 ms, total: 811 ms\nWall time: 15.4 s\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 1GB\nDimensions:                    (/time: 7, /lon: 2400, /lat: 2400)\nDimensions without coordinates: /time, /lon, /lat\nData variables: (12/19)\n    time                       (/time) int32 28B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\n    lon                        (/time, /lon) float32 67kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    lat                        (/time, /lat) float32 67kB dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;\n    wind_speed                 (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sses_standard_deviation    (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sst_dtime                  (/time, /lat, /lon) int32 161MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    ...                         ...\n    dt_analysis                (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    satellite_zenith_angle     (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    or_longitude               (/time, /lat, /lon) int16 81MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    adi_dtime_from_sst         (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    or_latitude                (/time, /lat, /lon) int16 81MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\n    sses_bias                  (/time, /lat, /lon) int8 40MB dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;\nAttributes: (12/53)\n    Conventions:                CF-1.4\n    title:                      Sea Surface Temperature\n    summary:                    The L3C product derived from GOES16/ABI brigh...\n    references:                 Geostationary Sea Surface Temperature Product...\n    institution:                OSISAF\n    comment:                    None\n    ...                         ...\n    netcdf_version_id:          4.6.3\n    build_dmrpp:                3.20.13-664\n    bes:                        3.20.13-664\n    libdap:                     libdap-3.20.11-198\n    configuration:              \\n# TheBESKeys::get_as_config()\\nAllowedHosts...\n    invocation:                 build_dmrpp -c /tmp/bes_conf_7rdf -f /tmp/tmp...xarray.DatasetDimensions:/time: 7/lon: 2400/lat: 2400Coordinates: (0)Data variables: (19)time(/time)int32dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :reference time of sst filestandard_name :timeaxis :Tunits :seconds since 1981-01-01 00:00:00Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n28 B\n4 B\n\n\nShape\n(7,)\n(1,)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n               7 1\n\n\n\n\nlon(/time, /lon)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n65.62 kiB\n9.38 kiB\n\n\nShape\n(7, 2400)\n(1, 2400)\n\n\nDask graph\n7 chunks in 22 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n               2400 7\n\n\n\n\nlat(/time, /lat)float32dask.array&lt;chunksize=(1, 2400), meta=np.ndarray&gt;long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0comment :geographical coordinates, WGS84 projectionMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n65.62 kiB\n9.38 kiB\n\n\nShape\n(7, 2400)\n(1, 2400)\n\n\nDask graph\n7 chunks in 22 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n               2400 7\n\n\n\n\nwind_speed(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :10m wind speedstandard_name :wind_speedunits :m s-1height :10 madd_offset :0.0scale_factor :1.0valid_min :0valid_max :127time_offset :0.0source :WSP-ECMWF-Forecastcomment :These wind speeds were created by the ECMWF and represent winds at 10 metres above the sea surface. Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsses_standard_deviation(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES standard deviationunits :kelvinadd_offset :1.0scale_factor :0.01valid_min :-127valid_max :127comment :Standard deviation estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsst_dtime(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-2147483648long_name :time difference from reference timeunits :secondsadd_offset :0.0scale_factor :1.0valid_min :-2147483647valid_max :2147483647comment :time plus sst_dtime gives seconds after 00:00:00 UTC January 1, 1981Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n153.81 MiB\n21.97 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsolar_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :solar zenith angleunits :angular_degreeadd_offset :90.0scale_factor :1.0valid_min :-90valid_max :90comment :The solar zenith angle at the time of the SST observations.Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsea_ice_fraction(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sea ice fractionstandard_name :sea_ice_area_fractionunits :Noneadd_offset :0.0scale_factor :0.01valid_min :0valid_max :100time_offset :0.0source :ICE-OSISAFcomment :Fractional sea ice cover from OSISAF ice productMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nl2p_flags(/time, /lat, /lon)int32dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;long_name :L2P flagsvalid_min :0valid_max :15flag_meanings :microwave land ice lakeflag_masks :[1, 2, 4, 8]comment :These flags are important to properly use the data.Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n153.81 MiB\n21.97 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsources_of_adi(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :sources of aerosol dynamic indicatorvalid_min :0valid_max :2flag_meanings :no_data AOD-NAAPS-ADI SDI-OSISAF-ADIflag_values :[0, 1, 2]comment :This variable provides a pixel by pixel description of where aerosol optical depth were derived from.Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\naerosol_dynamic_indicator(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :aerosol dynamic indicatorunits :Noneadd_offset :0.0scale_factor :0.1valid_min :0valid_max :127source :sources_of_adicomment :NoneMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsea_surface_temperature(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :sea surface subskin temperaturestandard_name :sea_surface_subskin_temperatureunits :kelvinadd_offset :273.15scale_factor :0.01valid_min :-300valid_max :4500depth :1 millimetersource :GOES_Imagercomment :Temperature of the subskin of the oceanMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n76.90 MiB\n10.99 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nquality_level(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :quality level of SST pixelvalid_min :0valid_max :5flag_meanings :no_data bad_data worst_quality low_quality acceptable_quality best_qualityflag_values :[0, 1, 2, 3, 4, 5]comment :These are the overall quality indicators and are used for all GHRSST SSTsMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\ndt_analysis(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :deviation from SST analysis or reference climatologyunits :kelvinadd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127reference :OSTIAcomment :The difference between this SST and the previous day's SST analysisMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsatellite_zenith_angle(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :satellite zenith angleunits :angular_degreeadd_offset :0.0scale_factor :1.0valid_min :-90valid_max :90comment :The satellite zenith angle at the time of the SST observations.Maps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nor_longitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original longitude of the SST valuestandard_name :longitudeunits :degrees_eastadd_offset :0.0scale_factor :0.01valid_min :-18000valid_max :18000comment :Original longitude of the SST valueMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n76.90 MiB\n10.99 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nadi_dtime_from_sst(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :time difference of ADI data from sst measurementunits :houradd_offset :0.0scale_factor :0.1valid_min :-127valid_max :127comment :Difference in hours between the ADI and SST dataMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nor_latitude(/time, /lat, /lon)int16dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-32768long_name :original latitude of the SST valuestandard_name :latitudeunits :degrees_northadd_offset :0.0scale_factor :0.01valid_min :-9000valid_max :9000comment :Original latitude of the SST valueMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n76.90 MiB\n10.99 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint16 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nsses_bias(/time, /lat, /lon)int8dask.array&lt;chunksize=(1, 2400, 2400), meta=np.ndarray&gt;_FillValue :-128long_name :SSES bias estimateunits :kelvinadd_offset :0.0scale_factor :0.01valid_min :-127valid_max :127comment :Bias estimate derived using the techniques described at http://www.ghrsst.org/SSES-Description-of-schemes.htmlMaps :('/time', '/lat', '/lon')\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.45 MiB\n5.49 MiB\n\n\nShape\n(7, 2400, 2400)\n(1, 2400, 2400)\n\n\nDask graph\n7 chunks in 15 graph layers\n\n\nData type\nint8 numpy.ndarray\n\n\n\n\n                                     2400 2400 7\n\n\n\n\nIndexes: (0)Attributes: (53)Conventions :CF-1.4title :Sea Surface Temperaturesummary :The L3C product derived from GOES16/ABI brightness temperatures.references :Geostationary Sea Surface Temperature Product User Manual, http://www.osi-saf.orginstitution :OSISAFcomment :Nonelicense :All intellectual property rights of the Ocean & Sea Ice SAF products belong to EUMETSAT. The use of these products is granted to every user, free of charge. If users wish to use these products, EUMETSAT's copyright credit must be shown by displaying the words 'Copyright EUMETSAT' under each of the products shown. EUMETSAT offers no warranty and accepts no liability in respect of the Ocean & Sea Ice SAF products. EUMETSAT neither commits to nor guarantees the continuity, availability, or quality or suitability for any purpose of, the Ocean & Sea Ice SAF products.id :GOES16-OSISAF-L3C-v1.0product_id :OSI-207-bnaming_authority :org.ghrsstproduct_version :1.0gds_version_id :2.0file_quality_level :3spatial_resolution :0.05 degreenorthernmost_latitude :60.0southernmost_latitude :-60.0easternmost_longitude :-15.0westernmost_longitude :-135.0source :GOES_ABIplatform :GOES16sensor :GOES_ABIMetadata_Conventions :Unidata Dataset Discovery v1.0metadata_link :N/Akeywords :Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature keywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventiongeospatial_lat_units :degrees_northgeospatial_lat_resolution :0.05geospatial_lon_units :degrees_eastgeospatial_lon_resolution :0.05acknowledgment :In case SAF data (pre-operational or operational) has been used for the study described in a paper the following sentence would be an appropriate reference to the funding coming from EUMETSAT: The data from the EUMETSAT Satellite Application Facility on Ocean & Sea Ice  used in this study are accessible through the SAF's homepage http://www.osi-saf.orgcreator_name :O&SI SAFcreator_email :osi-saf.helpdesk@meteo.frcreator_url :http://www.osi-saf.orgproject :Group for High Resolution Sea Surface Temperaturepublisher_name :The GHRSST Project Officepublisher_url :http://www.ghrsst.orgpublisher_email :ghrsst-po@nceo.ac.ukprocessing_level :L3Ccdm_data_type :gridhistory :METEO-FRANCE GEOSAFO v1.1.2uuid :A102CA8C-6A9D-11EC-AFC0-48DF370DAD00date_created :20220101T005618Zstart_time :20211231T234044Ztime_coverage_start :20211231T234044Zstop_time :20220101T001929Ztime_coverage_end :20220101T001929Znetcdf_version_id :4.6.3build_dmrpp :3.20.13-664bes :3.20.13-664libdap :libdap-3.20.11-198configuration :\n# TheBESKeys::get_as_config()\nAllowedHosts=^https?:\\/\\/\nBES.Catalog.catalog.FollowSymLinks=Yes\nBES.Catalog.catalog.RootDirectory=/tmp/tmp_uifhokl/\nBES.Catalog.catalog.TypeMatch=dmrpp:.*\\.(dmrpp)$;\nBES.Catalog.catalog.TypeMatch+=h5:.*(\\.bz2|\\.gz|\\.Z)?$;\nBES.Data.RootDirectory=/dev/null\nBES.LogName=./bes.log\nBES.UncompressCache.dir=/tmp/hyrax_ux\nBES.UncompressCache.prefix=ux_\nBES.UncompressCache.size=500\nBES.module.cmd=/usr/lib64/bes/libdap_xml_module.so\nBES.module.dap=/usr/lib64/bes/libdap_module.so\nBES.module.dmrpp=/usr/lib64/bes/libdmrpp_module.so\nBES.module.fonc=/usr/lib64/bes/libfonc_module.so\nBES.module.h5=/usr/lib64/bes/libhdf5_module.so\nBES.module.nc=/usr/lib64/bes/libnc_module.so\nBES.modules=dap,cmd,h5,dmrpp,nc,fonc\nFONc.ClassicModel=false\nFONc.NoGlobalAttrs=true\nH5.EnableCF=false\nH5.EnableCheckNameClashing=true\ninvocation :build_dmrpp -c /tmp/bes_conf_7rdf -f /tmp/tmp_uifhokl//20220101000000-OSISAF-L3C_GHRSST-SSTsubskin-GOES16-ssteqc_goes16_20220101_000000-v02.0-fv01.0.nc -r /tmp/dmr__7lvB4A -u OPeNDAP_DMRpp_DATA_ACCESS_URL -M\n\n\n\nds[\"wind_speed\"].isel({\"/time\": 5}).plot();\n\n/srv/conda/envs/notebook/lib/python3.12/site-packages/xarray/plot/utils.py:260: RuntimeWarning: overflow encountered in scalar absolute\n  vlim = max(abs(vmin - center), abs(vmax - center))",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#example-3-data-with-a-eula---grads-server",
    "href": "topics-2025/2025-opendap/3-nasa.html#example-3-data-with-a-eula---grads-server",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Example 3: Data with a EULA - GrADS server",
    "text": "Example 3: Data with a EULA - GrADS server\n\nPrerequisites\nMake sure you have the GESDISC EULA accepted.\n\nLog into https://urs.earthdata.nasa.gov\nThen go here https://urs.earthdata.nasa.gov/profile\nThen click EULAs\nGo to unaccepted EULAs and make sure that GESDISC is accepted\n\n\n# the GESDISC data requires a EULA\neula_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\n\n\n# make sure we have an authenticated session\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()\n\n\n# this is NOT Hyrax server. Use the url as is.\nds = xr.open_dataset(eula_url, engine=\"pydap\", session=edl_session)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (time: 24, lat: 361, lon: 576)\nCoordinates:\n  * lat       (lat) float64 3kB -90.0 -89.5 -89.0 -88.5 ... 88.5 89.0 89.5 90.0\n  * lon       (lon) float64 5kB -180.0 -179.4 -178.8 ... 178.1 178.8 179.4\n  * time      (time) datetime64[ns] 192B 2016-06-01T00:30:00 ... 2016-06-01T2...\nData variables: (12/47)\n    U2M       (time, lat, lon) float64 40MB ...\n    V250      (time, lat, lon) float64 40MB ...\n    TROPT     (time, lat, lon) float64 40MB ...\n    TROPPB    (time, lat, lon) float64 40MB ...\n    T2M       (time, lat, lon) float64 40MB ...\n    TQL       (time, lat, lon) float64 40MB ...\n    ...        ...\n    TROPPV    (time, lat, lon) float64 40MB ...\n    H500      (time, lat, lon) float64 40MB ...\n    V500      (time, lat, lon) float64 40MB ...\n    T2MWET    (time, lat, lon) float64 40MB ...\n    U500      (time, lat, lon) float64 40MB ...\n    QV10M     (time, lat, lon) float64 40MB ...\nAttributes: (12/31)\n    DODS_EXTRA.Unlimited_Dimension:    time\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    ...                                ...\n    Contact:                           http://gmao.gsfc.nasa.gov\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000xarray.DatasetDimensions:time: 24lat: 361lon: 576Coordinates: (3)lat(lat)float64-90.0 -89.5 -89.0 ... 89.5 90.0long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latarray([-90. , -89.5, -89. , ...,  89. ,  89.5,  90. ])lon(lon)float64-180.0 -179.4 ... 178.8 179.4long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/lonarray([-180.   , -179.375, -178.75 , ...,  178.125,  178.75 ,  179.375])time(time)datetime64[ns]2016-06-01T00:30:00 ... 2016-06-...long_name :timetime_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timearray(['2016-06-01T00:30:00.000000000', '2016-06-01T01:30:00.000000000',\n       '2016-06-01T02:30:00.000000000', '2016-06-01T03:30:00.000000000',\n       '2016-06-01T04:30:00.000000000', '2016-06-01T05:30:00.000000000',\n       '2016-06-01T06:30:00.000000000', '2016-06-01T07:30:00.000000000',\n       '2016-06-01T08:30:00.000000000', '2016-06-01T09:30:00.000000000',\n       '2016-06-01T10:30:00.000000000', '2016-06-01T11:30:00.000000000',\n       '2016-06-01T12:30:00.000000000', '2016-06-01T13:30:00.000000000',\n       '2016-06-01T14:30:00.000000000', '2016-06-01T15:30:00.000000000',\n       '2016-06-01T16:30:00.000000000', '2016-06-01T17:30:00.000000000',\n       '2016-06-01T18:30:00.000000000', '2016-06-01T19:30:00.000000000',\n       '2016-06-01T20:30:00.000000000', '2016-06-01T21:30:00.000000000',\n       '2016-06-01T22:30:00.000000000', '2016-06-01T23:30:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (47)U2M(time, lat, lon)float64...long_name :2-meter_eastward_windunits :m s-1fmissing_value :999999987000000.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2M[4990464 values with dtype=float64]V250(time, lat, lon)float64...long_name :northward_wind_at_250_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250[4990464 values with dtype=float64]TROPT(time, lat, lon)float64...long_name :tropopause_temperature_using_blended_TROPP_estimateunits :Kfmissing_value :999999987000000.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPT[4990464 values with dtype=float64]TROPPB(time, lat, lon)float64...long_name :tropopause_pressure_based_on_blended_estimateunits :Pafmissing_value :999999987000000.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPB[4990464 values with dtype=float64]T2M(time, lat, lon)float64...long_name :2-meter_air_temperatureunits :Kfmissing_value :999999987000000.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2M[4990464 values with dtype=float64]TQL(time, lat, lon)float64...long_name :total_precipitable_liquid_waterunits :kg m-2fmissing_value :999999987000000.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQL[4990464 values with dtype=float64]T500(time, lat, lon)float64...long_name :air_temperature_at_500_hPaunits :Kfmissing_value :999999987000000.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500[4990464 values with dtype=float64]TOX(time, lat, lon)float64...long_name :total_column_odd_oxygenunits :kg m-2fmissing_value :999999987000000.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOX[4990464 values with dtype=float64]U850(time, lat, lon)float64...long_name :eastward_wind_at_850_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850[4990464 values with dtype=float64]PS(time, lat, lon)float64...long_name :surface_pressureunits :Pafmissing_value :999999987000000.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PS[4990464 values with dtype=float64]V850(time, lat, lon)float64...long_name :northward_wind_at_850_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850[4990464 values with dtype=float64]OMEGA500(time, lat, lon)float64...long_name :omega_at_500_hPaunits :Pa s-1fmissing_value :999999987000000.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500[4990464 values with dtype=float64]H250(time, lat, lon)float64...long_name :height_at_250_hPaunits :mfmissing_value :999999987000000.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250[4990464 values with dtype=float64]Q250(time, lat, lon)float64...long_name :specific_humidity_at_250_hPaunits :kg kg-1fmissing_value :999999987000000.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250[4990464 values with dtype=float64]T2MDEW(time, lat, lon)float64...long_name :dew_point_temperature_at_2_munits :Kfmissing_value :999999987000000.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEW[4990464 values with dtype=float64]PBLTOP(time, lat, lon)float64...long_name :pbltop_pressureunits :Pafmissing_value :999999987000000.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOP[4990464 values with dtype=float64]CLDPRS(time, lat, lon)float64...long_name :cloud_top_pressureunits :Pafmissing_value :999999987000000.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRS[4990464 values with dtype=float64]V50M(time, lat, lon)float64...long_name :northward_wind_at_50_metersunits :m s-1fmissing_value :999999987000000.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50M[4990464 values with dtype=float64]Q500(time, lat, lon)float64...long_name :specific_humidity_at_500_hPaunits :kg kg-1fmissing_value :999999987000000.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500[4990464 values with dtype=float64]DISPH(time, lat, lon)float64...long_name :zero_plane_displacement_heightunits :mfmissing_value :999999987000000.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPH[4990464 values with dtype=float64]H1000(time, lat, lon)float64...long_name :height_at_1000_mbunits :mfmissing_value :999999987000000.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000[4990464 values with dtype=float64]TO3(time, lat, lon)float64...long_name :total_column_ozoneunits :Dobsonsfmissing_value :999999987000000.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3[4990464 values with dtype=float64]TS(time, lat, lon)float64...long_name :surface_skin_temperatureunits :Kfmissing_value :999999987000000.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TS[4990464 values with dtype=float64]T10M(time, lat, lon)float64...long_name :10-meter_air_temperatureunits :Kfmissing_value :999999987000000.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10M[4990464 values with dtype=float64]TROPPT(time, lat, lon)float64...long_name :tropopause_pressure_based_on_thermal_estimateunits :Pafmissing_value :999999987000000.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPT[4990464 values with dtype=float64]TQI(time, lat, lon)float64...long_name :total_precipitable_ice_waterunits :kg m-2fmissing_value :999999987000000.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQI[4990464 values with dtype=float64]SLP(time, lat, lon)float64...long_name :sea_level_pressureunits :Pafmissing_value :999999987000000.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLP[4990464 values with dtype=float64]U250(time, lat, lon)float64...long_name :eastward_wind_at_250_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250[4990464 values with dtype=float64]Q850(time, lat, lon)float64...long_name :specific_humidity_at_850_hPaunits :kg kg-1fmissing_value :999999987000000.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850[4990464 values with dtype=float64]ZLCL(time, lat, lon)float64...long_name :lifting_condensation_levelunits :mfmissing_value :999999987000000.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCL[4990464 values with dtype=float64]TQV(time, lat, lon)float64...long_name :total_precipitable_water_vaporunits :kg m-2fmissing_value :999999987000000.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQV[4990464 values with dtype=float64]V2M(time, lat, lon)float64...long_name :2-meter_northward_windunits :m s-1fmissing_value :999999987000000.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2M[4990464 values with dtype=float64]T250(time, lat, lon)float64...long_name :air_temperature_at_250_hPaunits :Kfmissing_value :999999987000000.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250[4990464 values with dtype=float64]TROPQ(time, lat, lon)float64...long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1fmissing_value :999999987000000.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQ[4990464 values with dtype=float64]V10M(time, lat, lon)float64...long_name :10-meter_northward_windunits :m s-1fmissing_value :999999987000000.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10M[4990464 values with dtype=float64]H850(time, lat, lon)float64...long_name :height_at_850_hPaunits :mfmissing_value :999999987000000.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850[4990464 values with dtype=float64]T850(time, lat, lon)float64...long_name :air_temperature_at_850_hPaunits :Kfmissing_value :999999987000000.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850[4990464 values with dtype=float64]U50M(time, lat, lon)float64...long_name :eastward_wind_at_50_metersunits :m s-1fmissing_value :999999987000000.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50M[4990464 values with dtype=float64]U10M(time, lat, lon)float64...long_name :10-meter_eastward_windunits :m s-1fmissing_value :999999987000000.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10M[4990464 values with dtype=float64]QV2M(time, lat, lon)float64...long_name :2-meter_specific_humidityunits :kg kg-1fmissing_value :999999987000000.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2M[4990464 values with dtype=float64]CLDTMP(time, lat, lon)float64...long_name :cloud_top_temperatureunits :Kfmissing_value :999999987000000.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMP[4990464 values with dtype=float64]TROPPV(time, lat, lon)float64...long_name :tropopause_pressure_based_on_EPV_estimateunits :Pafmissing_value :999999987000000.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPV[4990464 values with dtype=float64]H500(time, lat, lon)float64...long_name :height_at_500_hPaunits :mfmissing_value :999999987000000.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500[4990464 values with dtype=float64]V500(time, lat, lon)float64...long_name :northward_wind_at_500_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500[4990464 values with dtype=float64]T2MWET(time, lat, lon)float64...long_name :wet_bulb_temperature_at_2_munits :Kfmissing_value :999999987000000.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWET[4990464 values with dtype=float64]U500(time, lat, lon)float64...long_name :eastward_wind_at_500_hPaunits :m s-1fmissing_value :999999987000000.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500[4990464 values with dtype=float64]QV10M(time, lat, lon)float64...long_name :10-meter_specific_humidityunits :kg kg-1fmissing_value :999999987000000.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10M[4990464 values with dtype=float64]Indexes: (3)latPandasIndexPandasIndex(Index([-90.0, -89.5, -89.0, -88.5, -88.0, -87.5, -87.0, -86.5, -86.0, -85.5,\n       ...\n        85.5,  86.0,  86.5,  87.0,  87.5,  88.0,  88.5,  89.0,  89.5,  90.0],\n      dtype='float64', name='lat', length=361))lonPandasIndexPandasIndex(Index([  -180.0, -179.375,  -178.75, -178.125,   -177.5, -176.875,  -176.25,\n       -175.625,   -175.0, -174.375,\n       ...\n         173.75,  174.375,    175.0,  175.625,   176.25,  176.875,    177.5,\n        178.125,   178.75,  179.375],\n      dtype='float64', name='lon', length=576))timePandasIndexPandasIndex(DatetimeIndex(['2016-06-01 00:30:00', '2016-06-01 01:30:00',\n               '2016-06-01 02:30:00', '2016-06-01 03:30:00',\n               '2016-06-01 04:30:00', '2016-06-01 05:30:00',\n               '2016-06-01 06:30:00', '2016-06-01 07:30:00',\n               '2016-06-01 08:30:00', '2016-06-01 09:30:00',\n               '2016-06-01 10:30:00', '2016-06-01 11:30:00',\n               '2016-06-01 12:30:00', '2016-06-01 13:30:00',\n               '2016-06-01 14:30:00', '2016-06-01 15:30:00',\n               '2016-06-01 16:30:00', '2016-06-01 17:30:00',\n               '2016-06-01 18:30:00', '2016-06-01 19:30:00',\n               '2016-06-01 20:30:00', '2016-06-01 21:30:00',\n               '2016-06-01 22:30:00', '2016-06-01 23:30:00'],\n              dtype='datetime64[ns]', name='time', freq=None))Attributes: (31)DODS_EXTRA.Unlimited_Dimension :timeHistory :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000\n\n\nGetting ds did not get the data. So wouldn’t run into the EULA problem. But once we try to plot, we would run into the EULA. If we didn’t not have it accepted, it would not let us read the data. But even if it were accepted, it can be tricky to get past re-directs. Fortunately the earthaccess authenticated session works well as it gets the EDL token for us automatically.\n\nds[\"T2M\"].isel(time=1).plot();\n\n\n\n\n\n\n\n\n\n\nCreating data cubes\nData cubes with data that have EULA often goes south (re-directs) but with earthaccess authenticated sessions it works well.\n\neula_url1 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4'\neula_url2 = 'dap4://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/M2T1NXSLV.5.12.4/2016/06/MERRA2_400.tavg1_2d_slv_Nx.20160602.nc4'\neula_urls = [eula_url1, eula_url2]\n\n\n%%time\nimport xarray as xr\n# very fast and this is 1Tb of data\nds = xr.open_mfdataset(eula_urls, engine=\"pydap\", \n                       combine=\"nested\", concat_dim=\"/time\", \n                       decode_cf=False, session=edl_session)\nds\n\nCPU times: user 141 ms, sys: 12.2 ms, total: 153 ms\nWall time: 6.54 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:   (/time: 48, /lat: 361, /lon: 576)\nDimensions without coordinates: /time, /lat, /lon\nData variables: (12/50)\n    U2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    V250      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPT     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TROPPB    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    T2M       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    TQL       (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    ...        ...\n    T2MWET    (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    U500      (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    QV10M     (/time, /lat, /lon) float32 40MB dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;\n    lat       (/time, /lat) float64 139kB dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;\n    lon       (/time, /lon) float64 221kB dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;\n    time      (/time) int32 192B dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;\nAttributes: (12/31)\n    History:                           Original file generated: Tue Jun 14 18...\n    Comment:                           GMAO filename: d5124_m2_jan10.tavg1_2d...\n    Filename:                          MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4\n    Conventions:                       CF-1\n    Institution:                       NASA Global Modeling and Assimilation ...\n    References:                        http://gmao.gsfc.nasa.gov\n    ...                                ...\n    identifier_product_doi:            10.5067/VJAFPLI1CSIV\n    RangeBeginningDate:                2016-06-01\n    RangeBeginningTime:                00:00:00.000000\n    RangeEndingDate:                   2016-06-01\n    RangeEndingTime:                   23:59:59.000000\n    Unlimited_Dimension:               timexarray.DatasetDimensions:/time: 48/lat: 361/lon: 576Coordinates: (0)Data variables: (50)U2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U2Mfullnamepath :/U2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V250fullnamepath :/V250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_temperature_using_blended_TROPP_estimateunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_temperature_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPTfullnamepath :/TROPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPB(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_blended_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_blended_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPBfullnamepath :/TROPPBMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2Mfullnamepath :/T2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_liquid_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_liquid_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQLfullnamepath :/TQLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_500_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T500fullnamepath :/T500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTOX(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_odd_oxygenunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_odd_oxygenvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TOXfullnamepath :/TOXMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U850fullnamepath :/U850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PSfullnamepath :/PSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_850_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V850fullnamepath :/V850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nOMEGA500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :omega_at_500_hPaunits :Pa s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :omega_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :OMEGA500fullnamepath :/OMEGA500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_250_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H250fullnamepath :/H250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_250_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q250fullnamepath :/Q250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MDEW(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :dew_point_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :dew_point_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MDEWfullnamepath :/T2MDEWMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nPBLTOP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :pbltop_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :pbltop_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :PBLTOPfullnamepath :/PBLTOPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDPRS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDPRSfullnamepath :/CLDPRSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V50Mfullnamepath :/V50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_500_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q500fullnamepath :/Q500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nDISPH(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :zero_plane_displacement_heightunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :zero_plane_displacement_heightvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :DISPHfullnamepath :/DISPHMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH1000(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_1000_mbunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_1000_mbvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H1000fullnamepath :/H1000Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTO3(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_column_ozoneunits :Dobsons_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_column_ozonevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TO3fullnamepath :/TO3Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTS(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :surface_skin_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :surface_skin_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TSfullnamepath :/TSMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_air_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_air_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T10Mfullnamepath :/T10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPT(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_thermal_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_thermal_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPTfullnamepath :/TROPPTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQI(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_ice_waterunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_ice_watervmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQIfullnamepath :/TQIMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nSLP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :sea_level_pressureunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :sea_level_pressurevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :SLPfullnamepath :/SLPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_250_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U250fullnamepath :/U250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQ850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :specific_humidity_at_850_hPaunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :specific_humidity_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :Q850fullnamepath :/Q850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nZLCL(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :lifting_condensation_levelunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :lifting_condensation_levelvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :ZLCLfullnamepath :/ZLCLMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTQV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :total_precipitable_water_vaporunits :kg m-2_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :total_precipitable_water_vaporvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TQVfullnamepath :/TQVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V2Mfullnamepath :/V2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT250(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_250_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_250_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T250fullnamepath :/T250Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPQ(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_specific_humidity_using_blended_TROPP_estimateunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_specific_humidity_using_blended_TROPP_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPQfullnamepath :/TROPQMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_northward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_northward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V10Mfullnamepath :/V10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_850_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H850fullnamepath :/H850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT850(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :air_temperature_at_850_hPaunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :air_temperature_at_850_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T850fullnamepath :/T850Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU50M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_50_metersunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_50_metersvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U50Mfullnamepath :/U50MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_eastward_windunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_eastward_windvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U10Mfullnamepath :/U10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV2M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :2-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :2-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV2Mfullnamepath :/QV2MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nCLDTMP(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :cloud_top_temperatureunits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :cloud_top_temperaturevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :CLDTMPfullnamepath :/CLDTMPMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nTROPPV(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :tropopause_pressure_based_on_EPV_estimateunits :Pa_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :tropopause_pressure_based_on_EPV_estimatevmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :TROPPVfullnamepath :/TROPPVMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nH500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :height_at_500_hPaunits :m_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :height_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :H500fullnamepath :/H500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nV500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :northward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :northward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :V500fullnamepath :/V500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nT2MWET(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :wet_bulb_temperature_at_2_munits :K_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :wet_bulb_temperature_at_2_mvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :T2MWETfullnamepath :/T2MWETMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nU500(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :eastward_wind_at_500_hPaunits :m s-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :eastward_wind_at_500_hPavmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :U500fullnamepath :/U500Maps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nQV10M(/time, /lat, /lon)float32dask.array&lt;chunksize=(24, 361, 576), meta=np.ndarray&gt;long_name :10-meter_specific_humidityunits :kg kg-1_FillValue :999999987000000.0missing_value :999999987000000.0fmissing_value :999999987000000.0scale_factor :1.0add_offset :0.0standard_name :10-meter_specific_humidityvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :QV10Mfullnamepath :/QV10MMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n38.07 MiB\n19.04 MiB\n\n\nShape\n(48, 361, 576)\n(24, 361, 576)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                           576 361 48\n\n\n\n\nlat(/time, /lat)float64dask.array&lt;chunksize=(24, 361), meta=np.ndarray&gt;long_name :latitudeunits :degrees_northvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :latfullnamepath :/latMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n135.38 kiB\n67.69 kiB\n\n\nShape\n(48, 361)\n(24, 361)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          361 48\n\n\n\n\nlon(/time, /lon)float64dask.array&lt;chunksize=(24, 576), meta=np.ndarray&gt;long_name :longitudeunits :degrees_eastvmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :lonfullnamepath :/lonMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n216.00 kiB\n108.00 kiB\n\n\nShape\n(48, 576)\n(24, 576)\n\n\nDask graph\n2 chunks in 7 graph layers\n\n\nData type\nfloat64 numpy.ndarray\n\n\n\n\n          576 48\n\n\n\n\ntime(/time)int32dask.array&lt;chunksize=(24,), meta=np.ndarray&gt;long_name :timeunits :minutes since 2016-06-01 00:30:00time_increment :10000begin_date :20160601begin_time :3000vmax :999999987000000.0vmin :-999999987000000.0valid_range :[-999999987000000.0, 999999987000000.0]origname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n192 B\n96 B\n\n\nShape\n(48,)\n(24,)\n\n\nDask graph\n2 chunks in 5 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n          48 1\n\n\n\n\nIndexes: (0)Attributes: (31)History :Original file generated: Tue Jun 14 18:02:46 2016 GMTComment :GMAO filename: d5124_m2_jan10.tavg1_2d_slv_Nx.20160601.nc4Filename :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4Conventions :CF-1Institution :NASA Global Modeling and Assimilation OfficeReferences :http://gmao.gsfc.nasa.govFormat :NetCDF-4/HDF-5SpatialCoverage :globalVersionID :5.12.4TemporalRange :1980-01-01 -&gt; 2016-12-31identifier_product_doi_authority :http://dx.doi.org/ShortName :M2T1NXSLVGranuleID :MERRA2_400.tavg1_2d_slv_Nx.20160601.nc4ProductionDateTime :Original file generated: Tue Jun 14 18:02:46 2016 GMTLongName :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsTitle :MERRA2 tavg1_2d_slv_Nx: 2d,1-Hourly,Time-Averaged,Single-Level,Assimilation,Single-Level DiagnosticsSouthernmostLatitude :-90.0NorthernmostLatitude :90.0WesternmostLongitude :-180.0EasternmostLongitude :179.375LatitudeResolution :0.5LongitudeResolution :0.625DataResolution :0.5 x 0.625Source :CVS tag: GEOSadas-5_12_4_p5Contact :http://gmao.gsfc.nasa.govidentifier_product_doi :10.5067/VJAFPLI1CSIVRangeBeginningDate :2016-06-01RangeBeginningTime :00:00:00.000000RangeEndingDate :2016-06-01RangeEndingTime :23:59:59.000000Unlimited_Dimension :time\n\n\n\nprint(f\"Dataset size: {ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 1876.77 MB\n\n\n\nds[\"T2M\"].isel({\"/time\": 40}).plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#example-4-using-constraint-expresions",
    "href": "topics-2025/2025-opendap/3-nasa.html#example-4-using-constraint-expresions",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Example 4 Using constraint expresions",
    "text": "Example 4 Using constraint expresions\nIn this example from the pydap documentation, constraint expression is used just to get certain variables when the OPeNDAP server uses Hyrax. See full notebook here. Using the constraint expression can greatly speed up the data access.\n\nbaseURL = 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/'\nTemp_Salt = \"ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_\"\nyear = '2017-'\nmonth = '01'\nend_ = '_ECCO_V4r4_native_llc0090'\nCE = '?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time'\n\nTemp_2017 = [baseURL + Temp_Salt + year + f'{i:02}' + end_ + CE for i in range(1, 4)]\nTemp_2017\n\n['dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-02_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time',\n 'dap4://opendap.earthdata.nasa.gov/providers/POCLOUD/collections/ECCO%20Ocean%20Temperature%20and%20Salinity%20-%20Monthly%20Mean%20llc90%20Grid%20(Version%204%20Release%204)/granules/OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-03_ECCO_V4r4_native_llc0090?dap4.ce=/THETA;/SALT;/tile;/j;/k;/i;/time']\n\n\nCreate data cube with open_mfdataset but not concat dim is /time not time. This takes a really long time, but if we didn’t do the constraint expression part, it would take much longer. So it is good to do that step.\n\n%%time\n# 13 seconds to assemble the data cube for a 126Mb dataset...slow\ntheta_salt_ds = xr.open_mfdataset(\n    Temp_2017, \n    engine='pydap',\n    parallel=True, \n    combine='nested', \n    concat_dim='/time',\n    session=edl_session\n)\ntheta_salt_ds\n\nCPU times: user 173 ms, sys: 13.2 ms, total: 187 ms\nWall time: 13.4 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 126MB\nDimensions:  (/time: 3, /k: 50, /tile: 13, /j: 90, /i: 90)\nCoordinates:\n    time     (/time) datetime64[ns] 24B dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;\nDimensions without coordinates: /time, /k, /tile, /j, /i\nData variables:\n    SALT     (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    THETA    (/time, /k, /tile, /j, /i) float32 63MB dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;\n    i        (/time, /i) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    j        (/time, /j) int32 1kB dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;\n    k        (/time, /k) int32 600B dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;\n    tile     (/time, /tile) int32 156B dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;\nAttributes: (12/62)\n    acknowledgement:                 This research was carried out by the Jet...\n    author:                          Ian Fenty and Ou Wang\n    cdm_data_type:                   Grid\n    comment:                         Fields provided on the curvilinear lat-l...\n    Conventions:                     CF-1.8, ACDD-1.3\n    coordinates_comment:             Note: the global 'coordinates' attribute...\n    ...                              ...\n    time_coverage_duration:          P1M\n    time_coverage_end:               2017-02-01T00:00:00\n    time_coverage_resolution:        P1M\n    time_coverage_start:             2017-01-01T00:00:00\n    title:                           ECCO Ocean Temperature and Salinity - Mo...\n    uuid:                            f5b7028c-4181-11eb-b7e6-0cc47a3f47b1xarray.DatasetDimensions:/time: 3/k: 50/tile: 13/j: 90/i: 90Coordinates: (1)time(/time)datetime64[ns]dask.array&lt;chunksize=(1,), meta=np.ndarray&gt;long_name :center time of averaging periodaxis :Tbounds :time_bndscoverage_content_type :coordinatestandard_name :timeorigname :timefullnamepath :/timeMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n24 B\n8 B\n\n\nShape\n(3,)\n(1,)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n           3 1\n\n\n\n\nData variables: (6)SALT(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Salinityunits :1e-3coverage_content_type :modelResultstandard_name :sea_water_salinitycomment :Defined using CF convention 'Sea water salinity is the salt content of sea water, often on the Practical Salinity Scale of 1978. However, the unqualified term 'salinity' is generic and does not necessarily imply any particular method of calculation. The units of salinity are dimensionless and the units attribute should normally be given as 1e-3 or 0.001 i.e. parts per thousand.' see https://cfconventions.org/Data/cf-standard-names/73/build/cf-standard-name-table.htmlvalid_min :17.106637954711914valid_max :41.26802444458008origname :SALTfullnamepath :/SALTMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\nTHETA(/time, /k, /tile, /j, /i)float32dask.array&lt;chunksize=(1, 50, 13, 90, 90), meta=np.ndarray&gt;long_name :Potential temperature units :degree_Ccoverage_content_type :modelResultstandard_name :sea_water_potential_temperaturecomment :Sea water potential temperature is the temperature a parcel of sea water would have if moved adiabatically to sea level pressure. Note: the equation of state is a modified UNESCO formula by Jackett and McDougall (1995), which uses the model variable potential temperature as input assuming a horizontally and temporally constant pressure of $p_0=-g ho_{0} z$.valid_min :-2.2909388542175293valid_max :36.032955169677734origname :THETAfullnamepath :/THETAMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n60.25 MiB\n20.08 MiB\n\n\nShape\n(3, 50, 13, 90, 90)\n(1, 50, 13, 90, 90)\n\n\nDask graph\n3 chunks in 7 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n           50 3                          90 90 13\n\n\n\n\ni(/time, /i)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Xlong_name :grid index in x for variables at tracer and 'v' locationsswap_dim :XCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'v' variables (e.g., VVEL) have the same x coordinate on the model grid.coverage_content_type :coordinateorigname :ifullnamepath :/iMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nj(/time, /j)int32dask.array&lt;chunksize=(1, 90), meta=np.ndarray&gt;axis :Ylong_name :grid index in y for variables at tracer and 'u' locationsswap_dim :YCcomment :In the Arakawa C-grid system, tracer (e.g., THETA) and 'u' variables (e.g., UVEL) have the same y coordinate on the model grid.coverage_content_type :coordinateorigname :jfullnamepath :/jMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n1.05 kiB\n360 B\n\n\nShape\n(3, 90)\n(1, 90)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           90 3\n\n\n\n\nk(/time, /k)int32dask.array&lt;chunksize=(1, 50), meta=np.ndarray&gt;axis :Zlong_name :grid index in z for tracer variablesswap_dim :Zcoverage_content_type :coordinateorigname :kfullnamepath :/kMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n600 B\n200 B\n\n\nShape\n(3, 50)\n(1, 50)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           50 3\n\n\n\n\ntile(/time, /tile)int32dask.array&lt;chunksize=(1, 13), meta=np.ndarray&gt;long_name :lat-lon-cap tile indexcomment :The ECCO V4 horizontal model grid is divided into 13 tiles of 90x90 cells for convenience.coverage_content_type :coordinateorigname :tilefullnamepath :/tileMaps :()\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n156 B\n52 B\n\n\nShape\n(3, 13)\n(1, 13)\n\n\nDask graph\n3 chunks in 10 graph layers\n\n\nData type\nint32 numpy.ndarray\n\n\n\n\n           13 3\n\n\n\n\nIndexes: (0)Attributes: (62)acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.author :Ian Fenty and Ou Wangcdm_data_type :Gridcomment :Fields provided on the curvilinear lat-lon-cap 90 (llc90) native grid used in the ECCO model.Conventions :CF-1.8, ACDD-1.3coordinates_comment :Note: the global 'coordinates' attribute describes auxillary coordinates.creator_email :ecco-group@mit.educreator_institution :NASA Jet Propulsion Laboratory (JPL)creator_name :ECCO Consortiumcreator_type :groupcreator_url :https://ecco-group.orgdate_created :2020-12-18T14:39:59date_issued :2020-12-18T14:39:59date_metadata_modified :2021-03-15T21:56:21date_modified :2021-03-15T21:56:21geospatial_bounds_crs :EPSG:4326geospatial_lat_max :90.0geospatial_lat_min :-90.0geospatial_lat_resolution :variablegeospatial_lat_units :degrees_northgeospatial_lon_max :180.0geospatial_lon_min :-180.0geospatial_lon_resolution :variablegeospatial_lon_units :degrees_eastgeospatial_vertical_max :0.0geospatial_vertical_min :-6134.5geospatial_vertical_positive :upgeospatial_vertical_resolution :variablegeospatial_vertical_units :meterhistory :Inaugural release of an ECCO Central Estimate solution to PO.DAACid :10.5067/ECL5M-OTS44institution :NASA Jet Propulsion Laboratory (JPL)instrument_vocabulary :GCMD instrument keywordskeywords :EARTH SCIENCE &gt; OCEANS &gt; SALINITY/DENSITY &gt; SALINITY, EARTH SCIENCE SERVICES &gt; MODELS &gt; EARTH SCIENCE REANALYSES/ASSIMILATION MODELS, EARTH SCIENCE &gt; OCEANS &gt; OCEAN TEMPERATURE &gt; POTENTIAL TEMPERATUREkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordslicense :Public Domainmetadata_link :https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=ECCO_L4_TEMP_SALINITY_LLC0090GRID_MONTHLY_V4R4naming_authority :gov.nasa.jplplatform :ERS-1/2, TOPEX/Poseidon, Geosat Follow-On (GFO), ENVISAT, Jason-1, Jason-2, CryoSat-2, SARAL/AltiKa, Jason-3, AVHRR, Aquarius, SSM/I, SSMIS, GRACE, DTU17MDT, Argo, WOCE, GO-SHIP, MEOP, Ice Tethered Profilers (ITP)platform_vocabulary :GCMD platform keywordsprocessing_level :L4product_name :OCEAN_TEMPERATURE_SALINITY_mon_mean_2017-01_ECCO_V4r4_native_llc0090.ncproduct_time_coverage_end :2018-01-01T00:00:00product_time_coverage_start :1992-01-01T12:00:00product_version :Version 4, Release 4program :NASA Physical Oceanography, Cryosphere, Modeling, Analysis, and Prediction (MAP)project :Estimating the Circulation and Climate of the Ocean (ECCO)publisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :PO.DAACpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :institutionpublisher_url :https://podaac.jpl.nasa.govreferences :ECCO Consortium, Fukumori, I., Wang, O., Fenty, I., Forget, G., Heimbach, P., & Ponte, R. M. 2020. Synopsis of the ECCO Central Production Global Ocean and Sea-Ice State Estimate (Version 4 Release 4). doi:10.5281/zenodo.3765928source :The ECCO V4r4 state estimate was produced by fitting a free-running solution of the MITgcm (checkpoint 66g) to satellite and in situ observational data in a least squares sense using the adjoint methodstandard_name_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventionsummary :This dataset provides monthly-averaged ocean potential temperature and salinity on the lat-lon-cap 90 (llc90) native model grid from the ECCO Version 4 Release 4 (V4r4) ocean and sea-ice state estimate. Estimating the Circulation and Climate of the Ocean (ECCO) state estimates are dynamically and kinematically-consistent reconstructions of the three-dimensional, time-evolving ocean, sea-ice, and surface atmospheric states. ECCO V4r4 is a free-running solution of a global, nominally 1-degree configuration of the MIT general circulation model (MITgcm) that has been fit to observations in a least-squares sense. Observational data constraints used in V4r4 include sea surface height (SSH) from satellite altimeters [ERS-1/2, TOPEX/Poseidon, GFO, ENVISAT, Jason-1,2,3, CryoSat-2, and SARAL/AltiKa]; sea surface temperature (SST) from satellite radiometers [AVHRR], sea surface salinity (SSS) from the Aquarius satellite radiometer/scatterometer, ocean bottom pressure (OBP) from the GRACE satellite gravimeter; sea-ice concentration from satellite radiometers [SSM/I and SSMIS], and in-situ ocean temperature and salinity measured with conductivity-temperature-depth (CTD) sensors and expendable bathythermographs (XBTs) from several programs [e.g., WOCE, GO-SHIP, Argo, and others] and platforms [e.g., research vessels, gliders, moorings, ice-tethered profilers, and instrumented pinnipeds]. V4r4 covers the period 1992-01-01T12:00:00 to 2018-01-01T00:00:00.time_coverage_duration :P1Mtime_coverage_end :2017-02-01T00:00:00time_coverage_resolution :P1Mtime_coverage_start :2017-01-01T00:00:00title :ECCO Ocean Temperature and Salinity - Monthly Mean llc90 Grid (Version 4 Release 4)uuid :f5b7028c-4181-11eb-b7e6-0cc47a3f47b1\n\n\n\nprint(f\"Dataset size: {theta_salt_ds.nbytes/1e6:.2f} MB\")\n\nDataset size: 126.36 MB\n\n\n\ntheta_salt_ds[\"THETA\"].isel({\"/time\":0, \"/tile\":10, \"/k\":1}).plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#conclusion",
    "href": "topics-2025/2025-opendap/3-nasa.html#conclusion",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with NASA OPeNDAP servers using the earthaccess authenticated sessions, makes it easier to using xarray for single and multiple files.",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/3-nasa.html#references-1",
    "href": "topics-2025/2025-opendap/3-nasa.html#references-1",
    "title": "Accessing data on NASA Earthdata servers via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://pydap.github.io/pydap/intro.html\nhttps://opendap.github.io/documentation/tutorials/ClientAuthentication.html#_pydap\nhttps://github.com/OPENDAP/NASA-tutorials/tree/main",
    "crumbs": [
      "Python - OPeNDAP",
      "NASA OPeNDAP servers"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#overview",
    "href": "topics-2025/2025-opendap/5-usgs.html#overview",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Overview",
    "text": "Overview\nThe USGS LPDAAC server requires NASA Earthdata login authentication, but doesn’t require a EULA (as far as I know). However they have similar redirect issues as data that does require a EULA. I could not get the data into an xarray.\n\nPrerequisites\nI assume you have a .netrc file at ~ (home). ~/.netrc should look just like this with your username and password.\nmachine urs.earthdata.nasa.gov\n        login yourusername\n        password yourpassword\n\n\nPackages\n\nimport xarray as xr\nimport pydap.client\nimport pydap",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#set-up-a-token-based-session-with-earthaccess",
    "href": "topics-2025/2025-opendap/5-usgs.html#set-up-a-token-based-session-with-earthaccess",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Set up a token-based session with earthaccess",
    "text": "Set up a token-based session with earthaccess\n\n# create an authenticated session\nimport earthaccess\nearthaccess.login()\nedl_session = earthaccess.get_requests_https_session()\n\n\n# if it keeps asking for your password, run this instead to save to .netrc file\nauth = earthaccess.login()\n# are we authenticated?\nif not auth.authenticated:\n    # ask for credentials and persist them in a .netrc file\n    auth.login(strategy=\"interactive\", persist=True)",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#usgs-lpdaac",
    "href": "topics-2025/2025-opendap/5-usgs.html#usgs-lpdaac",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "USGS LPDAAC",
    "text": "USGS LPDAAC\nTheir OPeNDAP server also uses NASA Earthdata login authentication.\n\nurl=\"https://opendap.cr.usgs.gov/opendap/hyrax/MOD13Q1.061/h09v06.ncml\"\n\n\n# this works\npydap_ds = pydap.client.open_url(url, session=edl_session, protocol=\"dap4\")\n\n\n# this works too\nstore = xr.backends.PydapDataStore(pydap_ds)\nds = xr.open_dataset(store)\n\n\n# this works too\nurl=\"dap4://opendap.cr.usgs.gov/opendap/hyrax/MOD13Q1.061/h09v06.ncml\"\nds = xr.open_dataset(url, engine=\"pydap\", session=edl_session)\n\n\nprint(f\"Dataset size: {ds.nbytes/1e9:.2f} Tb\")\n\nDataset size: 690.46 Tb\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 690GB\nDimensions:                                          (/YDim: 4800, /XDim: 4800,\n                                                      /time: 576)\nCoordinates:\n    Latitude                                         (/YDim, /XDim) float64 184MB ...\n    Longitude                                        (/YDim, /XDim) float64 184MB ...\nDimensions without coordinates: /YDim, /XDim, /time\nData variables: (12/16)\n    YDim                                             (/YDim) float64 38kB ...\n    XDim                                             (/XDim) float64 38kB ...\n    MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projection  uint8 1B ...\n    _250m_16_days_VI_Quality                         (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_red_reflectance                    (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_sun_zenith_angle                   (/time, /YDim, /XDim) float32 53GB ...\n    ...                                               ...\n    _250m_16_days_EVI                                (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_composite_day_of_the_year          (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_MIR_reflectance                    (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_blue_reflectance                   (/time, /YDim, /XDim) float32 53GB ...\n    _250m_16_days_NDVI                               (/time, /YDim, /XDim) float32 53GB ...\n    time                                             (/time) datetime64[ns] 5kB ...\nAttributes:\n    HDFEOSVersion:                     HDFEOS_V2.19\n    identifier_product_doi:            10.5067/MODIS/MOD13Q1.061\n    identifier_product_doi_authority:  http://dx.doi.orgxarray.DatasetDimensions:/YDim: 4800/XDim: 4800/time: 576Coordinates: (2)Latitude(/YDim, /XDim)float64...long_name :Latitudeunits :degrees_northMaps :()[23040000 values with dtype=float64]Longitude(/YDim, /XDim)float64...long_name :Longitudeunits :degrees_eastMaps :()[23040000 values with dtype=float64]Data variables: (16)YDim(/YDim)float64...standard_name :projection_y_coordinatelong_name :y coordinate of projection for grid MODIS_Grid_16DAY_250m_500m_VIunits :meter_CoordinateAxisType :GeoYMaps :()[4800 values with dtype=float64]XDim(/XDim)float64...standard_name :projection_x_coordinatelong_name :x coordinate of projection for grid MODIS_Grid_16DAY_250m_500m_VIunits :meter_CoordinateAxisType :GeoXMaps :()[4800 values with dtype=float64]MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projection()uint8...grid_mapping_name :sinusoidallongitude_of_central_meridian :0.0earth_radius :6371007.181_CoordinateAxisTypes :GeoX GeoYMaps :()[1 values with dtype=uint8]_250m_16_days_VI_Quality(/time, /YDim, /XDim)float32...long_name :250m 16 days VI Qualityunits :bit fieldvalid_range :[0, 65534]Legend :\n     Bit Fields Description (Right to Left): \n    [0-1] : MODLAND_QA [2 bit range] \n         00: VI produced, good quality \n         01: VI produced, but check other QA \n         10: Pixel produced, but most probably cloudy \n         11: Pixel not produced due to other reasons than clouds \n    [2-5] : VI usefulness [4 bit range] \n         0000: Highest quality \n         0001: Lower quality  \n         0010..1010: Decreasing quality \n         1100: Lowest quality \n         1101: Quality so low that it is not useful \n         1110: L1B data faulty \n         1111: Not useful for any other reason/not processed \n    [6-7] : Aerosol quantity [2 bit range] \n         00: Climatology \n         01: Low \n         10: Average \n         11: High (11) \n    [8] : Adjacent cloud detected; [1 bit range] \n         1: Yes \n         0: No \n    [9] : Atmosphere BRDF correction performed [1 bit range] \n         1: Yes \n         0: No \n    [10] : Mixed clouds  [1 bit range] \n         1: Yes \n         0: No \n    [11-13] : Land/Water Flag [3 bit range] \n         000: Shallow ocean \n         001: Land (Nothing else but land) \n         010: Ocean coastlines and lake shorelines \n         011: Shallow inland water \n         100: Ephemeral water \n         101: Deep inland water \n         110: Moderate or continental ocean \n         111: Deep ocean \n    [14] : Possible snow/ice [1 bit range] \n         1: Yes \n         0: No \n    [15] : Possible shadow [1 bit range] \n         1: Yes \n         0: No \ngrid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_red_reflectance(/time, /YDim, /XDim)float32...long_name :250m 16 days red reflectanceunits :reflectancescale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :0.0valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_sun_zenith_angle(/time, /YDim, /XDim)float32...long_name :250m 16 days sun zenith angleunits :degreesscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :100.0orig_add_offset :0.0valid_min :0.0valid_max :180.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_relative_azimuth_angle(/time, /YDim, /XDim)float32...long_name :250m 16 days relative azimuth angleunits :degreesscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :100.0orig_add_offset :0.0valid_min :-180.0valid_max :180.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_NIR_reflectance(/time, /YDim, /XDim)float32...long_name :250m 16 days NIR reflectanceunits :reflectancescale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :0.0valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_view_zenith_angle(/time, /YDim, /XDim)float32...long_name :250m 16 days view zenith angleunits :degreesscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :100.0orig_add_offset :0.0valid_min :0.0valid_max :180.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_pixel_reliability(/time, /YDim, /XDim)float64...long_name :250m 16 days pixel reliabilityunits :rankvalid_range :[0, 3]Legend :\n     Rank Keys: \n        [-1]:  Fill/No Data-Not Processed. \n         [0]:  Good data      - Use with confidence \n         [1]:  Marginal data  - Useful, but look at other QA information \n         [2]:  Snow/Ice       - Target covered with snow/ice \n         [3]:  Cloudy         - Target not visible, covered with cloud \ngrid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float64]_250m_16_days_EVI(/time, /YDim, /XDim)float32...long_name :250m 16 days EVIunits :EVIscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :-0.200000003valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_composite_day_of_the_year(/time, /YDim, /XDim)float32...long_name :250m 16 days composite day of the yearunits :Julian day of the year valid_range :[1, 366]grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_MIR_reflectance(/time, /YDim, /XDim)float32...long_name :250m 16 days MIR reflectanceunits :reflectanceLegend :\n     The MIR band saved in the VI product is MODIS band 7 \n         Bandwidth : 2105-2155 nm \n         Band center: 2130 nm \nscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :0.0valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_blue_reflectance(/time, /YDim, /XDim)float32...long_name :250m 16 days blue reflectanceunits :reflectancescale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :0.0valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]_250m_16_days_NDVI(/time, /YDim, /XDim)float32...long_name :250m 16 days NDVIunits :NDVIscale_factor_err :0.0add_offset_err :0.0calibrated_nt :5orig_scale_factor :10000.0orig_add_offset :0.0valid_min :-0.200000003valid_max :1.0grid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()[13271040000 values with dtype=float32]time(/time)datetime64[ns]...Maps :()[576 values with dtype=datetime64[ns]]Indexes: (0)Attributes: (3)HDFEOSVersion :HDFEOS_V2.19identifier_product_doi :10.5067/MODIS/MOD13Q1.061identifier_product_doi_authority :http://dx.doi.org\n\n\n\n# loading data works\nds[\"_250m_16_days_VI_Quality\"].isel({\"/time\": 1, \"/YDim\": 1}).load()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray '_250m_16_days_VI_Quality' (/XDim: 4800)&gt; Size: 19kB\narray([2116., 2116., 2116., ..., 2116., 2116., 2116.], dtype=float32)\nCoordinates:\n    Latitude   (/XDim) float64 38kB 30.0 30.0 30.0 30.0 ... 30.0 30.0 30.0 30.0\n    Longitude  (/XDim) float64 38kB -103.9 -103.9 -103.9 ... -92.38 -92.37\nDimensions without coordinates: /XDim\nAttributes:\n    long_name:     250m 16 days VI Quality\n    units:         bit field\n    valid_range:   [0, 65534]\n    Legend:        \\n\\t Bit Fields Description (Right to Left): \\n\\t[0-1] : M...\n    grid_mapping:  MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projection\n    Maps:          ()xarray.DataArray'_250m_16_days_VI_Quality'/XDim: 48002.116e+03 2.116e+03 2.116e+03 ... 2.116e+03 2.116e+03 2.116e+03array([2116., 2116., 2116., ..., 2116., 2116., 2116.], dtype=float32)Coordinates: (2)Latitude(/XDim)float6430.0 30.0 30.0 ... 30.0 30.0 30.0long_name :Latitudeunits :degrees_northMaps :()array([29.996875, 29.996875, 29.996875, ..., 29.996875, 29.996875,\n       29.996875])Longitude(/XDim)float64-103.9 -103.9 ... -92.38 -92.37long_name :Longitudeunits :degrees_eastMaps :()array([-103.91857343, -103.91616788, -103.91376233, ...,  -92.37914828,\n        -92.37674273,  -92.37433718])Indexes: (0)Attributes: (6)long_name :250m 16 days VI Qualityunits :bit fieldvalid_range :[0, 65534]Legend :\n     Bit Fields Description (Right to Left): \n    [0-1] : MODLAND_QA [2 bit range] \n         00: VI produced, good quality \n         01: VI produced, but check other QA \n         10: Pixel produced, but most probably cloudy \n         11: Pixel not produced due to other reasons than clouds \n    [2-5] : VI usefulness [4 bit range] \n         0000: Highest quality \n         0001: Lower quality  \n         0010..1010: Decreasing quality \n         1100: Lowest quality \n         1101: Quality so low that it is not useful \n         1110: L1B data faulty \n         1111: Not useful for any other reason/not processed \n    [6-7] : Aerosol quantity [2 bit range] \n         00: Climatology \n         01: Low \n         10: Average \n         11: High (11) \n    [8] : Adjacent cloud detected; [1 bit range] \n         1: Yes \n         0: No \n    [9] : Atmosphere BRDF correction performed [1 bit range] \n         1: Yes \n         0: No \n    [10] : Mixed clouds  [1 bit range] \n         1: Yes \n         0: No \n    [11-13] : Land/Water Flag [3 bit range] \n         000: Shallow ocean \n         001: Land (Nothing else but land) \n         010: Ocean coastlines and lake shorelines \n         011: Shallow inland water \n         100: Ephemeral water \n         101: Deep inland water \n         110: Moderate or continental ocean \n         111: Deep ocean \n    [14] : Possible snow/ice [1 bit range] \n         1: Yes \n         0: No \n    [15] : Possible shadow [1 bit range] \n         1: Yes \n         0: No \ngrid_mapping :MODIS_Grid_16DAY_250m_500m_VI_eos_cf_projectionMaps :()\n\n\n\n# this is another url that works with pydap\nurl = \"https://opendap.cr.usgs.gov/opendap/hyrax/ECOSTRESS/ECO2CLD.001/2018.07.09/ECOSTRESS_L2_CLOUD_00048_001_20180709T204901_0601_03.h5\"\npydap_ds = pydap.client.open_url(url, session=edl_session, protocol=\"dap4\")",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#setting-token-based-authentication-with-pydap",
    "href": "topics-2025/2025-opendap/5-usgs.html#setting-token-based-authentication-with-pydap",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Setting token-based authentication with pydap",
    "text": "Setting token-based authentication with pydap\nIf you don’t want to use earthaccess, here is how to set up token-based authentication with pydap. Go on your Earthdata login user profile to create a token.\n\nedl_token = \"put your token here\"\nimport requests\nauth_hdr=\"Bearer \" + edl_token\ntoken_session = requests.Session()\ntoken_session.headers={\"Authorization\": auth_hdr}",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#conclusion",
    "href": "topics-2025/2025-opendap/5-usgs.html#conclusion",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "Conclusion",
    "text": "Conclusion\nWorking with USGS data on its OPeNDAP server requires Earthdata authentication. Use token-based authentication to solve redirect errors. earthaccess helps with this.",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/5-usgs.html#references",
    "href": "topics-2025/2025-opendap/5-usgs.html#references",
    "title": "Accessing data on the USGS LPDAAC server via OPeNDAP protocol",
    "section": "References",
    "text": "References\n\nhttps://github.com/pydap/pydap/issues/188\nhttps://nsidc.org/data/user-resources/help-center/how-do-i-access-data-using-opendap#anchor-using-a-command-line-interface",
    "crumbs": [
      "Python - OPeNDAP",
      "USGS LPDAAC"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/7-nomads.html#overview",
    "href": "topics-2025/2025-opendap/7-nomads.html#overview",
    "title": "NOAA Operational Model Archive and Distribution System",
    "section": "Overview",
    "text": "Overview\nWORK IN PROGRESS\nThis tutorial will show examples of working with OPeNDAP data from the NOAA Operational Model Archive and Distribution System. These OPeNDAP servers use the GrADS protocol.\nhttps://nomads.ncep.noaa.gov/\n\nPrerequisites\n\n\nPackages\n\nimport xarray as xr",
    "crumbs": [
      "Python - OPeNDAP",
      "NOMADS"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/7-nomads.html#load-some-data-and-plot",
    "href": "topics-2025/2025-opendap/7-nomads.html#load-some-data-and-plot",
    "title": "NOAA Operational Model Archive and Distribution System",
    "section": "Load some data and plot",
    "text": "Load some data and plot\n\nurl=\"http://nomads.ncep.noaa.gov:80/dods/fnl/fnl20250312/fnlflx_00z\"\n\n\n# time will need to be fixed probably\nds = xr.open_dataset(url, decode_times=False)\n\n\nprint(f\"Dataset size: {ds.nbytes/1e9:.2f} Tb\")\n\nDataset size: 2.13 Tb\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:         (time: 1, lat: 1536, lon: 3072)\nCoordinates:\n  * time            (time) float64 8B 7.393e+05\n  * lat             (lat) float64 12kB -89.91 -89.79 -89.68 ... 89.79 89.91\n  * lon             (lon) float64 25kB 0.0 0.1172 0.2344 ... 359.6 359.8 359.9\nData variables: (12/113)\n    acondsfc        (time, lat, lon) float32 19MB ...\n    albdosfc        (time, lat, lon) float32 19MB ...\n    cduvbsfc        (time, lat, lon) float32 19MB ...\n    cnwatsfc        (time, lat, lon) float32 19MB ...\n    cpofpsfc        (time, lat, lon) float32 19MB ...\n    cpratsfc        (time, lat, lon) float32 19MB ...\n    ...              ...\n    vgrd10m         (time, lat, lon) float32 19MB ...\n    vgrdhy1         (time, lat, lon) float32 19MB ...\n    vgtypsfc        (time, lat, lon) float32 19MB ...\n    watrsfc         (time, lat, lon) float32 19MB ...\n    weasdsfc        (time, lat, lon) float32 19MB ...\n    wiltsfc         (time, lat, lon) float32 19MB ...\nAttributes:\n    title:        GDAS flux file at 00Z12mar2025, downloaded Mar 12 06:56 UTC\n    Conventions:  COARDS\\nGrADS\n    dataType:     Grid\n    history:      Sat Mar 15 17:21:35 UTC 2025 : imported by GrADS Data Serve...xarray.DatasetDimensions:time: 1lat: 1536lon: 3072Coordinates: (3)time(time)float647.393e+05grads_dim :tgrads_mapping :lineargrads_size :1grads_min :00z12mar2025grads_step :1hrunits :days since 1-1-1 00:00:0.0long_name :timeminimum :00z12mar2025maximum :00z12mar2025array([739323.])lat(lat)float64-89.91 -89.79 ... 89.79 89.91grads_dim :ygrads_mapping :levelsgrads_size :1536units :degrees_northlong_name :latitudeminimum :-89.91maximum :89.91resolution :0.11714658array([-89.91 , -89.794, -89.677, ...,  89.677,  89.794,  89.91 ])lon(lon)float640.0 0.1172 0.2344 ... 359.8 359.9grads_dim :xgrads_mapping :lineargrads_size :3072units :degrees_eastlong_name :longitudeminimum :0.0maximum :359.882813resolution :0.1171875array([0.000000e+00, 1.171875e-01, 2.343750e-01, ..., 3.596484e+02,\n       3.597656e+02, 3.598828e+02])Data variables: (113)acondsfc(time, lat, lon)float32...long_name :** surface aerodynamic conductance [m/s] [4718592 values with dtype=float32]albdosfc(time, lat, lon)float32...long_name :** surface albedo [%] [4718592 values with dtype=float32]cduvbsfc(time, lat, lon)float32...long_name :** surface clear sky uv-b downward solar flux [w/m^2] [4718592 values with dtype=float32]cnwatsfc(time, lat, lon)float32...long_name :** surface plant canopy surface water [kg/m^2] [4718592 values with dtype=float32]cpofpsfc(time, lat, lon)float32...long_name :** surface percent frozen precipitation [%] [4718592 values with dtype=float32]cpratsfc(time, lat, lon)float32...long_name :** surface convective precipitation rate [kg/m^2/s] [4718592 values with dtype=float32]csdlfsfc(time, lat, lon)float32...long_name :** surface clear sky downward long wave flux [w/m^2] [4718592 values with dtype=float32]csdsfsfc(time, lat, lon)float32...long_name :** surface clear sky downward solar flux [w/m^2] [4718592 values with dtype=float32]csulfsfc(time, lat, lon)float32...long_name :** surface clear sky upward long wave flux [w/m^2] [4718592 values with dtype=float32]csulftoa(time, lat, lon)float32...long_name :** top of atmosphere clear sky upward long wave flux [w/m^2] [4718592 values with dtype=float32]csusfsfc(time, lat, lon)float32...long_name :** surface clear sky upward solar flux [w/m^2] [4718592 values with dtype=float32]csusftoa(time, lat, lon)float32...long_name :** top of atmosphere clear sky upward solar flux [w/m^2] [4718592 values with dtype=float32]cworkclm(time, lat, lon)float32...long_name :** entire atmosphere (considered as a single layer) cloud work function [j/kg] [4718592 values with dtype=float32]dlwrfavesfc(time, lat, lon)float32...long_name :** surface downward long-wave rad. flux [w/m^2] [4718592 values with dtype=float32]dlwrfsfc(time, lat, lon)float32...long_name :** surface downward long-wave rad. flux [w/m^2] [4718592 values with dtype=float32]dswrfavesfc(time, lat, lon)float32...long_name :** surface downward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]dswrftoa(time, lat, lon)float32...long_name :** top of atmosphere downward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]dswrfsfc(time, lat, lon)float32...long_name :** surface downward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]duvbsfc(time, lat, lon)float32...long_name :** surface uv-b downward solar flux [w/m^2] [4718592 values with dtype=float32]evbssfc(time, lat, lon)float32...long_name :** surface direct evaporation from bare soil [w/m^2] [4718592 values with dtype=float32]evcwsfc(time, lat, lon)float32...long_name :** surface canopy water evaporation [w/m^2] [4718592 values with dtype=float32]fldcpsfc(time, lat, lon)float32...long_name :** surface field capacity [fraction] [4718592 values with dtype=float32]fricvsfc(time, lat, lon)float32...long_name :** surface frictional velocity [m/s] [4718592 values with dtype=float32]gfluxavesfc(time, lat, lon)float32...long_name :** surface ground heat flux [w/m^2] [4718592 values with dtype=float32]gfluxsfc(time, lat, lon)float32...long_name :** surface ground heat flux [w/m^2] [4718592 values with dtype=float32]hcdchcll(time, lat, lon)float32...long_name :** high cloud layer high cloud cover [%] [4718592 values with dtype=float32]hgtsfc(time, lat, lon)float32...long_name :** surface geopotential height [gpm] [4718592 values with dtype=float32]hgthy1(time, lat, lon)float32...long_name :** 1 hybrid level geopotential height [gpm] [4718592 values with dtype=float32]hpblsfc(time, lat, lon)float32...long_name :** surface planetary boundary layer height [m] [4718592 values with dtype=float32]icecsfc(time, lat, lon)float32...long_name :** surface ice cover [proportion] [4718592 values with dtype=float32]icetksfc(time, lat, lon)float32...long_name :** surface ice thickness [m] [4718592 values with dtype=float32]landsfc(time, lat, lon)float32...long_name :** surface land cover (0=sea, 1=land) [proportion] [4718592 values with dtype=float32]lcdclcll(time, lat, lon)float32...long_name :** low cloud layer low cloud cover [%] [4718592 values with dtype=float32]lhtflavesfc(time, lat, lon)float32...long_name :** surface latent heat net flux [w/m^2] [4718592 values with dtype=float32]lhtflsfc(time, lat, lon)float32...long_name :** surface latent heat net flux [w/m^2] [4718592 values with dtype=float32]mcdcmcll(time, lat, lon)float32...long_name :** middle cloud layer medium cloud cover [%] [4718592 values with dtype=float32]nbdsfsfc(time, lat, lon)float32...long_name :** surface near ir beam downward solar flux [w/m^2] [4718592 values with dtype=float32]nddsfsfc(time, lat, lon)float32...long_name :** surface near ir diffuse downward solar flux [w/m^2] [4718592 values with dtype=float32]pevpravesfc(time, lat, lon)float32...long_name :** surface potential evaporation rate [w/m^2] [4718592 values with dtype=float32]pevprsfc(time, lat, lon)float32...long_name :** surface potential evaporation rate [w/m^2] [4718592 values with dtype=float32]pratesfc(time, lat, lon)float32...long_name :** surface precipitation rate [kg/m^2/s] [4718592 values with dtype=float32]preslclb(time, lat, lon)float32...long_name :** low cloud bottom level pressure [pa] [4718592 values with dtype=float32]preslclt(time, lat, lon)float32...long_name :** low cloud top level pressure [pa] [4718592 values with dtype=float32]presmclb(time, lat, lon)float32...long_name :** middle cloud bottom level pressure [pa] [4718592 values with dtype=float32]presmclt(time, lat, lon)float32...long_name :** middle cloud top level pressure [pa] [4718592 values with dtype=float32]preshclb(time, lat, lon)float32...long_name :** high cloud bottom level pressure [pa] [4718592 values with dtype=float32]preshclt(time, lat, lon)float32...long_name :** high cloud top level pressure [pa] [4718592 values with dtype=float32]pressfc(time, lat, lon)float32...long_name :** surface pressure [pa] [4718592 values with dtype=float32]prescclb(time, lat, lon)float32...long_name :** convective cloud bottom level pressure [pa] [4718592 values with dtype=float32]prescclt(time, lat, lon)float32...long_name :** convective cloud top level pressure [pa] [4718592 values with dtype=float32]pwatclm(time, lat, lon)float32...long_name :** entire atmosphere (considered as a single layer) precipitable water [kg/m^2] [4718592 values with dtype=float32]qmax2m(time, lat, lon)float32...long_name :** 2 m above ground maximum specific humidity at 2m [kg/kg] [4718592 values with dtype=float32]qmin2m(time, lat, lon)float32...long_name :** 2 m above ground minimum specific humidity at 2m [kg/kg] [4718592 values with dtype=float32]sbsnosfc(time, lat, lon)float32...long_name :** surface sublimation (evaporation from snow) [w/m^2] [4718592 values with dtype=float32]sfcrsfc(time, lat, lon)float32...long_name :** surface surface roughness [m] [4718592 values with dtype=float32]sfexcsfc(time, lat, lon)float32...long_name :** surface exchange coefficient [(kg/m^3)(m/s)] [4718592 values with dtype=float32]shtflavesfc(time, lat, lon)float32...long_name :** surface sensible heat net flux [w/m^2] [4718592 values with dtype=float32]shtflsfc(time, lat, lon)float32...long_name :** surface sensible heat net flux [w/m^2] [4718592 values with dtype=float32]sltypsfc(time, lat, lon)float32...long_name :** surface surface slope type [index] [4718592 values with dtype=float32]snodsfc(time, lat, lon)float32...long_name :** surface snow depth [m] [4718592 values with dtype=float32]snohfsfc(time, lat, lon)float32...long_name :** surface snow phase change heat flux [w/m^2] [4718592 values with dtype=float32]snowcsfc(time, lat, lon)float32...long_name :** surface snow cover [%] [4718592 values with dtype=float32]soill0_10cm(time, lat, lon)float32...long_name :** 0-0.1 m below ground liquid volumetric soil moisture (non frozen) [proportion] [4718592 values with dtype=float32]soill10_40cm(time, lat, lon)float32...long_name :** 0.1-0.4 m below ground liquid volumetric soil moisture (non frozen) [proportion] [4718592 values with dtype=float32]soill40_100cm(time, lat, lon)float32...long_name :** 0.4-1 m below ground liquid volumetric soil moisture (non frozen) [proportion] [4718592 values with dtype=float32]soill100_200cm(time, lat, lon)float32...long_name :** 1-2 m below ground liquid volumetric soil moisture (non frozen) [proportion] [4718592 values with dtype=float32]soilm0_200cm(time, lat, lon)float32...long_name :** 0-2 m below ground soil moisture validation to deprecate [kg/m^3] [4718592 values with dtype=float32]soilw0_10cm(time, lat, lon)float32...long_name :** 0-0.1 m below ground volumetric soil moisture content [fraction] [4718592 values with dtype=float32]soilw10_40cm(time, lat, lon)float32...long_name :** 0.1-0.4 m below ground volumetric soil moisture content [fraction] [4718592 values with dtype=float32]soilw40_100cm(time, lat, lon)float32...long_name :** 0.4-1 m below ground volumetric soil moisture content [fraction] [4718592 values with dtype=float32]soilw100_200cm(time, lat, lon)float32...long_name :** 1-2 m below ground volumetric soil moisture content [fraction] [4718592 values with dtype=float32]sotypsfc(time, lat, lon)float32...long_name :** surface soil type [-] [4718592 values with dtype=float32]spfh2m(time, lat, lon)float32...long_name :** 2 m above ground specific humidity [kg/kg] [4718592 values with dtype=float32]spfhhy1(time, lat, lon)float32...long_name :** 1 hybrid level specific humidity [kg/kg] [4718592 values with dtype=float32]ssrunsfc(time, lat, lon)float32...long_name :** surface storm surface runoff [kg/m^2] [4718592 values with dtype=float32]sunsdsfc(time, lat, lon)float32...long_name :** surface sunshine duration [s] [4718592 values with dtype=float32]tcdcclm(time, lat, lon)float32...long_name :** entire atmosphere total cloud cover [%] [4718592 values with dtype=float32]tcdcblcll(time, lat, lon)float32...long_name :** boundary layer cloud layer total cloud cover [%] [4718592 values with dtype=float32]tcdcccll(time, lat, lon)float32...long_name :** convective cloud layer total cloud cover [%] [4718592 values with dtype=float32]tmax2m(time, lat, lon)float32...long_name :** 2 m above ground maximum temperature [k] [4718592 values with dtype=float32]tmin2m(time, lat, lon)float32...long_name :** 2 m above ground minimum temperature [k] [4718592 values with dtype=float32]tmplclt(time, lat, lon)float32...long_name :** low cloud top level temperature [k] [4718592 values with dtype=float32]tmpmclt(time, lat, lon)float32...long_name :** middle cloud top level temperature [k] [4718592 values with dtype=float32]tmphclt(time, lat, lon)float32...long_name :** high cloud top level temperature [k] [4718592 values with dtype=float32]tmpsfc(time, lat, lon)float32...long_name :** surface temperature [k] [4718592 values with dtype=float32]tmp2m(time, lat, lon)float32...long_name :** 2 m above ground temperature [k] [4718592 values with dtype=float32]tmphy1(time, lat, lon)float32...long_name :** 1 hybrid level temperature [k] [4718592 values with dtype=float32]transsfc(time, lat, lon)float32...long_name :** surface transpiration [w/m^2] [4718592 values with dtype=float32]tsoil0_10cm(time, lat, lon)float32...long_name :** 0-0.1 m below ground soil temperature validation to deprecate [k] [4718592 values with dtype=float32]tsoil10_40cm(time, lat, lon)float32...long_name :** 0.1-0.4 m below ground soil temperature validation to deprecate [k] [4718592 values with dtype=float32]tsoil40_100cm(time, lat, lon)float32...long_name :** 0.4-1 m below ground soil temperature validation to deprecate [k] [4718592 values with dtype=float32]tsoil100_200cm(time, lat, lon)float32...long_name :** 1-2 m below ground soil temperature validation to deprecate [k] [4718592 values with dtype=float32]ugwdsfc(time, lat, lon)float32...long_name :** surface zonal flux of gravity wave stress [n/m^2] [4718592 values with dtype=float32]uflxsfc(time, lat, lon)float32...long_name :** surface momentum flux, u-component [n/m^2] [4718592 values with dtype=float32]ugrd10m(time, lat, lon)float32...long_name :** 10 m above ground u-component of wind [m/s] [4718592 values with dtype=float32]ugrdhy1(time, lat, lon)float32...long_name :** 1 hybrid level u-component of wind [m/s] [4718592 values with dtype=float32]ulwrfavesfc(time, lat, lon)float32...long_name :** surface upward long-wave rad. flux [w/m^2] [4718592 values with dtype=float32]ulwrftoa(time, lat, lon)float32...long_name :** top of atmosphere upward long-wave rad. flux [w/m^2] [4718592 values with dtype=float32]ulwrfsfc(time, lat, lon)float32...long_name :** surface upward long-wave rad. flux [w/m^2] [4718592 values with dtype=float32]uswrfavesfc(time, lat, lon)float32...long_name :** surface upward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]uswrftoa(time, lat, lon)float32...long_name :** top of atmosphere upward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]uswrfsfc(time, lat, lon)float32...long_name :** surface upward short-wave radiation flux [w/m^2] [4718592 values with dtype=float32]vgwdsfc(time, lat, lon)float32...long_name :** surface meridional flux of gravity wave stress [n/m^2] [4718592 values with dtype=float32]vbdsfsfc(time, lat, lon)float32...long_name :** surface visible beam downward solar flux [w/m^2] [4718592 values with dtype=float32]vddsfsfc(time, lat, lon)float32...long_name :** surface visible diffuse downward solar flux [w/m^2] [4718592 values with dtype=float32]vegsfc(time, lat, lon)float32...long_name :** surface vegetation [%] [4718592 values with dtype=float32]vflxsfc(time, lat, lon)float32...long_name :** surface momentum flux, v-component [n/m^2] [4718592 values with dtype=float32]vgrd10m(time, lat, lon)float32...long_name :** 10 m above ground v-component of wind [m/s] [4718592 values with dtype=float32]vgrdhy1(time, lat, lon)float32...long_name :** 1 hybrid level v-component of wind [m/s] [4718592 values with dtype=float32]vgtypsfc(time, lat, lon)float32...long_name :** surface vegetation type [integer(0-13)] [4718592 values with dtype=float32]watrsfc(time, lat, lon)float32...long_name :** surface water runoff [kg/m^2] [4718592 values with dtype=float32]weasdsfc(time, lat, lon)float32...long_name :** surface water equivalent of accumulated snow depth [kg/m^2] [4718592 values with dtype=float32]wiltsfc(time, lat, lon)float32...long_name :** surface wilting point [fraction] [4718592 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(Index([739323.0], dtype='float64', name='time'))latPandasIndexPandasIndex(Index([ -89.91, -89.794, -89.677,  -89.56, -89.443, -89.326, -89.209, -89.092,\n       -88.975, -88.858,\n       ...\n        88.858,  88.975,  89.092,  89.209,  89.326,  89.443,   89.56,  89.677,\n        89.794,   89.91],\n      dtype='float64', name='lat', length=1536))lonPandasIndexPandasIndex(Index([            0.0,   0.11718750016,   0.23437500033,   0.35156250049,\n         0.46875000065,   0.58593750081,   0.70312500098,   0.82031250114,\n          0.9375000013,   1.05468750147,\n       ...\n       358.82812549853,  358.9453129987, 359.06250049886, 359.17968799902,\n       359.29687549918, 359.41406299935, 359.53125049951, 359.64843799967,\n       359.76562549984,      359.882813],\n      dtype='float64', name='lon', length=3072))Attributes: (4)title :GDAS flux file at 00Z12mar2025, downloaded Mar 12 06:56 UTCConventions :COARDS\nGrADSdataType :Gridhistory :Sat Mar 15 17:21:35 UTC 2025 : imported by GrADS Data Server 2.0\n\n\n\n# it plots so we can get the data\nds[\"tsoil0_10cm\"].plot();",
    "crumbs": [
      "Python - OPeNDAP",
      "NOMADS"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/7-nomads.html#conclusion",
    "href": "topics-2025/2025-opendap/7-nomads.html#conclusion",
    "title": "NOAA Operational Model Archive and Distribution System",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Python - OPeNDAP",
      "NOMADS"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/7-nomads.html#references",
    "href": "topics-2025/2025-opendap/7-nomads.html#references",
    "title": "NOAA Operational Model Archive and Distribution System",
    "section": "References",
    "text": "References\n\nhttps://nomads.ncep.noaa.gov/",
    "crumbs": [
      "Python - OPeNDAP",
      "NOMADS"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/dap2-vs-dap4.html#key-differences",
    "href": "topics-2025/2025-opendap/dap2-vs-dap4.html#key-differences",
    "title": "Comparing DAP2 and DAP4",
    "section": "Key Differences",
    "text": "Key Differences\nDAP2 is the older protocol and DAP4 is the newer. In this notebook, I will run some comparison code. Per ChapGPT here are some of the differences.\n\n\n\n\n\n\n\n\nFeature\nDAP2\nDAP4\n\n\n\n\nData Model\nSupports simple types (e.g., integers, floats, strings, arrays) and some complex structures.\nSupports a richer set of data types, including better handling of nested structures and new metadata constructs.\n\n\nData Encoding\nUses ASCII and binary (older, less efficient encoding).\nUses more modern binary encoding, including NetCDF-4/HDF5-like structures.\n\n\nMetadata Handling\nLimited support for additional metadata.\nSupports richer metadata, allowing self-describing datasets.\n\n\nChunked Data Access\nLimited ability to access specific parts of large datasets.\nImproved ability to request and return chunks of data efficiently.\n\n\nSupport for Modern Formats\nLess support for modern formats like HDF5 and NetCDF-4.\nNative support for HDF5 and NetCDF-4, allowing better integration with existing scientific workflows.\n\n\nEfficient Transfers\nLess efficient for large datasets.\nMore efficient for large datasets due to better compression and structured data requests.\n\n\nConstraint Expressions\nLimited filtering and subsetting capabilities.\nMore expressive constraints, enabling more sophisticated data selection.",
    "crumbs": [
      "Python - OPeNDAP",
      "DAP2 vs DAP4"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/dap2-vs-dap4.html#setup",
    "href": "topics-2025/2025-opendap/dap2-vs-dap4.html#setup",
    "title": "Comparing DAP2 and DAP4",
    "section": "Setup",
    "text": "Setup\n\nimport earthaccess\nimport pydap\nimport xarray as xr\nearthaccess.login()\nsession = earthaccess.get_requests_https_session()",
    "crumbs": [
      "Python - OPeNDAP",
      "DAP2 vs DAP4"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/dap2-vs-dap4.html#comparison",
    "href": "topics-2025/2025-opendap/dap2-vs-dap4.html#comparison",
    "title": "Comparing DAP2 and DAP4",
    "section": "Comparison",
    "text": "Comparison\nThis dataset has both DAP4 and DAP2 access: https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/hyrax/MERRA/MAI1NXINT.5.2.0/2016/01/contents.html\nIf we use https:// in the url, pydap will automatically use DAP2. To use DAP4, we use dap4:// instead. pydap issues a warning if we are accessing with DAP2, so let’s turn that off.\n\nimport warnings\n\n# Suppress only the specific warning from PyDAP\nwarnings.filterwarnings(\"ignore\", message=\"PyDAP was unable to determine the DAP protocol*\", category=UserWarning)\n\n\nDAP4 is faster for data lazy loading\n\n%%time\nurl = \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf\"\nds2 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 111 ms, sys: 8.42 ms, total: 119 ms\nWall time: 6.84 s\n\n\n\n%%time\nurl = \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf\"\nds4 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 56.9 ms, sys: 9.32 ms, total: 66.2 ms\nWall time: 3.32 s\n\n\n\n\nDAP4 adds slashes to the dims\nThis is not a pydap thing. You can see it on the OPeNDAP access page too.\n\nds4.dims\n\nFrozenMappingWarningOnValuesAccess({'/TIME': 24, '/YDim': 361, '/XDim': 540})\n\n\n\nds2.dims\n\nFrozenMappingWarningOnValuesAccess({'TIME': 24, 'YDim': 361, 'XDim': 540})\n\n\n\n\nData loading time is faster with DAP2\nAt least in this example.\n\n%%time\nds4[\"TQL\"].isel({\"/TIME\": 2, \"/YDim\": 10}).load();\n\nCPU times: user 56.2 ms, sys: 3.21 ms, total: 59.4 ms\nWall time: 3.55 s\n\n\n\n%%time\nds2[\"TQL\"].isel({\"TIME\": 2, \"YDim\": 10}).load();\n\nCPU times: user 16.9 ms, sys: 0 ns, total: 16.9 ms\nWall time: 977 ms\n\n\n\n\nMean is not too different\nNeed to reset up data since I loaded the data in the previous example.\n\n%%time\nurl = \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf\"\nds2 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 674 ms, sys: 57.7 ms, total: 732 ms\nWall time: 7.5 s\n\n\n\n%%time\nds2[\"TQL\"].mean().compute();\n\nCPU times: user 132 ms, sys: 76.9 ms, total: 208 ms\nWall time: 2.61 s\n\n\n\n%%time\nurl = \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf\"\nds4 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 136 ms, sys: 8.96 ms, total: 145 ms\nWall time: 3.59 s\n\n\n\n%%time\nds4[\"TQL\"].mean().compute();\n\nCPU times: user 144 ms, sys: 66.7 ms, total: 210 ms\nWall time: 2.44 s\n\n\n\n\nAdding constraints decreases lazy loading time but DAP4 still faster\nFor DAP4, add ?dap4.ce=/TQL to select just TQL. For DAP2, add ?TQL.\n\n%%time\nurl = \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf?TQL\"\n#pydap_ds2 = pydap.client.open_url(url, protocol=\"dap2\", session=session)\nds2 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 36 ms, sys: 0 ns, total: 36 ms\nWall time: 1.74 s\n\n\n\n%%time\nurl = \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf?dap4.ce=/TQL\"\nds4 = xr.open_dataset(url, engine=\"pydap\", session=session)\n\nCPU times: user 15.1 ms, sys: 7.53 ms, total: 22.6 ms\nWall time: 947 ms\n\n\n\n\nxarray.open_mfdataset is faster with DAP4\nThough speed difference has decreased from using xarray.open_dataset.\n\n%%time\nurls = [\n    \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf?dap4.ce=/TQL\",\n    \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160102.hdf?dap4.ce=/TQL\",\n    \"dap4://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160103.hdf?dap4.ce=/TQL\"\n]\nds4 = xr.open_mfdataset(urls, engine=\"pydap\",\n                        parallel=True, \n                        combine='nested', \n                        concat_dim='/TIME',\n                        session=session)\n\nCPU times: user 68.8 ms, sys: 5 ms, total: 73.8 ms\nWall time: 968 ms\n\n\n\n%%time\nurls = [\n    \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160101.hdf?TQL\",\n    \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160102.hdf?TQL\",\n    \"https://goldsmr1.gesdisc.eosdis.nasa.gov/opendap/MERRA/MAI1NXINT.5.2.0/2016/01/MERRA300.prod.assim.inst1_2d_int_Nx.20160103.hdf?TQL\"\n]\nds2 = xr.open_mfdataset(urls, engine=\"pydap\",\n                        parallel=True, \n                        combine='nested', \n                        concat_dim='TIME',\n                        session=session)\n\nCPU times: user 114 ms, sys: 11.1 ms, total: 125 ms\nWall time: 1.59 s\n\n\n\n\nMean is now faster with DAP4\nFor this example.\n\n%%time\nds4.mean().compute();\n\nCPU times: user 475 ms, sys: 143 ms, total: 619 ms\nWall time: 2.95 s\n\n\n\n%%time\nds2.mean().compute();\n\nCPU times: user 393 ms, sys: 108 ms, total: 501 ms\nWall time: 3.12 s\n\n\n\n\nEffect of rechunking doesn’t seem to be different\nall the ds’s are like this to start. Rechunking doesn’t seem to slow one down more than another.\n\nprint(ds4[\"TQL\"].chunks)\n\n((24, 24, 24), (361,), (540,))\n\n\n\nds2_rechunked = ds2.chunk({\"TIME\": 2, \"XDim\": 25, \"YDim\": 25})\n\n\n%%time\nds2_rechunked.mean().compute();\n\nCPU times: user 9.39 s, sys: 1.28 s, total: 10.7 s\nWall time: 11.9 s\n\n\n\nds4_rechunked = ds4.chunk({\"/TIME\": 2, \"/XDim\": 25, \"/YDim\": 25})\n\n\n%%time\nds4_rechunked.mean().compute();\n\nCPU times: user 9.06 s, sys: 1.29 s, total: 10.4 s\nWall time: 11.4 s\n\n\n\nds2_rechunked = ds2.chunk({\"TIME\": 50, \"XDim\": -1, \"YDim\": -1})\n\n\n%%time\nds2_rechunked.mean().compute();\n\nCPU times: user 411 ms, sys: 98 ms, total: 509 ms\nWall time: 2.96 s\n\n\n\nds4_rechunked = ds4.chunk({\"/TIME\": 50, \"/XDim\": -1, \"/YDim\": -1})\n\n\n%%time\nds4_rechunked.mean().compute();\n\nCPU times: user 465 ms, sys: 182 ms, total: 647 ms\nWall time: 2.85 s\n\n\n\n\nSaving to netcdf\nNot too different. I have to do some clean up on the DAP4 dim names with slashes before saving.\n\n%%time\nds2.attrs = {}\nds2.to_netcdf(\"output.nc\")\n\nCPU times: user 389 ms, sys: 178 ms, total: 567 ms\nWall time: 4.17 s\n\n\n\nds4_clean = ds4.rename({\"/TIME\": \"TIME\", \"/YDim\": \"YDim\", \"/XDim\": \"XDim\"})\n\n\n%%time\nds4_clean.attrs = {}\nds4_clean.to_netcdf(\"output.nc\")\n\nCPU times: user 554 ms, sys: 190 ms, total: 744 ms\nWall time: 3.66 s",
    "crumbs": [
      "Python - OPeNDAP",
      "DAP2 vs DAP4"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/dap2-vs-dap4.html#conclusion",
    "href": "topics-2025/2025-opendap/dap2-vs-dap4.html#conclusion",
    "title": "Comparing DAP2 and DAP4",
    "section": "Conclusion",
    "text": "Conclusion\nThe main difference I was able to see with these tests is that lazy loading was faster with DAP4, but this was evidenced mostly when using open_dataset. One the xarray Dataset was lazy loading, loading the data or doing computations (mean) was similar or sometimes faster when DAP2 was used.\n\nds4.nbytes / 1e6\n\n299.44264",
    "crumbs": [
      "Python - OPeNDAP",
      "DAP2 vs DAP4"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html",
    "href": "topics-2025/2025-opendap/index.html",
    "title": "OPeNDAP - Python",
    "section": "",
    "text": "In this session, you will get an introduction to accessing data on data servers that provide OPeNDAP for data access. What is OPeNDAP? “OPeNDAP is the commonly used name for a discipline-neutral data access protocol (DAP) that’s widely utilized both to access and to provide data over the internet. You will find lots of data servers that use OPeNDAP.\nNote, these tutorials are narrowly focused on an xarray workflow with earthaccess handling authentication and pydap as the underlying engine. The goal is to create a lazy (no data loaded) xarray data cubes via open_dataset and open_mfdataset functions.\nIf you use NASA data, they are migrating to serving their data to the cloud and you should look at the earthaccess examples for data access. If you use NOAA data, they use a lot of ERDDAP servers, which are built off OPeNDAP but have some extra features. Go to the ERDDAP examples. If your data are somewhere in a cloud-native format (Zarr, geotiff) use that. That is going to be a lot easier (and faster) to work with for the xarray workflows that I focus in the 2025 HackDays tutorials.",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#tutorials",
    "href": "topics-2025/2025-opendap/index.html#tutorials",
    "title": "OPeNDAP - Python",
    "section": "Tutorials",
    "text": "Tutorials\n\nSimple example 1 with no authentication needed. NCEP-NCAR\nSimple example 2 with no authentication needed. Delaware Bay Operational Forecast System\nNASA OPeNDAP servers, authentication needed. NASA OPeNDAP\nNSIDC server, NASA Earthdata authentication needed.\nUSDA LPDAAC server, NASA Earthdata authentication needed.\nNOAA Operational Model Archive and Distribution System (NOMADS)",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#servers-that-use-nasa-earthdata-authentication",
    "href": "topics-2025/2025-opendap/index.html#servers-that-use-nasa-earthdata-authentication",
    "title": "OPeNDAP - Python",
    "section": "Servers that use NASA Earthdata authentication",
    "text": "Servers that use NASA Earthdata authentication\nThings with nasa.gov are obvious but a couple others without nasa.gov in the url also use it.\n\nlist (not fully up to date) of NASA opendap servers: https://www.earthdata.nasa.gov/engage/open-data-services-software/earthdata-developer-portal/opendap/servers\nNSIDC https://n5eil02u.ecs.nsidc.org/opendap/ docs\nUSGS LPDAAC https://opendap.cr.usgs.gov/opendap/hyrax/",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/2025-opendap/index.html#references",
    "href": "topics-2025/2025-opendap/index.html#references",
    "title": "OPeNDAP - Python",
    "section": "References",
    "text": "References\n\nxarray support for opendap\npydap\nhttps://github.com/ornldaac/daymet-python-opendap-xarray\nhttps://github.com/podaac/tutorials/blob/master/notebooks/opendap/MUR-OPeNDAP.ipynb\nhttps://earthaccess.readthedocs.io/en/latest/user-reference/api/api/#earthaccess.api.get_requests_https_session\nhttps://podaac.jpl.nasa.gov/OPeNDAP-in-the-Cloud\nNASA opendap servers: https://www.earthdata.nasa.gov/engage/open-data-services-software/earthdata-developer-portal/opendap/servers",
    "crumbs": [
      "Python - OPeNDAP"
    ]
  },
  {
    "objectID": "topics-2025/index.html",
    "href": "topics-2025/index.html",
    "title": "HackHours 2025",
    "section": "",
    "text": "During these stand-alone informal sessions we will get introduced to a variety of tools for ocean data access and analysis in Python and R. We will be using the NOAA Fisheries Openscapes JupyterHub and you will not need to install anything. About the HackHours\nWhen: Fridays 11am Pacific/2pm Eastern. How do I get access? Click here for Video Link and JupyterHub Access (NOAA only)\nDownload"
  },
  {
    "objectID": "topics-2025/index.html#schedule-links-to-content-on-left",
    "href": "topics-2025/index.html#schedule-links-to-content-on-left",
    "title": "HackHours 2025",
    "section": "Schedule (links to content on left)",
    "text": "Schedule (links to content on left)\n\nFeb 7 - Q&A and Intro to the Ocean Data Science JupyterHub and Friday HackHours\nFeb 14 - Accessing NASA Earth Observation data in Python (Eli Holmes) \nFeb 21 - Accessing NASA Earth Observation data in R (Eli Holmes) \nFeb 28 - Working with ERDDAP data in Python: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 7 - Working with ERDDAP data in R: CoastWatch tutorials (Sunny Hospital, Polarwatch; Daisy Shi, CoastWatch) \nMar 14 - Using LLMs in R to improve data dashboards (Carl Boettiger, UC Berkeley) \nMar 21 - VirtualiZarr, Dask and Holoviz to explore NODD data (Rich Signell, Open Science Consulting) \nMar 28 - Working with data on OPeNDAP servers in Python & R  \nApr 4 - xarray + GPU integration (Max Jones, Development Seed) \nApr 11 - Accessing CEFI data on OPeNDAP, AWS and Google (Chia-Wei Hsu, NOAA PSL) \nApr 25 - Working with acoustic data in Python: echopype (Wu-Jung Lee, UW APL) \nMay 2 - Coiled demo – parallel processing for big data pipelines (Coiled team)\nMay 9 - PACE Hyperspectral Ocean Color Data Access and Visualization in Python (earthaccess) \nMay 16 - PACE Hyperspectral Ocean Color Data Access and Visualization in R \nMay 19 - EDMW 3-hour Workshop working with PACE hyperspectral data\nMay 30 - Machine-Learning with Ocean Data: gap-filling with CNNs \nTBD - Introduction to the Nautilus HyperCluster for running containerized Big Data Applications, UC Berkeley)"
  },
  {
    "objectID": "topics-skills/02-git-clinic.html",
    "href": "topics-skills/02-git-clinic.html",
    "title": "Git Clinic",
    "section": "",
    "text": "In this tutorial, we will provide a brief introduction to version control with Git."
  },
  {
    "objectID": "topics-skills/02-git-clinic.html#step-3",
    "href": "topics-skills/02-git-clinic.html#step-3",
    "title": "Git Clinic",
    "section": "Step 3:",
    "text": "Step 3:\nConfigure git with your name and email address.\n``` bash\ngit config --global user.name \"Makhan Virdi\"\ngit config --global user.email \"Makhan.Virdi@gmail.com\"\n```\n\n**Note:** This name and email could be different from your github.com credentials. Remember `git` is a program that keeps track of your changes locally (on 2i2c JupyterHub or your own computer) and github.com is a platform to host your repositories. However, since your changes are tracked by `git`, the email/name used in git configuration will show up next to your contributions on github.com when you `push` your repository to github.com (`git push` is discussed in a later step).\n\nConfigure git to store your github credentials to avoid having to enter your github username and token each time you push changes to your repository(in Step 5, we will describe how to use github token instead of a password)\ngit config --global credential.helper store\nCopy link for the demo repository from your github account. Click the green “Code” button and copy the link as shown.\n\nClone the repository using git clone command in the terminal\nTo clone a repository from github, copy the link for the repository (previous step) and use git clone:\ngit clone https://github.com/YOUR-GITHUB-USERNAME/check_github_setup\nNote: Replace YOUR-GITHUB-USERNAME here with your github.com username. For example, it is virdi for my github.com account as seen in this image.\n\nUse ls (list files) to verify the existence of the repository that you just cloned\n\nChange directory to the cloned repository using cd check_github_setup and check the current directory using pwd command (present working directory)\n\nCheck status of your git repository to confirm git set up using git status\n\nYou are all set with using git on your 2i2c JupyterHub! But the collaborative power of git through github needs some additional setup.\nIn the next step, we will create a new file in this repository, track changes to this file, and link it with your github.com account.\n\n\nStep 4. Creating new file and tracking changes\n\nIn the left panel on your 2i2c JupyterHub, click on the “directory” icon and then double click on “check_github_setup” directory.\n\n\nOnce you are in the check_github_setup directory, create a new file using the text editor in your 2i2c JupyterHub (File &gt;&gt; New &gt;&gt; Text File).\n\nName the file lastname.txt. For example, virdi.txt for me (use your last name). Add some content to this file (for example, I added this to my virdi.txt file: my last name is virdi).\n\nNow you should have a new file (lastname.txt) in the git repository directory check_github_setup\nCheck if git can see that you have added a new file using git status. Git reports that you have a new file that is not tracked by git yet, and suggests adding that file to the git tracking system.\n\nAs seen in this image, git suggests adding that file so it can be tracked for changes. You can add file to git for tracking changes using git add. Then, you can commit changes to this file’s content using git commit as shown in the image.\ngit add virdi.txt\ngit status\ngit commit -m \"adding a new file\"\ngit status\n\nAs seen in the image above, git is suggesting to push the change that you just committed to the remote server at github.com (so that your collaborators can also see what changes you made).\nNote: DO NOT execute push yet. Before we push to github.com, let’s configure git further and store our github.com credentials to avoid entering the credentials every time we invoke git push. For doing so, we need to create a token on github.com to be used in place of your github.com password.\n\n\n\nStep 5. Create access token on github.com\n\nGo to your github account and create a new “personal access token”: https://github.com/settings/tokens/new\n\n\n\nGenerate Personal Access Token on github.com\n\n\nEnter a description in “Note” field as seen above, select “repo” checkbox, and scroll to the bottom and click the green button “Generate Token”. Once generated, copy the token (or save it in a text file for reference).\nIMPORTANT: You will see this token only once, so be sure to copy this. If you do not copy your token at this stage, you will need to generate a new token.\n\nTo push (transfer) your changes to github, use git push in terminal. It requires you to enter your github credentials. You will be prompted to enter your github username and “password”. When prompted for your “password”, DO NOT use your github password, use the github token that was copied in the previous step.\ngit push\n\nNote: When you paste your token in the terminal window, windows users will press Ctrl+V and mac os users will press Cmd+V. If it does not work, try generating another token and use the copy icon next to the token to copy the token. Then, paste using your computer’s keyboard shortcut for paste.\nNow your password is stored in ~/.git-credentials and you will not be prompted again unless the Github token expires. You can check the presence of this git-credentials file using Terminal. Here the ~ character represents your home directory (/home/jovyan/).\nls -la ~\nThe output looks like this:\ndrwxr-xr-x 13 jovyan jovyan 6144 Oct 22 17:35 .\ndrwxr-xr-x  1 root   root   4096 Oct  4 16:21 ..\n-rw-------  1 jovyan jovyan 1754 Oct 29 18:30 .bash_history\ndrwxr-xr-x  4 jovyan jovyan 6144 Oct 29 16:38 .config\n-rw-------  1 jovyan jovyan   66 Oct 22 17:35 .git-credentials\n-rw-r--r--  1 jovyan jovyan   84 Oct 22 17:14 .gitconfig\ndrwxr-xr-x 10 jovyan jovyan 6144 Oct 21 16:19 2021-Cloud-Hackathon\nYou can also verify your git configuration\n(notebook) jovyan@jupyter-virdi:~$ git config -l\nThe output should have credential.helper = store:\nuser.email        = Makhan.Virdi@gmail.com\nuser.name         = Makhan Virdi\ncredential.helper = store\n\nNow we are all set to collaborate with github on the JupyterHub during the Cloud Hackathon!\n\n\nSummary: Git Commands\n\nCommonly used git commands (modified from source)\n\n\nGit Command\nDescription\n\n\n\n\ngit status\nShows the current state of the repository: the current working branch, files in the staging area, etc.\n\n\ngit add\nAdds a new, previously untracked file to version control and marks already tracked files to be committed with the next commit\n\n\ngit commit\nSaves the current state of the repository and creates an entry in the log\n\n\ngit log\nShows the history for the repository\n\n\ngit diff\nShows content differences between commits, branches, individual files and more\n\n\ngit clone\nCopies a repository to your local environment, including all the history\n\n\ngit pull\nGets the latest changes of a previously cloned repository\n\n\ngit push\nPushes your local changes to the remote repository, sharing them with others\n\n\n\n\n\nGit: More Details\nLesson: For a more detailed self-paced lesson on git, visit Git Lesson from Software Carpentry\nCheatsheet: Frequently used git commands\nDangit, Git!?!: If you are stuck after a git mishap, there are ready-made solutions to common problems at Dangit, Git!?!\n\n\nCloning our repository using the git JupyterLab extension.\nIf we’re already familiar with git commands and feel more confortable using a GUI our Jupyterhub deployment comes with a git extension. This plugin allows us to operate with git using a simple user interface.\nFor example we can clone our repository using the extension.\n\n\n\ngit extension"
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#prerequisites",
    "href": "topics-skills/02-git-jupyter.html#prerequisites",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "href": "topics-skills/02-git-jupyter.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-jupyter.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to authenticate. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "href": "topics-skills/02-git-jupyter.html#git-extension-in-jupyterlab",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Git extension in JupyterLab",
    "text": "Git extension in JupyterLab\nWhen the instructions say to use or open or click the Git GUI, look here:",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#the-key-skills",
    "href": "topics-skills/02-git-jupyter.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "href": "topics-skills/02-git-jupyter.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\n\nClick the + in the upper left from YOUR GitHub account (https://www.github.com/yourusername).\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nFirst make sure you are at the home directory level. Look at the folder icon under the blue launcher button. It should show, folder icon only like in this image. If not, then click on the folder icon.\n\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nClick on the git icon and then click “Clone a Repository” \nPaste in the URL of your repo from Step 1\nClick Clone. You can stay with the defaults for the checkboxes.\n\nShow me\n\n\nSkill 3: Make some changes and commit your changes\nThis writes a note about what changes you have made. It also marks a ‘point’ in time that you can go back to if you need to.\n\nClick on the README.md file in the Test repo.\nMake some changes to the file.\nClick the Git icon (in left navbar), and stage the change(s) by checking the “+” next to the files listed.\nAdd a commit message in the box.\nClick the Commit button at bottom.\n\nShow me\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom Git icon, look for the little cloud at the top. It is rather small. Click that to push changes.\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub (not in JupyterLab)\nFrom Git icon, click on the little cloud with a down arrow.\n\n\n\nActivity 1\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\n\nIn the Test repo, create a file called to &lt;yourname&gt;.md.\nStage and then commit that new file.\nPush to GitHub.\nMake some more changes and push to GitHub.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-jupyter.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-jupyter.html#footnotes",
    "href": "topics-skills/02-git-jupyter.html#footnotes",
    "title": "Basic Git/GitHub Skills in JupyterLab git GUI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#prerequisites",
    "href": "topics-skills/02-git-terminal.html#prerequisites",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nRead Intro to Git\nHave a GitHub account\nGit Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#create-a-github-account",
    "href": "topics-skills/02-git-terminal.html#create-a-github-account",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\nFor access to the NMFS Openscapes JupyterHub, you will need at GitHub account. See the main HackHour page on how to request access (NOAA staff). For NMFS staff, you can look at the NMFS OpenSci GitHub Guide information for how to create your user account and you will find lots of information on the NMFS GitHub Governance Team Training Page (visible only to NOAA staff).",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "href": "topics-skills/02-git-terminal.html#setting-up-git-authentication",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Setting up Git Authentication",
    "text": "Setting up Git Authentication\nBefore we can work with Git in the JupyterHub, your need to do some set up. Do the steps here: Git Authentication",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "href": "topics-skills/02-git-terminal.html#git-in-the-terminal",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Git in the terminal",
    "text": "Git in the terminal\nYou will need to open a terminal in JupyterLab or RStudio.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#the-key-skills",
    "href": "topics-skills/02-git-terminal.html#the-key-skills",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "The Key Skills",
    "text": "The Key Skills\n\nSkill 1: Create a blank repo on GitHub\nSkill 2: Clone your GitHub repo\nSkill 3: Make some changes and commit those local changes\nSkill 4: Push the changes to GitHub\nSkill 1b: Copy someone else’s GitHub repository",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "href": "topics-skills/02-git-terminal.html#lets-see-it-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Let’s see it done!",
    "text": "Let’s see it done!\n\nSkill 1: Create a blank repo on GitHub\nThis skill is done on GitHub.com.\n\nClick the + in the upper left from YOUR GitHub page.\nGive your repo the name Test and make sure it is public.\nClick new and check checkbox to add the Readme file and .gitignore\nCopy the URL of your new repo. It’s in the browser where you normally see a URL.\n\n\n\nSkill 2: Clone your repo\nThese skills are done in a terminal from JupyterLab or RStudio.\n\nCopy the URL of your repo. https://www.github.com/yourname/Test\nOpen a terminal.\nMake sure you are at the home directory level. Type this: cd ~\nClone the repo with this command. Replace yourname with your username. git clone https://www.github.com/yourname/Test\n\n\n\nSkill 3: Make some changes and commit your changes\nDo step 1 in your editor, JupyterLab or RStudio.\n\nMake some changes to the README.md file in the Test repo.\nGo to the terminal and make sure you are in your Test repo. cd ~/Test\nSee what has changed. You should see that README.md has changed. git status\nStage the change to the README.md git add README.md\nCommit the change. `git commit -m “small change”\n\n\n\nSkill 4: Push changes to GitHub / Pull changes from GitHub\nTo push changes you committed in Skill #3\n\nFrom the terminal, type git push\n\nTo pull changes on GitHub that are not on your local computer:\n\nMake some changes directly on GitHub.com and commit\nFrom the terminal, type git pull\n\n\n\nActivity 1\nDo steps 1 to 3 in your editor, JupyterLab or RStudio, and steps 4 and 5 in the terminal on the JupyterHub.\n\nMake a copy of README.md\nRename it to .md\nAdd some text.\nStage and commit the added file.\nPush to GitHub.\n\nShow me\n\n\nActivity 2\nDo steps 1-3 on GitHub and step 4 from the terminal on the JupyterHub.\n\nGo to your Test repo on GitHub. https://www.github.com/yourname/Test\nCreate a file called test.md.\nStage and then commit that new file.\nPull in that new file.\n\n\n\nActivity 3\nYou can copy your own or other people’s repos1.\n\nIn a browser, go to the GitHub repository https://github.com/RWorkflow-Workshops/Week5\nCopy its URL.\nNavigate to your GitHub page: click your icon in the upper right and then ‘your repositories’\nClick the + in top right and click import repository. Paste in the URL and give your repo a name.\nUse Skill #1 to clone your new repo to the JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "href": "topics-skills/02-git-terminal.html#clean-up-after-you-are-done",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Clean up after you are done",
    "text": "Clean up after you are done\n\nOpen a Terminal\nType\ncd ~\nrm -rf Test\nrm -rf Week5",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-git-terminal.html#footnotes",
    "href": "topics-skills/02-git-terminal.html#footnotes",
    "title": "Basic Git/GitHub Skills in the Terminal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is different from forking. There is no connection to the original repository.↩︎",
    "crumbs": [
      "JupyterHub Skills",
      "Git in the terminal"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html",
    "href": "topics-skills/02-intro-to-lab.html",
    "title": "Intro to JupyterLab",
    "section": "",
    "text": "When you start the JupyterHub, you will be in JupyterLab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#terminalshell",
    "href": "topics-skills/02-intro-to-lab.html#terminalshell",
    "title": "Intro to JupyterLab",
    "section": "Terminal/Shell",
    "text": "Terminal/Shell\nLog into the JupyterHub. If you do not see something like this\n\nThen go to File &gt; New Launcher\nClick on the “Terminal” box to open a new terminal window.\n\nShell or Terminal Basics\nIf you have no experience working in a terminal, check out this self-paced lesson on running scripts from the shell: Shell Lesson from Software Carpentry\nBasic shell commands:\n\npwd where am I\ncd nameofdir move into a directory\ncd .. move up a directory\nls list the files in the current directory\nls -a list the files including hidden files\nls -l list the files with more info\ncat filename print out the contents of a file\nrm filename remove a file\nrm -r directoryname remove a directory\nrm -rf directoryname force remove a directory; careful no recovery\n\nClose the terminal by clicking on the X in the terminal tab.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#file-navigation",
    "href": "topics-skills/02-intro-to-lab.html#file-navigation",
    "title": "Intro to JupyterLab",
    "section": "File Navigation",
    "text": "File Navigation\nIn the far left, you will see a line of icons. The top one is a folder and allows us to move around our file system.\n\nClick on file icon below the blue button with a +. Now you see files in your home directory.\nClick on the folder icon that looks like this. Click on the actual folder image. \nThis shows me doing this\n\nCreate a new folder.\n\nNext to the blue rectange with a +, is a grey folder with a +. Click that to create a new folder, called lesson-scripts.\n\n\nCreate a new file\n\nCreate with File &gt; New &gt; Text file\nThe file will open and you can edit it.\nSave with File &gt; Save Text\n\nDelete a file\n\nDelete a file by right-clicking on it and clicking “Delete”",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "href": "topics-skills/02-intro-to-lab.html#create-a-new-jupyter-notebook",
    "title": "Intro to JupyterLab",
    "section": "Create a new Jupyter notebook",
    "text": "Create a new Jupyter notebook\nFrom Launcher, click on the “Python 3” button, this will open a new Jupyter notebook.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "href": "topics-skills/02-intro-to-lab.html#basic-jupyter-notebook-navigation",
    "title": "Intro to JupyterLab",
    "section": "Basic Jupyter notebook navigation",
    "text": "Basic Jupyter notebook navigation\nA Jupyter notebook is a series of cells than can be code (default), markdown or raw text.\n\nLook at the top cell, this is a code cell which I could see if I click on the cell and look at the top navbar. Next to “Download”, it says “Code”. I can click that dropdown and change the cell type to markdown or raw.\nTo the left of the “Save” icon in the top navbar is a “+”. This will add a new cell.\nWithin a cell, you will see some icons on the right. Roll over these icons to see what they do.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "href": "topics-skills/02-intro-to-lab.html#running-code-in-a-cell",
    "title": "Intro to JupyterLab",
    "section": "Running code in a cell",
    "text": "Running code in a cell\nTo run code in a cell, click in the cell and then hit “Shift Return”. You can also click “Run” in the menu or click the little right arrow in the top navbar above the cells.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "href": "topics-skills/02-intro-to-lab.html#creating-and-rendering-markdown",
    "title": "Intro to JupyterLab",
    "section": "Creating and rendering markdown",
    "text": "Creating and rendering markdown\nCreate an new cell (you can click the “+” in the top navbar) and then change to markdown by clicking the dropdown next to “Download” in the top navbar. Type in some markdown and the run the cell (see above on how to run cells).",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "href": "topics-skills/02-intro-to-lab.html#running-all-cells-in-a-notebook",
    "title": "Intro to JupyterLab",
    "section": "Running all cells in a notebook",
    "text": "Running all cells in a notebook\nUse the “Run” menu.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#install-packages",
    "href": "topics-skills/02-intro-to-lab.html#install-packages",
    "title": "Intro to JupyterLab",
    "section": "Install packages",
    "text": "Install packages\nUse pip install in a cell. This will not persist between sessions.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/02-intro-to-lab.html#learn-more",
    "href": "topics-skills/02-intro-to-lab.html#learn-more",
    "title": "Intro to JupyterLab",
    "section": "Learn more",
    "text": "Learn more\nThere are lots of tutorials on JupyterLab out there. Do a search to find content that works for you.",
    "crumbs": [
      "JupyterHub Skills",
      "JupyterLab"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html",
    "href": "topics-skills/03-ScratchBucket.html",
    "title": "Using the S3 Scratch Bucket",
    "section": "",
    "text": "The JupyterHub has a preconfigured S3 “Scratch Bucket” that automatically deletes files after 7 days. This is a great resource for experimenting with large datasets and working collaboratively on a shared dataset with other users.",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "href": "topics-skills/03-ScratchBucket.html#access-the-scratch-bucket",
    "title": "Using the S3 Scratch Bucket",
    "section": "Access the scratch bucket",
    "text": "Access the scratch bucket\nThe scratch bucket is hosted at s3://nmfs-openscapes-scratch. The JupyterHub automatically sets an environment variable SCRATCH_BUCKET that appends a suffix to the s3 url with your GitHub username. This is intended to keep track of file ownership, stay organized, and prevent users from overwriting data!\nEveryone has full access to the scratch bucket, so be careful not to overwrite data from other users when uploading files. Also, any data you put there will be deleted 7 days after it is uploaded\nIf you need more permanent S3 bucket storage refer to AWS_S3_bucket documentation (left) to configure your own S3 Bucket.\nWe’ll use the S3FS Python package, which provides a nice interface for interacting with S3 buckets.\n\nimport os\nimport s3fs\nimport fsspec\nimport boto3\nimport xarray as xr\nimport geopandas as gpd\n\n\n# My GitHub username is `eeholmes`\nscratch = os.environ['SCRATCH_BUCKET']\nscratch \n\n's3://nmfs-openscapes-scratch/eeholmes'\n\n\n\n# But you can set a different S3 object prefix to use:\nscratch = 's3://nmfs-openscapes-scratch/hackhours'",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#uploading-data",
    "href": "topics-skills/03-ScratchBucket.html#uploading-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Uploading data",
    "text": "Uploading data\nIt’s great to store data in S3 buckets because this storage features very high network throughput. If many users are simultaneously accessing the same file on a spinning networked harddrive (/home/jovyan/shared) performance can be quite slow. S3 has much higher performance for such cases.\n\nUpload single file\n\nlocal_file = '~/NOAAHackDays/topics-2025/2025-02-14-earthdata/littlecube.nc'\n\nremote_object = f\"{scratch}/littlecube.nc\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\nOnce a bucket has files, I can list them. If the bucket is empty, you will get errors instead of [].\n\ns3 = s3fs.S3FileSystem()\ns3.ls(scratch)\n\n['nmfs-openscapes-scratch/hackhours/littlecube.nc']\n\n\n\ns3.stat(remote_object)\n\n{'Key': 'nmfs-openscapes-scratch/hackhours/littlecube.nc',\n 'LastModified': datetime.datetime(2025, 2, 13, 21, 41, 5, tzinfo=tzlocal()),\n 'ETag': '\"d73616d9e3ad84cf58a4a676b1e3d454\"',\n 'ChecksumAlgorithm': ['CRC32'],\n 'ChecksumType': 'FULL_OBJECT',\n 'Size': 50224,\n 'StorageClass': 'STANDARD',\n 'type': 'file',\n 'size': 50224,\n 'name': 'nmfs-openscapes-scratch/hackhours/littlecube.nc'}\n\n\n\n\nUpload a directory\n\nlocal_dir = '~/NOAAHackDays/topics-2025/resources'\n\n!ls -lh {local_dir}\n\ntotal 5.9M\n-rw-r--r-- 1 jovyan jovyan 5.9M Feb 12 21:05 e_sst.nc\ndrwxr-xr-x 3 jovyan jovyan  281 Feb 12 21:18 longhurst_v4_2010\n\n\n\ns3.upload(local_dir, scratch, recursive=True)\n\n[None, None, None, None, None, None, None, None, None]\n\n\nThe directory name is the directory name (only) of the local directory.\n\ns3.ls(f'{scratch}/resources')\n\n['nmfs-openscapes-scratch/hackhours/resources/e_sst.nc',\n 'nmfs-openscapes-scratch/hackhours/resources/longhurst_v4_2010']",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#accessing-data",
    "href": "topics-skills/03-ScratchBucket.html#accessing-data",
    "title": "Using the S3 Scratch Bucket",
    "section": "Accessing Data",
    "text": "Accessing Data\nSome software packages allow you to stream data directly from S3 Buckets. But you can always pull objects from S3 and work with local file paths.\nThis download-first, then analyze workflow typically works well for older file formats like HDF and netCDF that were designed to perform well on local hard drives rather than Cloud storage systems like S3.\nFor best performance do not work with data in your home directory. Instead use a local scratch space like `/tmp`\n\nremote_object\n\n's3://nmfs-openscapes-scratch/hackhours/littlecube.nc'\n\n\n\nlocal_object = '/tmp/test.nc'\ns3.download(remote_object, local_object)\n\n[None]\n\n\n\nds = xr.open_dataset(local_object)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)\n\n\nIf you don't want to think about downloading files you can let `fsspec` handle this behind the scenes for you! This way you only need to think about remote paths\n\nfs = fsspec.filesystem(\"simplecache\", \n                       cache_storage='/tmp/files/',\n                       same_names=True,  \n                       target_protocol='s3',\n                       )\n\n\n# The `simplecache` setting above will download the full file to /tmp/files\nprint(remote_object)\nwith fs.open(remote_object) as f:\n    ds = xr.open_dataset(f.name) # NOTE: pass f.name for local cached path\n\ns3://nmfs-openscapes-scratch/hackhours/littlecube.nc\n\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 97kB\nDimensions:       (time: 366, lat: 8, lon: 8)\nCoordinates:\n  * time          (time) datetime64[ns] 3kB 2020-01-01 2020-01-02 ... 2020-12-31\n  * lat           (lat) float32 32B 33.62 33.88 34.12 ... 34.88 35.12 35.38\n  * lon           (lon) float32 32B -75.38 -75.12 -74.88 ... -73.88 -73.62\nData variables:\n    analysed_sst  (time, lat, lon) float32 94kB ...xarray.DatasetDimensions:time: 366lat: 8lon: 8Coordinates: (3)time(time)datetime64[ns]2020-01-01 ... 2020-12-31long_name :reference time of sst fieldstandard_name :timeaxis :Tcomment :Nominal time because observations are from different sources and are made at different times of the day.array(['2020-01-01T00:00:00.000000000', '2020-01-02T00:00:00.000000000',\n       '2020-01-03T00:00:00.000000000', ..., '2020-12-29T00:00:00.000000000',\n       '2020-12-30T00:00:00.000000000', '2020-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')lat(lat)float3233.62 33.88 34.12 ... 35.12 35.38long_name :latitudestandard_name :latitudeaxis :Yunits :degrees_northvalid_min :-90.0valid_max :90.0bounds :lat_bndscomment :Uniform grid with centers from -89.875 to 89.875 by 0.25 degreesarray([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375],\n      dtype=float32)lon(lon)float32-75.38 -75.12 ... -73.88 -73.62long_name :longitudestandard_name :longitudeaxis :Xunits :degrees_eastvalid_min :-180.0valid_max :180.0bounds :lon_bndscomment :Uniform grid with centers from -179.875 to 179.875 by 0.25 degreesarray([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625],\n      dtype=float32)Data variables: (1)analysed_sst(time, lat, lon)float32...long_name :analysed sea surface temperaturestandard_name :sea_surface_temperatureunits :kelvinvalid_min :-300valid_max :4500source :UNKNOWN,ICOADS SHIPS,ICOADS BUOYS,ICOADS argos,MMAB_50KM-NCEP-ICEcomment :Single-sensor Pathfinder 5.0/5.1 AVHRR SSTs used until 2005; two AVHRRs at a time are used 2007 onward. Sea ice and in-situ data used also are near real time quality for recent period. SST (bulk) is at ambiguous depth because multiple types of observations are used.[23424 values with dtype=float32]Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',\n               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',\n               '2020-01-09', '2020-01-10',\n               ...\n               '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n               '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n               '2020-12-30', '2020-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))latPandasIndexPandasIndex(Index([33.625, 33.875, 34.125, 34.375, 34.625, 34.875, 35.125, 35.375], dtype='float32', name='lat'))lonPandasIndexPandasIndex(Index([-75.375, -75.125, -74.875, -74.625, -74.375, -74.125, -73.875, -73.625], dtype='float32', name='lon'))Attributes: (0)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "href": "topics-skills/03-ScratchBucket.html#cloud-optimized-formats",
    "title": "Using the S3 Scratch Bucket",
    "section": "Cloud-optimized formats",
    "text": "Cloud-optimized formats\nOther formats like COG, ZARR, Parquet are ‘Cloud-optimized’ and allow for very efficient streaming directly from S3. In other words, you do not need to download entire files and instead can easily read subsets of the data.\nThe example below reads a Parquet file directly into memory (RAM) from S3 without using a local disk:\n\n# first upload the file\nlocal_file = '~/NOAAHackDays/topics-2025/resources/example.parquet'\n\nremote_object = f\"{scratch}/example.parquet\"\n\ns3.upload(local_file, remote_object)\n\n[None]\n\n\n\ngf = gpd.read_parquet(remote_object)\ngf.head(2)\n\n\n\n\n\n\n\n\npop_est\ncontinent\nname\niso_a3\ngdp_md_est\ngeometry\n\n\n\n\n0\n889953.0\nOceania\nFiji\nFJI\n5496\nMULTIPOLYGON (((180 -16.06713, 180 -16.55522, ...\n\n\n1\n58005463.0\nAfrica\nTanzania\nTZA\n63177\nPOLYGON ((33.90371 -0.95, 34.07262 -1.05982, 3...",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "href": "topics-skills/03-ScratchBucket.html#advanced-access-scratch-bucket-outside-of-jupyterhub",
    "title": "Using the S3 Scratch Bucket",
    "section": "Advanced: Access Scratch bucket outside of JupyterHub",
    "text": "Advanced: Access Scratch bucket outside of JupyterHub\nLet’s say you have a lot of files on your laptop you want to work with. The S3 Bucket is a convient way to upload large datasets for collaborative analysis. To do this, you need to copy AWS Credentials from the JupyterHub to use on other machines. More extensive documentation on this workflow can be found in this repository https://github.com/scottyhq/jupyter-cloud-scoped-creds.\nThe following code must be run on the JupyterHub to get temporary credentials:\n\nclient = boto3.client('sts')\n\nwith open(os.environ['AWS_WEB_IDENTITY_TOKEN_FILE']) as f:\n    TOKEN = f.read()\n\nresponse = client.assume_role_with_web_identity(\n    RoleArn=os.environ['AWS_ROLE_ARN'],\n    RoleSessionName=os.environ['JUPYTERHUB_CLIENT_ID'],\n    WebIdentityToken=TOKEN,\n    DurationSeconds=3600\n)\n\nreponse will be a python dictionary that looks like this:\n{'Credentials': {'AccessKeyId': 'ASIAYLNAJMXY2KXXXXX',\n  'SecretAccessKey': 'J06p5IOHcxq1Rgv8XE4BYCYl8TG1XXXXXXX',\n  'SessionToken': 'IQoJb3JpZ2luX2VjEDsaCXVzLXdlc////0dsD4zHfjdGi/0+s3XKOUKkLrhdXgZ8nrch2KtzKyYyb...',\n  'Expiration': datetime.datetime(2023, 7, 21, 19, 51, 56, tzinfo=tzlocal())},\n  ...\nYou can copy and paste the values to another computer, and use them to configure your access to S3:\n\ns3 = s3fs.S3FileSystem(key=response['Credentials']['AccessKeyId'],\n                       secret=response['Credentials']['SecretAccessKey'],\n                       token=response['Credentials']['SessionToken'] )\n\n\n# Confirm your credentials give you access\ns3.ls('nmfs-openscapes-scratch', refresh=True)",
    "crumbs": [
      "JupyterHub Skills",
      "S3 Scratch Bucket"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html",
    "href": "topics-skills/04-learning.html",
    "title": "Learning Resources",
    "section": "",
    "text": "You are welcome to use the JupyterHub outside of the HackHours and workshops in order to facilitate your data science and cloud-computing learning. Here are some ideas:",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#python",
    "href": "topics-skills/04-learning.html#python",
    "title": "Learning Resources",
    "section": "Python",
    "text": "Python\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\n\nI did Python for Data Science and Machine Learning Bootcamp in Udemy\n\nThe same have deep-learning and ML courses\nHarvard https://pll.harvard.edu/catalog and MIT https://ocw.mit.edu/search/?q=python have lots of free material\nGeosciences\n\nhttps://cookbooks.projectpythia.org/\nhttps://nasa-openscapes.github.io/earthdata-cloud-cookbook/\nhttps://ioos.github.io/ioos_code_lab/content/intro.html\nhttps://github.com/coastwatch-training/CoastWatch-Tutorials\nhttps://github.com/NASAARSET\nhttps://earth-env-data-science.github.io/intro.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/04-learning.html#r",
    "href": "topics-skills/04-learning.html#r",
    "title": "Learning Resources",
    "section": "R",
    "text": "R\n\nUdemy, Coursera, and DataCamp are popular platforms that have data science courses.\nGeosciences\n\nhttps://github.com/USGS-R\nhttps://pmarchand1.github.io/atelier_rgeo/rgeo_workshop.html\nhttps://datacarpentry.github.io/r-raster-vector-geospatial/\nhttps://r.geocompx.org/\nhttps://bookdown.org/mcwimberly/gdswr-book/\nhttps://rspatial.org/index.html",
    "crumbs": [
      "JupyterHub Skills",
      "Learning resources"
    ]
  },
  {
    "objectID": "topics-skills/index.html",
    "href": "topics-skills/index.html",
    "title": "JupyterHub",
    "section": "",
    "text": "Explore the topics in the left navigation bar to learn how to use JupyterLab, RStudio and Git in the JupyterHub plus the other resources available.",
    "crumbs": [
      "JupyterHub Skills"
    ]
  },
  {
    "objectID": "topics-2025/2025-planetarycomputer-r/index.html",
    "href": "topics-2025/2025-planetarycomputer-r/index.html",
    "title": "Microsoft Planetary Computer in R",
    "section": "",
    "text": "https://planetarycomputer.microsoft.com/docs/quickstarts/reading-stac-r/"
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html",
    "href": "topics-skills/01-intro-to-jupyterhub.html",
    "title": "Intro to JupyterHubs",
    "section": "",
    "text": "In this tutorial, you will get an overview of our JupyterHub.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "href": "topics-skills/01-intro-to-jupyterhub.html#log-into-the-jupyterhub",
    "title": "Intro to JupyterHubs",
    "section": "Log into the JupyterHub",
    "text": "Log into the JupyterHub\nGo to https://nmfs-openscapes.2i2c.cloud/. Click “Login to continue”. You will be asked to log in with your GitHub Account, if you are not logged in already.\n\n\n\nNMFS Openscapes JupyterHub Login\n\n\n\nImage type: Python or R\nNext you select your image type from the drop-down. The default is a geospatial image with Python and R.\n\n\nVirtual Machine size\nYou’ll see a dropdown that allows you to choose the size of virtual machine. For the tutorials, you will only need the smallest virtual machine. Please only choose the large machines if you run out of RAM as the larger machines cost us more.\n\n\n\nMachine Profiles\n\n\n\n\nStart up\nAfter we select our server type and click on start, JupyterHub will allocate our virtual machine. This may take several minutes.\n\n\n\nJupyterhub Spawning",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "href": "topics-skills/01-intro-to-jupyterhub.html#the-launcher",
    "title": "Intro to JupyterHubs",
    "section": "The Launcher",
    "text": "The Launcher\nWhen you are in the JupyterLab tab (note the Jupyter Logo), you will see a Launcher page. If you don’t see this, go to File &gt; New Launcher or click the blue button on the top left. From the Launcher, you will see a buttons to open a new Jupyter Notebook, RStudio, Desktop and VSCode on the top row and buttons to open a Terminal and other file types below.\n Clicking on the “Python 3”, Terminal, Text File and Markdown File buttons will open a new tab in JupyterLab. You can also use the File dropdown menu for these.\nTo get an overview of JupyterLab, go here: Intro to JupyterLab",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "href": "topics-skills/01-intro-to-jupyterhub.html#rstudio",
    "title": "Intro to JupyterHubs",
    "section": "RStudio",
    "text": "RStudio\nIf you click the RStudio button in Launcher, RStudio will open in a new browser tab.\n\n\n\nRStudio\n\n\nTo get an overview of RStudio, go here: Intro to RStudio",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "href": "topics-skills/01-intro-to-jupyterhub.html#end-your-session",
    "title": "Intro to JupyterHubs",
    "section": "End your session",
    "text": "End your session\nWhen you are finished working for the day you should log out of the JupyterHub, although it will log you out automatically after 90 minutes. You will not lose work; your home directory is persistent. When you keep a session active it uses up AWS resources (costs money) because it keeps a series of virtual machines deployed.\n\n\n\n\n\n\nCaution\n\n\n\nYou log out from the JupyterLab tab not the RStudio tab.\n\n\nFrom the JupyterLab browser tab, do one of two things to stop the server:\n\nLog out File -&gt; Log Out and click “Log Out”!\nor File -&gt; Hub Control Panel -&gt; Stop My Server\n\n\n\n\n\n\n\nTip\n\n\n\nCan’t find the JupyterLab tab? Go to https://nmfs-openscapes.2i2c.cloud/hub/home\n\n\nLogging out or stopping your server will NOT cause any of your work to be lost or deleted. It simply shuts down some resources. It would be equivalent to turning off your desktop computer at the end of the day.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "href": "topics-skills/01-intro-to-jupyterhub.html#restart-your-server",
    "title": "Intro to JupyterHubs",
    "section": "Restart your server",
    "text": "Restart your server\nSometimes the server will crash/stop. This can happen if too many people use a lot of memory all at once. If that happens, go to the JupyterLab tab and then File -&gt; Hub Control Panel -&gt; Stop My Server and then Start My Server. You shouldn’t lose your work unless you were uploading a file.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#your-files",
    "title": "Intro to JupyterHubs",
    "section": "Your files",
    "text": "Your files\nWhen you start your server, you will have access to your own virtual drive space. No other users will be able to see or access your files. You can upload files to your virtual drive space and save files here. You can create folders to organize your files. You personal directory is home/jovyan. Everyone has the same home directory but your files are separate and cannot be seen by others.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "href": "topics-skills/01-intro-to-jupyterhub.html#shared-files",
    "title": "Intro to JupyterHubs",
    "section": "Shared files",
    "text": "Shared files\n\n\n\nShared folder\n\n\nIn the file panel, you will see a folder called shared. These are read-only shared files that we have prepared for you.\nYou will also see shared-public. This is a read-write folder for you to put files for everyone to see and use. You can create a team folder here for shared data and files. Note, everyone can see and change these so be careful to communicate with your team so multiple people don’t work on the same file at the same time. You can also create folders for each team member and agree not to change other team members files.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "href": "topics-skills/01-intro-to-jupyterhub.html#getting-help",
    "title": "Intro to JupyterHubs",
    "section": "Getting help",
    "text": "Getting help\n\nDiscussions https://github.com/nmfs-opensci/NOAAHackDays/discussions\nAdd an issue if you see something amiss or you would like a package added https://github.com/nmfs-opensci/NOAAHackDays/issues\nVisit the NMFS Open Science Google Space (NOAA only). Search Google Spaces to find.\nNMFS staff: The NMFS Python User Group and NMFS R User Group are great places to get help from fellow users for Python and R questions.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  },
  {
    "objectID": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "href": "topics-skills/01-intro-to-jupyterhub.html#faq",
    "title": "Intro to JupyterHubs",
    "section": "FAQ",
    "text": "FAQ\nWhy do we have the same home directory as /home/jovyan? /home/jovyan is the default home directory for ‘Jupyter’ based images/dockers. It is the historic home directory for Jupyter deployments.\nCan other users see the files in my /home/jovyan folder? No, other users can not see your files.\n\nAcknowledgements\nSome sections of this document have been taken from hackweeks organized by the University of Washington eScience Institute, CryoCloud and Openscapes.",
    "crumbs": [
      "JupyterHub Skills",
      "Intro to JupyterHubs"
    ]
  }
]